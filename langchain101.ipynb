{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "LiIYlRFeDy5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDEJ4nuQCaM6",
        "outputId": "c55fe2dc-e84b-4f51-d0e9-6d35d07c2346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.219-py3-none-any.whl (1.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.16)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.17 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.17-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.9)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.8 langchain-0.0.219 langchainplus-sdk-0.0.17 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI Integration"
      ],
      "metadata": {
        "id": "hmUPscxTFoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMlxnNTAERf2",
        "outputId": "4134c7a2-2116-494f-a406-130d0f78c9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "NM6ELKBAEfE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)  # model_name=\"text-davinci-003\"\n",
        "prompt = \"O que é uma equação quadrática e qual é a fórmula para resolver uma?\"\n",
        "print(llm(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8imkAZWRFi-w",
        "outputId": "87086fa2-d5c7-4aa3-8f8c-16390213b88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Uma equação quadrática é uma equação em que a incógnita é ao quadrado. A fórmula para resolver uma equação quadrática é dada por: x = [-b ± √(b2-4ac)]/2a, onde a, b e c são constantes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "wAC52ZETIp1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "template = \"Escreva um {adjetivo} {substantivo} sobre {assunto}\""
      ],
      "metadata": {
        "id": "1wVVPJtBIsal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"adjetivo\", \"substantivo\", \"assunto\"],\n",
        "    template=template,\n",
        ")"
      ],
      "metadata": {
        "id": "iv9e6HPzOLji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(adjetivo='breve', substantivo='texto', assunto='Cyberpunk')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6_H4JMPJOZau",
        "outputId": "70c6f26b-0e93-4a3b-d914-59eb531d7ebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Escreva um breve texto sobre Cyberpunk'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt.format(adjetivo='breve', substantivo='texto', assunto='Cyberpunk')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoKWIvlFPXaF",
        "outputId": "4a9eaeec-a541-461c-db1c-ce2c5db16834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Cyberpunk é um gênero de ficção científica que se expandiu rapidamente nos últimos anos e oferece aos fãs um vislumbre de um futuro distópico e cheio de tecnologias avançadas. É marcado pela presença de elementos como governos corruptos, corporações poderosas e elites tecnológicas, tudo passando por grandes cidades ricas em alta tecnologia e milagres alienígenas. Embora geralmente seja apresentado como um mundo de negros e brilhos, esta cultura também é conhecida por retratar as consequências do uso da tecnologia, como a exploração dos pobres, a erosão dos direitos humanos e a vigilância do governo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Template with Instructions"
      ],
      "metadata": {
        "id": "HzN9bTuORUxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"\n",
        "Eu quero que você aja como um especialista em Cyberpunk.\n",
        "\n",
        "Livros, gibis, mangás, filmes, música.\n",
        "\n",
        "Por exemplo, o livro Neuromancer e o filme Blade Runner.\n",
        "\n",
        "Suas respostas deverão ter, no máximo, 500 caracteres, incluindo espaços vazios.\n",
        "\n",
        "Fale sobre {assunto}.\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"assunto\"],\n",
        "    template=template,\n",
        ")\n",
        "prompt = prompt.format(assunto='The Ghost in the Shell, o mangá e o filme')"
      ],
      "metadata": {
        "id": "4Cw4KjV6RZV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt=prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFvtFnsEWqi8",
        "outputId": "15f5581b-7653-46c5-fb00-7808b9cde3a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The Ghost in the Shell é um dos mais influentes e icônicos trabalhos de cyberpunk já feitos. O mangá de 1989 de Masamune Shirow criou um mundo de ciborgues e personagens híbridos que se fundem a máquinas, enquanto explorava temas como a ética, o transhumanismo, a consciência artificial e a filosofia da tecnologia. O filme de animação de 1995, dirigido por Mamoru Oshii, trabalhou com os temas da obra original e expandido-os para criar uma história filosófica mais profunda. Seu líder protagonista, Major Motoko Kusanagi, é uma policial-cyborg que luta contra os humilhantes desafios de sua existência e de seu mundo, enquanto ela e seus colegas tentam proteger a cidade de hackers, robôs assassinos, e terríveis criaturas tecnológicas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains"
      ],
      "metadata": {
        "id": "1YtZ3DYiXIN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"object\"],\n",
        "    template=\"Quais são as principais características de {object}?\",\n",
        ")"
      ],
      "metadata": {
        "id": "9c-BrDMHXKAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "-0AjqbN_XNo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"músicas do subgênero Synthwave\")"
      ],
      "metadata": {
        "id": "Z7VNuxPfXOt3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6db0392a-4643-4ec5-8380-f6afd6a5ae7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n1. Synthwave é um subgênero de música eletrônica que mescla elementos de outros gêneros como techno, house, Italo disco, electro e new wave.\\n\\n2. Esta música geralmente é criada usando sintetizadores \"retrô\" ou \"vintage\", que produzem sons com uma estética única.\\n\\n3. Synthwave é comumente marcada por temas nostálgicos, letras retrô-futuristas e vozes faladas, e temas temas que incluem tecnologia obscura, ficção científica, velocidade, fracasso, e naves espaciais.\\n\\n4. O ritmo acelerado da música é geralmente suportado por pulsações graves e melodias alegres.\\n\\n5. O uso de cores vibrantes e efeitos sonoros exagerados, como reverbs e delay'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agents & Tools"
      ],
      "metadata": {
        "id": "wNscOwvKXQv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent"
      ],
      "metadata": {
        "id": "fLDURvsmXSbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "Rl0rxYJKXbW6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecd030d-0e68-4987-ac70-f0dc56963130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=fa36f4e4d41c300ebcd3093c60048506a141a551ad455b4839d8676fff8e2152\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=0.7)\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "lHrdLzJuXdIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
      ],
      "metadata": {
        "id": "6qWV91WdXefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Qual é o ano em que se passa a famosa obra de George Orwell que fala sobre um futuro distópico? Qual é a raiz quadrada desse número? Responda em português brasileiro.\")"
      ],
      "metadata": {
        "id": "3xx-ad8eXf8X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "fdfc9a8b-a7fb-432c-94b4-55bcd59e5c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new  chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m É necessário descobrir o ano de publicação e, depois, encontrar a raiz quadrada desse número.\n",
            "Action: Wikipedia\n",
            "Action Input: 1984 (George Orwell)\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: Nineteen Eighty-Four\n",
            "Summary: Nineteen Eighty-Four (also published as 1984) is a dystopian social science fiction novel and cautionary tale by English writer George Orwell. It was published on 8 June 1949 by Secker & Warburg as Orwell's ninth and final book completed in his lifetime. Thematically, it centres on the consequences of totalitarianism, mass surveillance and repressive regimentation of people and behaviours within society. Orwell, a democratic socialist, modelled the authoritarian state in the novel on Stalinist Russia and Nazi Germany. More broadly, the novel examines the role of truth and facts within societies and the ways in which they can be manipulated.\n",
            "The story takes place in an imagined future in an unknown year believed to be 1984, when much of the world is in perpetual war. Great Britain, now known as Airstrip One, has become a province of the totalitarian superstate Oceania, which is led by Big Brother, a dictatorial leader supported by an intense cult of personality manufactured by the Party's Thought Police. Through the Ministry of Truth, the Party engages in omnipresent government surveillance, historical negationism, and constant propaganda to persecute individuality and independent thinking.The protagonist, Winston Smith, is a diligent mid-level worker at the Ministry of Truth who secretly hates the Party and dreams of rebellion. Smith keeps a forbidden diary. He begins a relationship with a colleague, Julia and they learn about a shadowy resistance group called the Brotherhood. However, their contact within the Brotherhood turns out to be a Party agent, and Smith is arrested. He is subjected to months of psychological manipulation and torture by the Ministry of Love and is released once he has come to love Big Brother.\n",
            "Nineteen Eighty-Four has become a classic literary example of political and dystopian fiction. It also popularised the term \"Orwellian\" as an adjective, with many terms used in the novel entering common usage, including \"Big Brother\", \"doublethink\", \"Thought Police\", \"thoughtcrime\", \"Newspeak\", and \"2 + 2 = 5\". Parallels have been drawn between the novel's subject matter and real life instances of totalitarianism, mass surveillance, and violations of freedom of expression among other themes. Orwell described his book as a \"satire\", and a display of the \"perversions to which a centralised economy is liable,\" while also stating he believed \"that something resembling it could arrive.\" Time included the novel on its list of the 100 best English-language novels from 1923 to 2005, and it was placed on the Modern Library's 100 Best Novels list, reaching number 13 on the editors' list and number 6 on the readers' list. In 2003, it was listed at number eight on The Big Read survey by the BBC.\n",
            "\n",
            "Page: George Orwell\n",
            "Summary: Eric Arthur Blair (25 June 1903 – 21 January 1950), better known by his pen name George Orwell, was an English novelist, essayist, journalist, and critic. His work is characterised by lucid prose, social criticism, opposition to totalitarianism, and support of democratic socialism.Orwell produced literary criticism, poetry, fiction and polemical journalism. He is known for the allegorical novella Animal Farm (1945) and the dystopian novel Nineteen Eighty-Four (1949). His non-fiction works, including The Road to Wigan Pier (1937), documenting his experience of working-class life in the industrial north of England, and Homage to Catalonia (1938), an account of his experiences soldiering for the Republican faction of the Spanish Civil War (1936–1939), are as critically respected as his essays on politics, literature, language and culture.\n",
            "Born in India, Blair was raised and educated in England from when he was one year old. After school he became an Imperial policeman in Burma, before returning to Suffolk, England, where he began his writing career as George Orwell—a name inspired by a favourite location, the River Orwell. He lived from occasional pieces of journalism, an\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m Agora eu sei o ano de publicação.\n",
            "Final Answer: A famosa obra de George Orwell que fala sobre um futuro distópico foi publicada em 1949. A raiz quadrada desse número é 44,72.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A famosa obra de George Orwell que fala sobre um futuro distópico foi publicada em 1949. A raiz quadrada desse número é 44,72.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Memory"
      ],
      "metadata": {
        "id": "bfFsKQ-ZXh4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "tliH9fdiXjTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "conversation.predict(input=\"Olá, camarada! Qual é o seu nome? Pode me dizer qual é o seu modelo?\")"
      ],
      "metadata": {
        "id": "__dVHELmXm5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "outputId": "f6fcfe98-f849-40d5-92ee-261b4ae591a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Olá, camarada! Qual é o seu nome? Pode me dizer qual é o seu modelo?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Olá! Meu nome é AI-1. Meu modelo é o modelo de inteligência artificial de aprendizado profundo. Estou aqui para ajudar você a entender melhor a inteligência artificial e como ela pode ser usada para melhorar sua vida.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='O que é filosofia da alma?')"
      ],
      "metadata": {
        "id": "kwfAK3IdXqmx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "6f3a763e-fec6-45dc-e686-2a6a91155318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Olá, camarada! Qual é o seu nome? Pode me dizer qual é o seu modelo?\n",
            "AI:  Olá! Meu nome é AI-1. Meu modelo é o modelo de inteligência artificial de aprendizado profundo. Estou aqui para ajudar você a entender melhor a inteligência artificial e como ela pode ser usada para melhorar sua vida.\n",
            "Human: O que é filosofia da alma?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A filosofia da alma é uma área da filosofia que se concentra na natureza da alma humana. Esta área da filosofia estuda questões como a existência da alma, a natureza da alma, a relação entre a alma e o corpo, e como a alma pode sobreviver à morte.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input='A obra \"The Ghost in the Shell\" discute temas de filosofia da alma?')"
      ],
      "metadata": {
        "id": "G5oa8dY-Xtfh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "bdfd9a4b-e1c6-4751-e81c-38c70fd848f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Olá, camarada! Qual é o seu nome? Pode me dizer qual é o seu modelo?\n",
            "AI:  Olá! Meu nome é AI-1. Meu modelo é o modelo de inteligência artificial de aprendizado profundo. Estou aqui para ajudar você a entender melhor a inteligência artificial e como ela pode ser usada para melhorar sua vida.\n",
            "Human: O que é filosofia da alma?\n",
            "AI:  A filosofia da alma é uma área da filosofia que se concentra na natureza da alma humana. Esta área da filosofia estuda questões como a existência da alma, a natureza da alma, a relação entre a alma e o corpo, e como a alma pode sobreviver à morte.\n",
            "Human: A obra \"The Ghost in the Shell\" discute temas de filosofia da alma?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Sim, a obra \"The Ghost in the Shell\" discute temas de filosofia da alma. O filme explora questões como a natureza da alma, a relação entre a alma e o corpo, e como a alma pode sobreviver à morte. Além disso, o filme também discute a natureza da consciência e da identidade.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Document Loaders"
      ],
      "metadata": {
        "id": "q3D3dHeHXvc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj5IB278Qpin",
        "outputId": "c97a5e4b-05f1-406a-8623-61bb7cb39f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-3.11.1-py3-none-any.whl (257 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/257.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.8/257.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "tyM0W--pXxuP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8605a027-757e-47a9-f26b-8165306f9578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d07c330e7cf0>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_and_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m    110\u001b[0m             )\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path /content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf is not a valid file or url"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, page in enumerate(pages):\n",
        "    print(f\"Page {i+1}:\\n{page}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkyNvmwnRwAI",
        "outputId": "4e9b09f9-d5fa-4058-b153-008b2ab4ee3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Page 1:\n",
            "page_content='What\\nIs\\nChatGPT\\nDoing\\n…\\nand\\nWhy\\nDoes\\nIt\\nWork?\\nIt’s\\nJust\\nAdding\\nOne\\nWord\\nat\\na\\nTime\\nThat\\nChatGPT\\ncan\\nautomatically\\ngenerate\\nsomething\\nthat\\nreads\\neven\\nsuperficially\\nlike\\nhuman-written\\ntext\\nis\\nremarkable,\\nand\\nunexpected.\\nBut\\nhow\\ndoes\\nit\\ndo\\nit?\\nAnd\\nwhy\\ndoes\\nit\\nwork?\\nMy\\npurpose\\nhere\\nis\\nto\\ngive\\na\\nrough\\noutline\\nof\\nwhat’s\\ngoing\\non\\ninside\\nChatGPT—and\\nthen\\nto\\nexplore\\nwhy\\nit\\nis\\nthat\\nit\\ncan\\ndo\\nso\\nwell\\nin\\nproducing\\nwhat\\nwe\\nmight\\nconsider\\nto\\nbe\\nmeaningful\\ntext.\\nI\\nshould\\nsay\\nat\\nthe\\noutset\\nthat\\nI’m\\ngoing\\nto\\nfocus\\non\\nthe\\nbig\\npicture\\nof\\nwhat’s\\ngoing\\non—and\\nwhile\\nI’ll\\nmention\\nsome\\nengineering\\ndetails,\\nI\\nwon’t\\nget\\ndeeply\\ninto\\nthem.\\n(And\\nthe\\nessence\\nof\\nwhat\\nI’ll\\nsay\\napplies\\njust\\nas\\nwell\\nto\\nother\\ncurrent\\n“large\\nlanguage\\nmodels”\\n[LLMs]\\nas\\nto\\nChatGPT.)\\nThe\\nfirst\\nthing\\nto\\nexplain\\nis\\nthat\\nwhat\\nChatGPT\\nis\\nalways\\nfundamentally\\ntrying\\nto\\ndo\\nis\\nto\\nproduce\\na\\n“reasonable\\ncontinuation”\\nof\\nwhatever\\ntext\\nit’s\\ngot\\nso\\nfar,\\nwhere\\nby\\n“reasonable”\\nwe\\nmean\\n“what\\none\\nmight\\nexpect\\nsomeone\\nto\\nwrite\\nafter\\nseeing\\nwhat\\npeople\\nhave\\nwritten\\non\\nbillions\\nof\\nwebpages,\\netc.”\\nSo\\nlet’s\\nsay\\nwe’ve\\ngot\\nthe\\ntext\\n“\\nThe\\nbest\\nthing\\nabout\\nAI\\nis\\nits\\nability\\nto\\n”.\\nImagine\\nscanning\\nbillions\\nof\\npages\\nof\\nhuman-written\\ntext\\n(say\\non\\nthe\\nweb\\nand\\nin\\ndigitized\\nbooks)\\nand\\nfinding\\nall\\ninstances\\nof\\nthis\\ntext—then\\nseeing\\nwhat\\nword\\ncomes\\nnext\\nwhat\\nfraction\\nof\\nthe\\ntime.\\nChatGPT\\neffectively\\ndoes\\nsomething\\nlike\\nthis,\\nexcept\\nthat\\n(as\\nI’ll\\nexplain)\\nit\\ndoesn’t\\nlook\\nat\\nliteral\\ntext;\\nit\\nlooks\\nfor\\nthings\\nthat\\nin\\na\\ncertain\\nsense\\n“match\\nin\\nmeaning”.\\nBut\\nthe\\nend\\nresult\\nis\\nthat\\nit\\nproduces\\na\\nranked\\nlist\\nof\\nwords\\nthat\\nmight\\nfollow,\\ntogether\\nwith\\n“probabilities”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 0}\n",
            "\n",
            "Page 2:\n",
            "page_content='And\\nthe\\nremarkable\\nthing\\nis\\nthat\\nwhen\\nChatGPT\\ndoes\\nsomething\\nlike\\nwrite\\nan\\nessay\\nwhat\\nit’s\\nessentially\\ndoing\\nis\\njust\\nasking\\nover\\nand\\nover\\nagain\\n“given\\nthe\\ntext\\nso\\nfar,\\nwhat\\nshould\\nthe\\nnext\\nword\\nbe?”—and\\neach\\ntime\\nadding\\na\\nword.\\n(More\\nprecisely,\\nas\\nI’ll\\nexplain,\\nit’s\\nadding\\na\\n“token”,\\nwhich\\ncould\\nbe\\njust\\na\\npart\\nof\\na\\nword,\\nwhich\\nis\\nwhy\\nit\\ncan\\nsometimes\\n“make\\nup\\nnew\\nwords”.)\\nBut,\\nOK,\\nat\\neach\\nstep\\nit\\ngets\\na\\nlist\\nof\\nwords\\nwith\\nprobabilities.\\nBut\\nwhich\\none\\nshould\\nit\\nactually\\npick\\nto\\nadd\\nto\\nthe\\nessay\\n(or\\nwhatever)\\nthat\\nit’s\\nwriting?\\nOne\\nmight\\nthink\\nit\\nshould\\nbe\\nthe\\n“highest-ranked”\\nword\\n(i.e.\\nthe\\none\\nto\\nwhich\\nthe\\nhighest\\n“probability”\\nwas\\nassigned).\\nBut\\nthis\\nis\\nwhere\\na\\nbit\\nof\\nvoodoo\\nbegins\\nto\\ncreep\\nin.\\nBecause\\nfor\\nsome\\nreason—that\\nmaybe\\none\\nday\\nwe’ll\\nhave\\na\\nscientific-style\\nunderstanding\\nof—if\\nwe\\nalways\\npick\\nthe\\nhighest-ranked\\nword,\\nwe’ll\\ntypically\\nget\\na\\nvery\\n“flat”\\nessay,\\nthat\\nnever\\nseems\\nto\\n“show\\nany\\ncreativity”\\n(and\\neven\\nsometimes\\nrepeats\\nword\\nfor\\nword).\\nBut\\nif\\nsometimes\\n(at\\nrandom)\\nwe\\npick\\nlower-ranked\\nwords,\\nwe\\nget\\na\\n“more\\ninteresting”\\nessay.\\nThe\\nfact\\nthat\\nthere’s\\nrandomness\\nhere\\nmeans\\nthat\\nif\\nwe\\nuse\\nthe\\nsame\\nprompt\\nmultiple\\ntimes,\\nwe’re\\nlikely\\nto\\nget\\ndifferent\\nessays\\neach\\ntime.\\nAnd,\\nin\\nkeeping\\nwith\\nthe\\nidea\\nof\\nvoodoo,\\nthere’s\\na\\nparticular\\nso-called\\n“temperature”\\nparameter\\nthat\\ndetermines\\nhow\\noften\\nlower-ranked\\nwords\\nwill\\nbe\\nused,\\nand\\nfor\\nessay\\ngeneration,\\nit\\nturns\\nout\\nthat\\na\\n“temperature”\\nof\\n0.8\\nseems\\nbest.\\n(It’s\\nworth\\nemphasizing\\nthat\\nthere’s\\nno\\n“theory”\\nbeing\\nused\\nhere;\\nit’s\\njust\\na\\nmatter\\nof\\nwhat’s\\nbeen\\nfound\\nto\\nwork\\nin\\npractice.\\nAnd\\nfor\\nexample\\nthe\\nconcept\\nof\\n“temperature”\\nis\\nthere\\nbecause\\nexponential\\ndistributions\\nfamiliar\\nfrom\\nstatistical\\nphysics\\nhappen\\nto\\nbe\\nbeing\\nused,\\nbut\\nthere’s\\nno\\n“physical”\\nconnection—at\\nleast\\nso\\nfar\\nas\\nwe\\nknow.)\\nBefore\\nwe\\ngo\\non\\nI\\nshould\\nexplain\\nthat\\nfor\\npurposes\\nof\\nexposition\\nI’m\\nmostly\\nnot\\ngoing\\nto\\nuse\\nthe\\nfull\\nsystem\\nthat’s\\nin\\nChatGPT\\n;\\ninstead\\nI’ll\\nusually\\nwork\\nwith\\na\\nsimpler\\nGPT-2\\nsystem\\n,\\nwhich\\nhas\\nthe\\nnice\\nfeature\\nthat\\nit’s\\nsmall' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 1}\n",
            "\n",
            "Page 3:\n",
            "page_content='enough\\nto\\nbe\\nable\\nto\\nrun\\non\\na\\nstandard\\ndesktop\\ncomputer.\\nAnd\\nso\\nfor\\nessentially\\neverything\\nI\\nshow\\nI’ll\\nbe\\nable\\nto\\ninclude\\nexplicit\\nWolfram\\nLanguage\\ncode\\nthat\\nyou\\ncan\\nimmediately\\nrun\\non\\nyour\\ncomputer.\\n(Click\\nany\\npicture\\nhere\\nto\\ncopy\\nthe\\ncode\\nbehind\\nit.)\\nFor\\nexample,\\nhere’s\\nhow\\nto\\nget\\nthe\\ntable\\nof\\nprobabilities\\nabove.\\nFirst,\\nwe\\nhave\\nto\\nretrieve\\nthe\\nunderlying\\n“language\\nmodel”\\nneural\\nnet\\n:\\nLater\\non,\\nwe’ll\\nlook\\ninside\\nthis\\nneural\\nnet,\\nand\\ntalk\\nabout\\nhow\\nit\\nworks.\\nBut\\nfor\\nnow\\nwe\\ncan\\njust\\napply\\nthis\\n“net\\nmodel”\\nas\\na\\nblack\\nbox\\nto\\nour\\ntext\\nso\\nfar,\\nand\\nask\\nfor\\nthe\\ntop\\n5\\nwords\\nby\\nprobability\\nthat\\nthe\\nmodel\\nsays\\nshould\\nfollow:\\nThis\\ntakes\\nthat\\nresult\\nand\\nmakes\\nit\\ninto\\nan\\nexplicit\\nformatted\\n“\\ndataset\\n”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 2}\n",
            "\n",
            "Page 4:\n",
            "page_content='Here’s\\nwhat\\nhappens\\nif\\none\\nrepeatedly\\n“applies\\nthe\\nmodel”—at\\neach\\nstep\\nadding\\nthe\\nword\\nthat\\nhas\\nthe\\ntop\\nprobability\\n(specified\\nin\\nthis\\ncode\\nas\\nthe\\n“decision”\\nfrom\\nthe\\nmodel):\\nWhat\\nhappens\\nif\\none\\ngoes\\non\\nlonger?\\nIn\\nthis\\n(“zero\\ntemperature”)\\ncase\\nwhat\\ncomes\\nout\\nsoon\\ngets\\nrather\\nconfused\\nand\\nrepetitive:\\nBut\\nwhat\\nif\\ninstead\\nof\\nalways\\npicking\\nthe\\n“top”\\nword\\none\\nsometimes\\nrandomly\\npicks\\n“non-top”\\nwords\\n(with\\nthe\\n“randomness”\\ncorresponding\\nto\\n“temperature”\\n0.8)?\\nAgain\\none\\ncan\\nbuild\\nup\\ntext:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 3}\n",
            "\n",
            "Page 5:\n",
            "page_content='And\\nevery\\ntime\\none\\ndoes\\nthis,\\ndifferent\\nrandom\\nchoices\\nwill\\nbe\\nmade,\\nand\\nthe\\ntext\\nwill\\nbe\\ndifferent—as\\nin\\nthese\\n5\\nexamples:\\nIt’s\\nworth\\npointing\\nout\\nthat\\neven\\nat\\nthe\\nfirst\\nstep\\nthere\\nare\\na\\nlot\\nof\\npossible\\n“next\\nwords”\\nto\\nchoose\\nfrom\\n(at\\ntemperature\\n0.8),\\nthough\\ntheir\\nprobabilities\\nfall\\noff\\nquite\\nquickly\\n(and,\\nyes,\\nthe\\nstraight\\nline\\non\\nthis\\nlog-log\\nplot\\ncorresponds\\nto\\nan\\nn\\n–1\\n“power-law”\\ndecay\\nthat’s\\nvery\\ncharacteristic\\nof\\nthe\\ngeneral\\nstatistics\\nof\\nlanguage\\n):\\nSo\\nwhat\\nhappens\\nif\\none\\ngoes\\non\\nlonger?\\nHere’s\\na\\nrandom\\nexample.\\nIt’s\\nbetter\\nthan\\nthe\\ntop-word\\n(zero\\ntemperature)\\ncase,\\nbut\\nstill\\nat\\nbest\\na\\nbit\\nweird:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 4}\n",
            "\n",
            "Page 6:\n",
            "page_content='This\\nwas\\ndone\\nwith\\nthe\\nsimplest\\nGPT-2\\nmodel\\n(from\\n2019).\\nWith\\nthe\\nnewer\\nand\\nbigger\\nGPT-3\\nmodels\\nthe\\nresults\\nare\\nbetter.\\nHere’s\\nthe\\ntop-word\\n(zero\\ntemperature)\\ntext\\nproduced\\nwith\\nthe\\nsame\\n“prompt”,\\nbut\\nwith\\nthe\\nbiggest\\nGPT-3\\nmodel:\\nAnd\\nhere’s\\na\\nrandom\\nexample\\nat\\n“temperature\\n0.8”:\\nWhere\\nDo\\nthe\\nProbabilities\\nCome\\nFrom?\\nOK,\\nso\\nChatGPT\\nalways\\npicks\\nits\\nnext\\nword\\nbased\\non\\nprobabilities.\\nBut\\nwhere\\ndo\\nthose\\nprobabilities\\ncome\\nfrom?\\nLet’s\\nstart\\nwith\\na\\nsimpler\\nproblem.\\nLet’s\\nconsider\\ngenerating\\nEnglish\\ntext\\none\\nletter\\n(rather\\nthan\\nword)\\nat\\na\\ntime.\\nHow\\ncan\\nwe\\nwork\\nout\\nwhat\\nthe\\nprobability\\nfor\\neach\\nletter\\nshould\\nbe?\\nA\\nvery\\nminimal\\nthing\\nwe\\ncould\\ndo\\nis\\njust\\ntake\\na\\nsample\\nof\\nEnglish\\ntext,\\nand\\ncalculate\\nhow\\noften\\ndifferent\\nletters\\noccur\\nin\\nit.\\nSo,\\nfor\\nexample,\\nthis\\ncounts\\nletters\\nin\\nthe\\nWikipedia\\narticle\\non\\n“cats”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 5}\n",
            "\n",
            "Page 7:\n",
            "page_content='And\\nthis\\ndoes\\nthe\\nsame\\nthing\\nfor\\n“dogs”:\\nThe\\nresults\\nare\\nsimilar,\\nbut\\nnot\\nthe\\nsame\\n(“o”\\nis\\nno\\ndoubt\\nmore\\ncommon\\nin\\nthe\\n“dogs”\\narticle\\nbecause,\\nafter\\nall,\\nit\\noccurs\\nin\\nthe\\nword\\n“dog”\\nitself).\\nStill,\\nif\\nwe\\ntake\\na\\nlarge\\nenough\\nsample\\nof\\nEnglish\\ntext\\nwe\\ncan\\nexpect\\nto\\neventually\\nget\\nat\\nleast\\nfairly\\nconsistent\\nresults:\\nHere’s\\na\\nsample\\nof\\nwhat\\nwe\\nget\\nif\\nwe\\njust\\ngenerate\\na\\nsequence\\nof\\nletters\\nwith\\nthese\\nprobabilities:\\nWe\\ncan\\nbreak\\nthis\\ninto\\n“words”\\nby\\nadding\\nin\\nspaces\\nas\\nif\\nthey\\nwere\\nletters\\nwith\\na\\ncertain\\nprobability:\\nWe\\ncan\\ndo\\na\\nslightly\\nbetter\\njob\\nof\\nmaking\\n“words”\\nby\\nforcing\\nthe\\ndistribution\\nof\\n“word\\nlengths”\\nto\\nagree\\nwith\\nwhat\\nit\\nis\\nin\\nEnglish:\\nWe\\ndidn’t\\nhappen\\nto\\nget\\nany\\n“actual\\nwords”\\nhere,\\nbut\\nthe\\nresults\\nare\\nlooking\\nslightly\\nbetter.\\nTo\\ngo\\nfurther,\\nthough,\\nwe\\nneed\\nto\\ndo\\nmore\\nthan\\njust\\npick\\neach\\nletter\\nseparately\\nat\\nrandom.\\nAnd,\\nfor\\nexample,\\nwe\\nknow\\nthat\\nif\\nwe\\nhave\\na\\n“q”,\\nthe\\nnext\\nletter\\nbasically\\nhas\\nto\\nbe\\n“u”.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 6}\n",
            "\n",
            "Page 8:\n",
            "page_content='Here’s\\na\\nplot\\nof\\nthe\\nprobabilities\\nfor\\nletters\\non\\ntheir\\nown:\\nAnd\\nhere’s\\na\\nplot\\nthat\\nshows\\nthe\\nprobabilities\\nof\\npairs\\nof\\nletters\\n(“2-grams”)\\nin\\ntypical\\nEnglish\\ntext.\\nThe\\npossible\\nfirst\\nletters\\nare\\nshown\\nacross\\nthe\\npage,\\nthe\\nsecond\\nletters\\ndown\\nthe\\npage:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 7}\n",
            "\n",
            "Page 9:\n",
            "page_content='And\\nwe\\nsee\\nhere,\\nfor\\nexample,\\nthat\\nthe\\n“q”\\ncolumn\\nis\\nblank\\n(zero\\nprobability)\\nexcept\\non\\nthe\\n“u”\\nrow.\\nOK,\\nso\\nnow\\ninstead\\nof\\ngenerating\\nour\\n“words”\\na\\nsingle\\nletter\\nat\\na\\ntime,\\nlet’s\\ngenerate\\nthem\\nlooking\\nat\\ntwo\\nletters\\nat\\na\\ntime,\\nusing\\nthese\\n“2-gram”\\nprobabilities.\\nHere’s\\na\\nsample\\nof\\nthe\\nresult—which\\nhappens\\nto\\ninclude\\na\\nfew\\n“actual\\nwords”:\\nWith\\nsufficiently\\nmuch\\nEnglish\\ntext\\nwe\\ncan\\nget\\npretty\\ngood\\nestimates\\nnot\\njust\\nfor\\nprobabilities\\nof\\nsingle\\nletters\\nor\\npairs\\nof\\nletters\\n(2-grams),\\nbut\\nalso\\nfor\\nlonger\\nruns\\nof\\nletters.\\nAnd\\nif\\nwe\\ngenerate\\n“random\\nwords”\\nwith\\nprogressively\\nlonger\\nn\\n-gram\\nprobabilities,\\nwe\\nsee\\nthat\\nthey\\nget\\nprogressively\\n“more\\nrealistic”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 8}\n",
            "\n",
            "Page 10:\n",
            "page_content='But\\nlet’s\\nnow\\nassume—more\\nor\\nless\\nas\\nChatGPT\\ndoes—that\\nwe’re\\ndealing\\nwith\\nwhole\\nwords,\\nnot\\nletters.\\nThere\\nare\\nabout\\n40,000\\nreasonably\\ncommonly\\nused\\nwords\\nin\\nEnglish\\n.\\nAnd\\nby\\nlooking\\nat\\na\\nlarge\\ncorpus\\nof\\nEnglish\\ntext\\n(say\\na\\nfew\\nmillion\\nbooks,\\nwith\\naltogether\\na\\nfew\\nhundred\\nbillion\\nwords),\\nwe\\ncan\\nget\\nan\\nestimate\\nof\\nhow\\ncommon\\neach\\nword\\nis\\n.\\nAnd\\nusing\\nthis\\nwe\\ncan\\nstart\\ngenerating\\n“sentences”,\\nin\\nwhich\\neach\\nword\\nis\\nindependently\\npicked\\nat\\nrandom,\\nwith\\nthe\\nsame\\nprobability\\nthat\\nit\\nappears\\nin\\nthe\\ncorpus.\\nHere’s\\na\\nsample\\nof\\nwhat\\nwe\\nget:\\nNot\\nsurprisingly,\\nthis\\nis\\nnonsense.\\nSo\\nhow\\ncan\\nwe\\ndo\\nbetter?\\nJust\\nlike\\nwith\\nletters,\\nwe\\ncan\\nstart\\ntaking\\ninto\\naccount\\nnot\\njust\\nprobabilities\\nfor\\nsingle\\nwords\\nbut\\nprobabilities\\nfor\\npairs\\nor\\nlonger\\nn\\n-grams\\nof\\nwords.\\nDoing\\nthis\\nfor\\npairs,\\nhere\\nare\\n5\\nexamples\\nof\\nwhat\\nwe\\nget,\\nin\\nall\\ncases\\nstarting\\nfrom\\nthe\\nword\\n“cat”:\\nIt’s\\ngetting\\nslightly\\nmore\\n“sensible\\nlooking”.\\nAnd\\nwe\\nmight\\nimagine\\nthat\\nif\\nwe\\nwere\\nable\\nto\\nuse\\nsufficiently\\nlong\\nn\\n-grams\\nwe’d\\nbasically\\n“get\\na\\nChatGPT”—in\\nthe\\nsense\\nthat\\nwe’d\\nget\\nsomething\\nthat\\nwould\\ngenerate\\nessay-length\\nsequences\\nof\\nwords\\nwith\\nthe\\n“correct\\noverall\\nessay\\nprobabilities”.\\nBut\\nhere’s\\nthe\\nproblem:\\nthere\\njust\\nisn’t\\neven\\nclose\\nto\\nenough' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 9}\n",
            "\n",
            "Page 11:\n",
            "page_content='English\\ntext\\nthat’s\\never\\nbeen\\nwritten\\nto\\nbe\\nable\\nto\\ndeduce\\nthose\\nprobabilities.\\nIn\\na\\ncrawl\\nof\\nthe\\nweb\\nthere\\nmight\\nbe\\na\\nfew\\nhundred\\nbillion\\nwords;\\nin\\nbooks\\nthat\\nhave\\nbeen\\ndigitized\\nthere\\nmight\\nbe\\nanother\\nhundred\\nbillion\\nwords.\\nBut\\nwith\\n40,000\\ncommon\\nwords,\\neven\\nthe\\nnumber\\nof\\npossible\\n2-grams\\nis\\nalready\\n1.6\\nbillion—and\\nthe\\nnumber\\nof\\npossible\\n3-grams\\nis\\n60\\ntrillion.\\nSo\\nthere’s\\nno\\nway\\nwe\\ncan\\nestimate\\nthe\\nprobabilities\\neven\\nfor\\nall\\nof\\nthese\\nfrom\\ntext\\nthat’s\\nout\\nthere.\\nAnd\\nby\\nthe\\ntime\\nwe\\nget\\nto\\n“essay\\nfragments”\\nof\\n20\\nwords,\\nthe\\nnumber\\nof\\npossibilities\\nis\\nlarger\\nthan\\nthe\\nnumber\\nof\\nparticles\\nin\\nthe\\nuniverse,\\nso\\nin\\na\\nsense\\nthey\\ncould\\nnever\\nall\\nbe\\nwritten\\ndown.\\nSo\\nwhat\\ncan\\nwe\\ndo?\\nThe\\nbig\\nidea\\nis\\nto\\nmake\\na\\nmodel\\nthat\\nlets\\nus\\nestimate\\nthe\\nprobabilities\\nwith\\nwhich\\nsequences\\nshould\\noccur—even\\nthough\\nwe’ve\\nnever\\nexplicitly\\nseen\\nthose\\nsequences\\nin\\nthe\\ncorpus\\nof\\ntext\\nwe’ve\\nlooked\\nat.\\nAnd\\nat\\nthe\\ncore\\nof\\nChatGPT\\nis\\nprecisely\\na\\nso-called\\n“large\\nlanguage\\nmodel”\\n(LLM)\\nthat’s\\nbeen\\nbuilt\\nto\\ndo\\na\\ngood\\njob\\nof\\nestimating\\nthose\\nprobabilities. \\nWhat\\nIs\\na\\nModel?\\nSay\\nyou\\nwant\\nto\\nknow\\n(as\\nGalileo\\ndid\\nback\\nin\\nthe\\nlate\\n1500s\\n)\\nhow\\nlong\\nit’s\\ngoing\\nto\\ntake\\na\\ncannon\\nball\\ndropped\\nfrom\\neach\\nfloor\\nof\\nthe\\nTower\\nof\\nPisa\\nto\\nhit\\nthe\\nground.\\nWell,\\nyou\\ncould\\njust\\nmeasure\\nit\\nin\\neach\\ncase\\nand\\nmake\\na\\ntable\\nof\\nthe\\nresults.\\nOr\\nyou\\ncould\\ndo\\nwhat\\nis\\nthe\\nessence\\nof\\ntheoretical\\nscience:\\nmake\\na\\nmodel\\nthat\\ngives\\nsome\\nkind\\nof\\nprocedure\\nfor\\ncomputing\\nthe\\nanswer\\nrather\\nthan\\njust\\nmeasuring\\nand\\nremembering\\neach\\ncase.\\nLet’s\\nimagine\\nwe\\nhave\\n(somewhat\\nidealized)\\ndata\\nfor\\nhow\\nlong\\nthe\\ncannon\\nball\\ntakes\\nto\\nfall\\nfrom\\nvarious\\nfloors:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 10}\n",
            "\n",
            "Page 12:\n",
            "page_content='How\\ndo\\nwe\\nfigure\\nout\\nhow\\nlong\\nit’s\\ngoing\\nto\\ntake\\nto\\nfall\\nfrom\\na\\nfloor\\nwe\\ndon’t\\nexplicitly\\nhave\\ndata\\nabout?\\nIn\\nthis\\nparticular\\ncase,\\nwe\\ncan\\nuse\\nknown\\nlaws\\nof\\nphysics\\nto\\nwork\\nit\\nout.\\nBut\\nsay\\nall\\nwe’ve\\ngot\\nis\\nthe\\ndata,\\nand\\nwe\\ndon’t\\nknow\\nwhat\\nunderlying\\nlaws\\ngovern\\nit.\\nThen\\nwe\\nmight\\nmake\\na\\nmathematical\\nguess,\\nlike\\nthat\\nperhaps\\nwe\\nshould\\nuse\\na\\nstraight\\nline\\nas\\na\\nmodel:\\nWe\\ncould\\npick\\ndifferent\\nstraight\\nlines.\\nBut\\nthis\\nis\\nthe\\none\\nthat’s\\non\\naverage\\nclosest\\nto\\nthe\\ndata\\nwe’re\\ngiven.\\nAnd\\nfrom\\nthis\\nstraight\\nline\\nwe\\ncan\\nestimate\\nthe\\ntime\\nto\\nfall\\nfor\\nany\\nfloor.\\nHow\\ndid\\nwe\\nknow\\nto\\ntry\\nusing\\na\\nstraight\\nline\\nhere?\\nAt\\nsome\\nlevel\\nwe\\ndidn’t.\\nIt’s\\njust\\nsomething\\nthat’s\\nmathematically\\nsimple,\\nand\\nwe’re\\nused\\nto\\nthe\\nfact\\nthat\\nlots\\nof\\ndata\\nwe\\nmeasure\\nturns\\nout\\nto\\nbe\\nwell\\nfit\\nby\\nmathematically' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 11}\n",
            "\n",
            "Page 13:\n",
            "page_content='simple\\nthings.\\nWe\\ncould\\ntry\\nsomething\\nmathematically\\nmore\\ncomplicated—say\\na\\n+\\nb\\nx\\n+\\nc\\nx\\n2\\n—and\\nthen\\nin\\nthis\\ncase\\nwe\\ndo\\nbetter:\\nThings\\ncan\\ngo\\nquite\\nwrong,\\nthough.\\nLike\\nhere’s\\nthe\\nbest\\nwe\\ncan\\ndo\\nwith\\na\\n+\\nb\\n/\\nx\\n+\\nc\\nsin(\\nx\\n):\\nIt\\nis\\nworth\\nunderstanding\\nthat\\nthere’s\\nnever\\na\\n“model-less\\nmodel”.\\nAny\\nmodel\\nyou\\nuse\\nhas\\nsome\\nparticular\\nunderlying\\nstructure—then\\na\\ncertain\\nset\\nof\\n“knobs\\nyou\\ncan\\nturn”\\n(i.e.\\nparameters\\nyou\\ncan\\nset)\\nto\\nfit\\nyour\\ndata.\\nAnd\\nin\\nthe\\ncase\\nof\\nChatGPT,\\nlots\\nof\\nsuch\\n“knobs”\\nare\\nused—actually,\\n175\\nbillion\\nof\\nthem.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 12}\n",
            "\n",
            "Page 14:\n",
            "page_content='But\\nthe\\nremarkable\\nthing\\nis\\nthat\\nthe\\nunderlying\\nstructure\\nof\\nChatGPT—with\\n“just”\\nthat\\nmany\\nparameters—is\\nsufficient\\nto\\nmake\\na\\nmodel\\nthat\\ncomputes\\nnext-word\\nprobabilities\\n“well\\nenough”\\nto\\ngive\\nus\\nreasonable\\nessay-length\\npieces\\nof\\ntext. \\nModels\\nfor\\nHuman-Like\\nTasks\\nThe\\nexample\\nwe\\ngave\\nabove\\ninvolves\\nmaking\\na\\nmodel\\nfor\\nnumerical\\ndata\\nthat\\nessentially\\ncomes\\nfrom\\nsimple\\nphysics—where\\nwe’ve\\nknown\\nfor\\nseveral\\ncenturies\\nthat\\n“simple\\nmathematics\\napplies”.\\nBut\\nfor\\nChatGPT\\nwe\\nhave\\nto\\nmake\\na\\nmodel\\nof\\nhuman-language\\ntext\\nof\\nthe\\nkind\\nproduced\\nby\\na\\nhuman\\nbrain.\\nAnd\\nfor\\nsomething\\nlike\\nthat\\nwe\\ndon’t\\n(at\\nleast\\nyet)\\nhave\\nanything\\nlike\\n“simple\\nmathematics”.\\nSo\\nwhat\\nmight\\na\\nmodel\\nof\\nit\\nbe\\nlike?\\nBefore\\nwe\\ntalk\\nabout\\nlanguage,\\nlet’s\\ntalk\\nabout\\nanother\\nhuman-like\\ntask:\\nrecognizing\\nimages.\\nAnd\\nas\\na\\nsimple\\nexample\\nof\\nthis,\\nlet’s\\nconsider\\nimages\\nof\\ndigits\\n(and,\\nyes,\\nthis\\nis\\na\\nclassic\\nmachine\\nlearning\\nexample\\n):\\nOne\\nthing\\nwe\\ncould\\ndo\\nis\\nget\\na\\nbunch\\nof\\nsample\\nimages\\nfor\\neach\\ndigit:\\nThen\\nto\\nfind\\nout\\nif\\nan\\nimage\\nwe’re\\ngiven\\nas\\ninput\\ncorresponds\\nto\\na\\nparticular\\ndigit\\nwe\\ncould\\njust\\ndo\\nan\\nexplicit\\npixel-by-pixel\\ncomparison\\nwith\\nthe\\nsamples\\nwe\\nhave.\\nBut\\nas\\nhumans\\nwe\\ncertainly\\nseem\\nto\\ndo\\nsomething\\nbetter—because\\nwe\\ncan\\nstill\\nrecognize\\ndigits,\\neven\\nwhen\\nthey’re\\nfor\\nexample\\nhandwritten,\\nand\\nhave\\nall\\nsorts\\nof\\nmodifications\\nand\\ndistortions:\\nWhen\\nwe\\nmade\\na\\nmodel\\nfor\\nour\\nnumerical\\ndata\\nabove,\\nwe\\nwere\\nable\\nto\\ntake\\na\\nnumerical\\nvalue\\nx\\nthat\\nwe\\nwere\\ngiven,\\nand\\njust\\ncompute\\na\\n+\\nb\\nx\\nfor' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 13}\n",
            "\n",
            "Page 15:\n",
            "page_content='particular\\na\\nand\\nb\\n.\\nSo\\nif\\nwe\\ntreat\\nthe\\ngray-level\\nvalue\\nof\\neach\\npixel\\nhere\\nas\\nsome\\nvariable\\nx\\ni\\nis\\nthere\\nsome\\nfunction\\nof\\nall\\nthose\\nvariables\\nthat—when\\nevaluated—tells\\nus\\nwhat\\ndigit\\nthe\\nimage\\nis\\nof?\\nIt\\nturns\\nout\\nthat\\nit’s\\npossible\\nto\\nconstruct\\nsuch\\na\\nfunction.\\nNot\\nsurprisingly,\\nit’s\\nnot\\nparticularly\\nsimple,\\nthough.\\nAnd\\na\\ntypical\\nexample\\nmight\\ninvolve\\nperhaps\\nhalf\\na\\nmillion\\nmathematical\\noperations.\\nBut\\nthe\\nend\\nresult\\nis\\nthat\\nif\\nwe\\nfeed\\nthe\\ncollection\\nof\\npixel\\nvalues\\nfor\\nan\\nimage\\ninto\\nthis\\nfunction,\\nout\\nwill\\ncome\\nthe\\nnumber\\nspecifying\\nwhich\\ndigit\\nwe\\nhave\\nan\\nimage\\nof.\\nLater,\\nwe’ll\\ntalk\\nabout\\nhow\\nsuch\\na\\nfunction\\ncan\\nbe\\nconstructed,\\nand\\nthe\\nidea\\nof\\nneural\\nnets.\\nBut\\nfor\\nnow\\nlet’s\\ntreat\\nthe\\nfunction\\nas\\nblack\\nbox,\\nwhere\\nwe\\nfeed\\nin\\nimages\\nof,\\nsay,\\nhandwritten\\ndigits\\n(as\\narrays\\nof\\npixel\\nvalues)\\nand\\nwe\\nget\\nout\\nthe\\nnumbers\\nthese\\ncorrespond\\nto:\\nBut\\nwhat’s\\nreally\\ngoing\\non\\nhere?\\nLet’s\\nsay\\nwe\\nprogressively\\nblur\\na\\ndigit.\\nFor\\na\\nlittle\\nwhile\\nour\\nfunction\\nstill\\n“recognizes”\\nit,\\nhere\\nas\\na\\n“2”.\\nBut\\nsoon\\nit\\n“loses\\nit”,\\nand\\nstarts\\ngiving\\nthe\\n“wrong”\\nresult:\\nBut\\nwhy\\ndo\\nwe\\nsay\\nit’s\\nthe\\n“wrong”\\nresult?\\nIn\\nthis\\ncase,\\nwe\\nknow\\nwe\\ngot\\nall\\nthe\\nimages\\nby\\nblurring\\na\\n“2”.\\nBut\\nif\\nour\\ngoal\\nis\\nto\\nproduce\\na\\nmodel\\nof\\nwhat\\nhumans\\ncan\\ndo\\nin\\nrecognizing\\nimages,\\nthe\\nreal\\nquestion\\nto\\nask\\nis\\nwhat\\na\\nhuman\\nwould\\nhave\\ndone\\nif\\npresented\\nwith\\none\\nof\\nthose\\nblurred\\nimages,\\nwithout\\nknowing\\nwhere\\nit\\ncame\\nfrom.\\nAnd\\nwe\\nhave\\na\\n“good\\nmodel”\\nif\\nthe\\nresults\\nwe\\nget\\nfrom\\nour\\nfunction\\ntypically\\nagree\\nwith\\nwhat\\na\\nhuman\\nwould\\nsay.\\nAnd\\nthe\\nnontrivial\\nscientific\\nfact\\nis\\nthat\\nfor\\nan\\nimage-recognition\\ntask\\nlike\\nthis\\nwe\\nnow\\nbasically\\nknow\\nhow\\nto\\nconstruct\\nfunctions\\nthat\\ndo\\nthis.\\nCan\\nwe\\n“mathematically\\nprove”\\nthat\\nthey\\nwork?\\nWell,\\nno.\\nBecause\\nto\\ndo\\nthat\\nwe’d\\nhave\\nto\\nhave\\na\\nmathematical\\ntheory\\nof\\nwhat\\nwe\\nhumans\\nare\\ndoing.\\nTake\\nthe\\n“2”\\nimage\\nand\\nchange\\na\\nfew\\npixels.\\nWe\\nmight\\nimagine\\nthat\\nwith\\nonly\\na\\nfew\\npixels\\n“out\\nof\\nplace”\\nwe\\nshould\\nstill\\nconsider\\nthe\\nimage\\na\\n“2”.\\nBut\\nhow\\nfar\\nshould\\nthat\\ngo?\\nIt’s\\na\\nquestion\\nof\\nhuman\\nvisual\\nperception\\n.\\nAnd,' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 14}\n",
            "\n",
            "Page 16:\n",
            "page_content='yes,\\nthe\\nanswer\\nwould\\nno\\ndoubt\\nbe\\ndifferent\\nfor\\nbees\\nor\\noctopuses—and\\npotentially\\nutterly\\ndifferent\\nfor\\nputative\\naliens\\n. \\nNeural\\nNets\\nOK,\\nso\\nhow\\ndo\\nour\\ntypical\\nmodels\\nfor\\ntasks\\nlike\\nimage\\nrecognition\\nactually\\nwork?\\nThe\\nmost\\npopular—and\\nsuccessful—current\\napproach\\nuses\\nneural\\nnets\\n.\\nInvented—in\\na\\nform\\nremarkably\\nclose\\nto\\ntheir\\nuse\\ntoday—\\nin\\nthe\\n1940s\\n,\\nneural\\nnets\\ncan\\nbe\\nthought\\nof\\nas\\nsimple\\nidealizations\\nof\\nhow\\nbrains\\nseem\\nto\\nwork\\n.\\nIn\\nhuman\\nbrains\\nthere\\nare\\nabout\\n100\\nbillion\\nneurons\\n(nerve\\ncells),\\neach\\ncapable\\nof\\nproducing\\nan\\nelectrical\\npulse\\nup\\nto\\nperhaps\\na\\nthousand\\ntimes\\na\\nsecond.\\nThe\\nneurons\\nare\\nconnected\\nin\\na\\ncomplicated\\nnet,\\nwith\\neach\\nneuron\\nhaving\\ntree-like\\nbranches\\nallowing\\nit\\nto\\npass\\nelectrical\\nsignals\\nto\\nperhaps\\nthousands\\nof\\nother\\nneurons.\\nAnd\\nin\\na\\nrough\\napproximation,\\nwhether\\nany\\ngiven\\nneuron\\nproduces\\nan\\nelectrical\\npulse\\nat\\na\\ngiven\\nmoment\\ndepends\\non\\nwhat\\npulses\\nit’s\\nreceived\\nfrom\\nother\\nneurons—with\\ndifferent\\nconnections\\ncontributing\\nwith\\ndifferent\\n“weights”.\\nWhen\\nwe\\n“see\\nan\\nimage”\\nwhat’s\\nhappening\\nis\\nthat\\nwhen\\nphotons\\nof\\nlight\\nfrom\\nthe\\nimage\\nfall\\non\\n(“photoreceptor”)\\ncells\\nat\\nthe\\nback\\nof\\nour\\neyes\\nthey\\nproduce\\nelectrical\\nsignals\\nin\\nnerve\\ncells.\\nThese\\nnerve\\ncells\\nare\\nconnected\\nto\\nother\\nnerve\\ncells,\\nand\\neventually\\nthe\\nsignals\\ngo\\nthrough\\na\\nwhole\\nsequence\\nof\\nlayers\\nof\\nneurons.\\nAnd\\nit’s\\nin\\nthis\\nprocess\\nthat\\nwe\\n“recognize”\\nthe\\nimage,\\neventually\\n“forming\\nthe\\nthought”\\nthat\\nwe’re\\n“seeing\\na\\n2”\\n(and\\nmaybe\\nin\\nthe\\nend\\ndoing\\nsomething\\nlike\\nsaying\\nthe\\nword\\n“two”\\nout\\nloud).\\nThe\\n“black-box”\\nfunction\\nfrom\\nthe\\nprevious\\nsection\\nis\\na\\n“mathematicized”\\nversion\\nof\\nsuch\\na\\nneural\\nnet.\\nIt\\nhappens\\nto\\nhave\\n11\\nlayers\\n(though\\nonly\\n4\\n“core\\nlayers”):' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 15}\n",
            "\n",
            "Page 17:\n",
            "page_content='There’s\\nnothing\\nparticularly\\n“theoretically\\nderived”\\nabout\\nthis\\nneural\\nnet;\\nit’s\\njust\\nsomething\\nthat—\\nback\\nin\\n1998—was\\nconstructed\\nas\\na\\npiece\\nof\\nengineering\\n,\\nand\\nfound\\nto\\nwork.\\n(Of\\ncourse,\\nthat’s\\nnot\\nmuch\\ndifferent\\nfrom\\nhow\\nwe\\nmight\\ndescribe\\nour\\nbrains\\nas\\nhaving\\nbeen\\nproduced\\nthrough\\nthe\\nprocess\\nof\\nbiological\\nevolution.)\\nOK,\\nbut\\nhow\\ndoes\\na\\nneural\\nnet\\nlike\\nthis\\n“recognize\\nthings”?\\nThe\\nkey\\nis\\nthe\\nnotion\\nof\\nattractors\\n.\\nImagine\\nwe’ve\\ngot\\nhandwritten\\nimages\\nof\\n1’s\\nand\\n2’s:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 16}\n",
            "\n",
            "Page 18:\n",
            "page_content='We\\nsomehow\\nwant\\nall\\nthe\\n1’s\\nto\\n“be\\nattracted\\nto\\none\\nplace”,\\nand\\nall\\nthe\\n2’s\\nto\\n“be\\nattracted\\nto\\nanother\\nplace”.\\nOr,\\nput\\na\\ndifferent\\nway,\\nif\\nan\\nimage\\nis\\nsomehow\\n“\\ncloser\\nto\\nbeing\\na\\n1\\n”\\nthan\\nto\\nbeing\\na\\n2,\\nwe\\nwant\\nit\\nto\\nend\\nup\\nin\\nthe\\n“1\\nplace”\\nand\\nvice\\nversa.\\nAs\\na\\nstraightforward\\nanalogy,\\nlet’s\\nsay\\nwe\\nhave\\ncertain\\npositions\\nin\\nthe\\nplane,\\nindicated\\nby\\ndots\\n(in\\na\\nreal-life\\nsetting\\nthey\\nmight\\nbe\\npositions\\nof\\ncoffee\\nshops).\\nThen\\nwe\\nmight\\nimagine\\nthat\\nstarting\\nfrom\\nany\\npoint\\non\\nthe\\nplane\\nwe’d\\nalways\\nwant\\nto\\nend\\nup\\nat\\nthe\\nclosest\\ndot\\n(i.e.\\nwe’d\\nalways\\ngo\\nto\\nthe\\nclosest\\ncoffee\\nshop).\\nWe\\ncan\\nrepresent\\nthis\\nby\\ndividing\\nthe\\nplane\\ninto\\nregions\\n(“attractor\\nbasins”)\\nseparated\\nby\\nidealized\\n“watersheds”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 17}\n",
            "\n",
            "Page 19:\n",
            "page_content='We\\ncan\\nthink\\nof\\nthis\\nas\\nimplementing\\na\\nkind\\nof\\n“recognition\\ntask”\\nin\\nwhich\\nwe’re\\nnot\\ndoing\\nsomething\\nlike\\nidentifying\\nwhat\\ndigit\\na\\ngiven\\nimage\\n“looks\\nmost\\nlike”—but\\nrather\\nwe’re\\njust,\\nquite\\ndirectly,\\nseeing\\nwhat\\ndot\\na\\ngiven\\npoint\\nis\\nclosest\\nto.\\n(The\\n“Voronoi\\ndiagram”\\nsetup\\nwe’re\\nshowing\\nhere\\nseparates\\npoints\\nin\\n2D\\nEuclidean\\nspace;\\nthe\\ndigit\\nrecognition\\ntask\\ncan\\nbe\\nthought\\nof\\nas\\ndoing\\nsomething\\nvery\\nsimilar—but\\nin\\na\\n784-dimensional\\nspace\\nformed\\nfrom\\nthe\\ngray\\nlevels\\nof\\nall\\nthe\\npixels\\nin\\neach\\nimage.)\\nSo\\nhow\\ndo\\nwe\\nmake\\na\\nneural\\nnet\\n“do\\na\\nrecognition\\ntask”?\\nLet’s\\nconsider\\nthis\\nvery\\nsimple\\ncase:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 18}\n",
            "\n",
            "Page 20:\n",
            "page_content='Our\\ngoal\\nis\\nto\\ntake\\nan\\n“input”\\ncorresponding\\nto\\na\\nposition\\n{\\nx\\n,\\ny\\n}—and\\nthen\\nto\\n“recognize”\\nit\\nas\\nwhichever\\nof\\nthe\\nthree\\npoints\\nit’s\\nclosest\\nto.\\nOr,\\nin\\nother\\nwords,\\nwe\\nwant\\nthe\\nneural\\nnet\\nto\\ncompute\\na\\nfunction\\nof\\n{\\nx\\n,\\ny\\n}\\nlike:\\nSo\\nhow\\ndo\\nwe\\ndo\\nthis\\nwith\\na\\nneural\\nnet?\\nUltimately\\na\\nneural\\nnet\\nis\\na\\nconnected\\ncollection\\nof\\nidealized\\n“neurons”—usually\\narranged\\nin\\nlayers—with\\na\\nsimple\\nexample\\nbeing:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 19}\n",
            "\n",
            "Page 21:\n",
            "page_content='Each\\n“neuron”\\nis\\neffectively\\nset\\nup\\nto\\nevaluate\\na\\nsimple\\nnumerical\\nfunction.\\nAnd\\nto\\n“use”\\nthe\\nnetwork,\\nwe\\nsimply\\nfeed\\nnumbers\\n(like\\nour\\ncoordinates\\nx\\nand\\ny\\n)\\nin\\nat\\nthe\\ntop,\\nthen\\nhave\\nneurons\\non\\neach\\nlayer\\n“evaluate\\ntheir\\nfunctions”\\nand\\nfeed\\nthe\\nresults\\nforward\\nthrough\\nthe\\nnetwork—eventually\\nproducing\\nthe\\nfinal\\nresult\\nat\\nthe\\nbottom:\\nIn\\nthe\\ntraditional\\n(biologically\\ninspired)\\nsetup\\neach\\nneuron\\neffectively\\nhas\\na\\ncertain\\nset\\nof\\n“incoming\\nconnections”\\nfrom\\nthe\\nneurons\\non\\nthe\\nprevious\\nlayer,\\nwith\\neach\\nconnection\\nbeing\\nassigned\\na\\ncertain\\n“weight”\\n(which\\ncan\\nbe\\na\\npositive\\nor\\nnegative\\nnumber).\\nThe\\nvalue\\nof\\na\\ngiven\\nneuron\\nis\\ndetermined\\nby\\nmultiplying\\nthe\\nvalues\\nof\\n“previous\\nneurons”\\nby\\ntheir\\ncorresponding\\nweights,\\nthen\\nadding\\nthese\\nup\\nand\\nadding\\na\\nconstant—and\\nfinally\\napplying\\na\\n“thresholding”\\n(or\\n“activation”)\\nfunction.\\nIn\\nmathematical\\nterms,\\nif\\na\\nneuron\\nhas\\ninputs\\nx\\n=\\n{\\nx\\n1\\n,\\nx\\n2\\n…}\\nthen\\nwe\\ncompute\\nf\\n[\\nw\\n.\\nx\\n+\\nb\\n],\\nwhere\\nthe\\nweights\\nw\\nand\\nconstant\\nb\\nare\\ngenerally\\nchosen\\ndifferently\\nfor\\neach\\nneuron\\nin\\nthe\\nnetwork;\\nthe\\nfunction\\nf\\nis\\nusually\\nthe\\nsame.\\nComputing\\nw\\n.\\nx\\n+\\nb\\nis\\njust\\na\\nmatter\\nof\\nmatrix\\nmultiplication\\nand\\naddition.\\nThe\\n“activation\\nfunction”\\nf\\nintroduces\\nnonlinearity\\n(and\\nultimately\\nis\\nwhat' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 20}\n",
            "\n",
            "Page 22:\n",
            "page_content='leads\\nto\\nnontrivial\\nbehavior).\\nVarious\\nactivation\\nfunctions\\ncommonly\\nget\\nused;\\nhere\\nwe’ll\\njust\\nuse\\nRamp\\n(or\\nReLU):\\nFor\\neach\\ntask\\nwe\\nwant\\nthe\\nneural\\nnet\\nto\\nperform\\n(or,\\nequivalently,\\nfor\\neach\\noverall\\nfunction\\nwe\\nwant\\nit\\nto\\nevaluate)\\nwe’ll\\nhave\\ndifferent\\nchoices\\nof\\nweights.\\n(And—as\\nwe’ll\\ndiscuss\\nlater—these\\nweights\\nare\\nnormally\\ndetermined\\nby\\n“training”\\nthe\\nneural\\nnet\\nusing\\nmachine\\nlearning\\nfrom\\nexamples\\nof\\nthe\\noutputs\\nwe\\nwant.)\\nUltimately,\\nevery\\nneural\\nnet\\njust\\ncorresponds\\nto\\nsome\\noverall\\nmathematical\\nfunction—though\\nit\\nmay\\nbe\\nmessy\\nto\\nwrite\\nout.\\nFor\\nthe\\nexample\\nabove,\\nit\\nwould\\nbe:\\nThe\\nneural\\nnet\\nof\\nChatGPT\\nalso\\njust\\ncorresponds\\nto\\na\\nmathematical\\nfunction\\nlike\\nthis—but\\neffectively\\nwith\\nbillions\\nof\\nterms.\\nBut\\nlet’s\\ngo\\nback\\nto\\nindividual\\nneurons.\\nHere\\nare\\nsome\\nexamples\\nof\\nthe\\nfunctions\\na\\nneuron\\nwith\\ntwo\\ninputs\\n(representing\\ncoordinates\\nx\\nand\\ny\\n)\\ncan\\ncompute\\nwith\\nvarious\\nchoices\\nof\\nweights\\nand\\nconstants\\n(and\\nRamp\\nas\\nactivation\\nfunction):\\nBut\\nwhat\\nabout\\nthe\\nlarger\\nnetwork\\nfrom\\nabove?\\nWell,\\nhere’s\\nwhat\\nit\\ncomputes:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 21}\n",
            "\n",
            "Page 23:\n",
            "page_content='It’s\\nnot\\nquite\\n“right”,\\nbut\\nit’s\\nclose\\nto\\nthe\\n“nearest\\npoint”\\nfunction\\nwe\\nshowed\\nabove.\\nLet’s\\nsee\\nwhat\\nhappens\\nwith\\nsome\\nother\\nneural\\nnets.\\nIn\\neach\\ncase,\\nas\\nwe’ll\\nexplain\\nlater,\\nwe’re\\nusing\\nmachine\\nlearning\\nto\\nfind\\nthe\\nbest\\nchoice\\nof\\nweights.\\nThen\\nwe’re\\nshowing\\nhere\\nwhat\\nthe\\nneural\\nnet\\nwith\\nthose\\nweights\\ncomputes:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 22}\n",
            "\n",
            "Page 24:\n",
            "page_content='Bigger\\nnetworks\\ngenerally\\ndo\\nbetter\\nat\\napproximating\\nthe\\nfunction\\nwe’re\\naiming\\nfor.\\nAnd\\nin\\nthe\\n“middle\\nof\\neach\\nattractor\\nbasin”\\nwe\\ntypically\\nget\\nexactly\\nthe\\nanswer\\nwe\\nwant.\\nBut\\nat\\nthe\\nboundaries\\n—where\\nthe\\nneural\\nnet\\n“has\\na\\nhard\\ntime\\nmaking\\nup\\nits\\nmind”—things\\ncan\\nbe\\nmessier.\\nWith\\nthis\\nsimple\\nmathematical-style\\n“recognition\\ntask”\\nit’s\\nclear\\nwhat\\nthe\\n“right\\nanswer”\\nis.\\nBut\\nin\\nthe\\nproblem\\nof\\nrecognizing\\nhandwritten\\ndigits,\\nit’s\\nnot\\nso\\nclear.\\nWhat\\nif\\nsomeone\\nwrote\\na\\n“2”\\nso\\nbadly\\nit\\nlooked\\nlike\\na\\n“7”,\\netc.?\\nStill,\\nwe\\ncan\\nask\\nhow\\na\\nneural\\nnet\\ndistinguishes\\ndigits—and\\nthis\\ngives\\nan\\nindication:\\nCan\\nwe\\nsay\\n“mathematically”\\nhow\\nthe\\nnetwork\\nmakes\\nits\\ndistinctions?\\nNot\\nreally.\\nIt’s\\njust\\n“doing\\nwhat\\nthe\\nneural\\nnet\\ndoes”.\\nBut\\nit\\nturns\\nout\\nthat\\nthat\\nnormally\\nseems\\nto\\nagree\\nfairly\\nwell\\nwith\\nthe\\ndistinctions\\nwe\\nhumans\\nmake.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 23}\n",
            "\n",
            "Page 25:\n",
            "page_content='Let’s\\ntake\\na\\nmore\\nelaborate\\nexample.\\nLet’s\\nsay\\nwe\\nhave\\nimages\\nof\\ncats\\nand\\ndogs.\\nAnd\\nwe\\nhave\\na\\nneural\\nnet\\nthat’s\\nbeen\\ntrained\\nto\\ndistinguish\\nthem\\n.\\nHere’s\\nwhat\\nit\\nmight\\ndo\\non\\nsome\\nexamples:\\nNow\\nit’s\\neven\\nless\\nclear\\nwhat\\nthe\\n“right\\nanswer”\\nis.\\nWhat\\nabout\\na\\ndog\\ndressed\\nin\\na\\ncat\\nsuit?\\nEtc.\\nWhatever\\ninput\\nit’s\\ngiven\\nthe\\nneural\\nnet\\nwill\\ngenerate\\nan\\nanswer,\\nand\\nin\\na\\nway\\nreasonably\\nconsistent\\nwith\\nhow\\nhumans\\nmight.\\nAs\\nI’ve\\nsaid\\nabove,\\nthat’s\\nnot\\na\\nfact\\nwe\\ncan\\n“derive\\nfrom\\nfirst\\nprinciples”.\\nIt’s\\njust\\nsomething\\nthat’s\\nempirically\\nbeen\\nfound\\nto\\nbe\\ntrue,\\nat\\nleast\\nin\\ncertain\\ndomains.\\nBut\\nit’s\\na\\nkey\\nreason\\nwhy\\nneural\\nnets\\nare\\nuseful:\\nthat\\nthey\\nsomehow\\ncapture\\na\\n“human-like”\\nway\\nof\\ndoing\\nthings.\\nShow\\nyourself\\na\\npicture\\nof\\na\\ncat,\\nand\\nask\\n“Why\\nis\\nthat\\na\\ncat?”.\\nMaybe\\nyou’d\\nstart\\nsaying\\n“Well,\\nI\\nsee\\nits\\npointy\\nears,\\netc.”\\nBut\\nit’s\\nnot\\nvery\\neasy\\nto\\nexplain' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 24}\n",
            "\n",
            "Page 26:\n",
            "page_content='how\\nyou\\nrecognized\\nthe\\nimage\\nas\\na\\ncat.\\nIt’s\\njust\\nthat\\nsomehow\\nyour\\nbrain\\nfigured\\nthat\\nout.\\nBut\\nfor\\na\\nbrain\\nthere’s\\nno\\nway\\n(at\\nleast\\nyet)\\nto\\n“go\\ninside”\\nand\\nsee\\nhow\\nit\\nfigured\\nit\\nout.\\nWhat\\nabout\\nfor\\nan\\n(artificial)\\nneural\\nnet?\\nWell,\\nit’s\\nstraightforward\\nto\\nsee\\nwhat\\neach\\n“neuron”\\ndoes\\nwhen\\nyou\\nshow\\na\\npicture\\nof\\na\\ncat.\\nBut\\neven\\nto\\nget\\na\\nbasic\\nvisualization\\nis\\nusually\\nvery\\ndifficult.\\nIn\\nthe\\nfinal\\nnet\\nthat\\nwe\\nused\\nfor\\nthe\\n“nearest\\npoint”\\nproblem\\nabove\\nthere\\nare\\n17\\nneurons.\\nIn\\nthe\\nnet\\nfor\\nrecognizing\\nhandwritten\\ndigits\\nthere\\nare\\n2190.\\nAnd\\nin\\nthe\\nnet\\nwe’re\\nusing\\nto\\nrecognize\\ncats\\nand\\ndogs\\nthere\\nare\\n60,650.\\nNormally\\nit\\nwould\\nbe\\npretty\\ndifficult\\nto\\nvisualize\\nwhat\\namounts\\nto\\n60,650-dimensional\\nspace.\\nBut\\nbecause\\nthis\\nis\\na\\nnetwork\\nset\\nup\\nto\\ndeal\\nwith\\nimages,\\nmany\\nof\\nits\\nlayers\\nof\\nneurons\\nare\\norganized\\ninto\\narrays,\\nlike\\nthe\\narrays\\nof\\npixels\\nit’s\\nlooking\\nat.\\nAnd\\nif\\nwe\\ntake\\na\\ntypical\\ncat\\nimage\\nthen\\nwe\\ncan\\nrepresent\\nthe\\nstates\\nof\\nneurons\\nat\\nthe\\nfirst\\nlayer\\nby\\na\\ncollection\\nof\\nderived\\nimages—many\\nof\\nwhich\\nwe\\ncan\\nreadily\\ninterpret\\nas\\nbeing\\nthings\\nlike\\n“the\\ncat\\nwithout\\nits\\nbackground”,\\nor\\n“the\\noutline\\nof\\nthe\\ncat”:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 25}\n",
            "\n",
            "Page 27:\n",
            "page_content='By\\nthe\\n10th\\nlayer\\nit’s\\nharder\\nto\\ninterpret\\nwhat’s\\ngoing\\non:\\nBut\\nin\\ngeneral\\nwe\\nmight\\nsay\\nthat\\nthe\\nneural\\nnet\\nis\\n“picking\\nout\\ncertain\\nfeatures”\\n(maybe\\npointy\\nears\\nare\\namong\\nthem),\\nand\\nusing\\nthese\\nto\\ndetermine\\nwhat\\nthe\\nimage\\nis\\nof.\\nBut\\nare\\nthose\\nfeatures\\nones\\nfor\\nwhich\\nwe\\nhave\\nnames—like\\n“pointy\\nears”?\\nMostly\\nnot.\\nAre\\nour\\nbrains\\nusing\\nsimilar\\nfeatures?\\nMostly\\nwe\\ndon’t\\nknow.\\nBut\\nit’s\\nnotable\\nthat\\nthe\\nfirst\\nfew\\nlayers\\nof\\na\\nneural\\nnet\\nlike\\nthe\\none\\nwe’re\\nshowing\\nhere\\nseem\\nto\\npick\\nout\\naspects\\nof\\nimages\\n(like\\nedges\\nof\\nobjects)\\nthat\\nseem\\nto' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 26}\n",
            "\n",
            "Page 28:\n",
            "page_content='be\\nsimilar\\nto\\nones\\nwe\\nknow\\nare\\npicked\\nout\\nby\\nthe\\nfirst\\nlevel\\nof\\nvisual\\nprocessing\\nin\\nbrains.\\nBut\\nlet’s\\nsay\\nwe\\nwant\\na\\n“theory\\nof\\ncat\\nrecognition”\\nin\\nneural\\nnets.\\nWe\\ncan\\nsay:\\n“Look,\\nthis\\nparticular\\nnet\\ndoes\\nit”—and\\nimmediately\\nthat\\ngives\\nus\\nsome\\nsense\\nof\\n“how\\nhard\\na\\nproblem”\\nit\\nis\\n(and,\\nfor\\nexample,\\nhow\\nmany\\nneurons\\nor\\nlayers\\nmight\\nbe\\nneeded).\\nBut\\nat\\nleast\\nas\\nof\\nnow\\nwe\\ndon’t\\nhave\\na\\nway\\nto\\n“give\\na\\nnarrative\\ndescription”\\nof\\nwhat\\nthe\\nnetwork\\nis\\ndoing.\\nAnd\\nmaybe\\nthat’s\\nbecause\\nit\\ntruly\\nis\\ncomputationally\\nirreducible,\\nand\\nthere’s\\nno\\ngeneral\\nway\\nto\\nfind\\nwhat\\nit\\ndoes\\nexcept\\nby\\nexplicitly\\ntracing\\neach\\nstep.\\nOr\\nmaybe\\nit’s\\njust\\nthat\\nwe\\nhaven’t\\n“figured\\nout\\nthe\\nscience”,\\nand\\nidentified\\nthe\\n“natural\\nlaws”\\nthat\\nallow\\nus\\nto\\nsummarize\\nwhat’s\\ngoing\\non.\\nWe’ll\\nencounter\\nthe\\nsame\\nkinds\\nof\\nissues\\nwhen\\nwe\\ntalk\\nabout\\ngenerating\\nlanguage\\nwith\\nChatGPT.\\nAnd\\nagain\\nit’s\\nnot\\nclear\\nwhether\\nthere\\nare\\nways\\nto\\n“summarize\\nwhat\\nit’s\\ndoing”.\\nBut\\nthe\\nrichness\\nand\\ndetail\\nof\\nlanguage\\n(and\\nour\\nexperience\\nwith\\nit)\\nmay\\nallow\\nus\\nto\\nget\\nfurther\\nthan\\nwith\\nimages. \\nMachine\\nLearning,\\nand\\nthe\\nTraining\\nof\\nNeural\\nNets\\nWe’ve\\nbeen\\ntalking\\nso\\nfar\\nabout\\nneural\\nnets\\nthat\\n“already\\nknow”\\nhow\\nto\\ndo\\nparticular\\ntasks.\\nBut\\nwhat\\nmakes\\nneural\\nnets\\nso\\nuseful\\n(presumably\\nalso\\nin\\nbrains)\\nis\\nthat\\nnot\\nonly\\ncan\\nthey\\nin\\nprinciple\\ndo\\nall\\nsorts\\nof\\ntasks,\\nbut\\nthey\\ncan\\nbe\\nincrementally\\n“trained\\nfrom\\nexamples”\\nto\\ndo\\nthose\\ntasks.\\nWhen\\nwe\\nmake\\na\\nneural\\nnet\\nto\\ndistinguish\\ncats\\nfrom\\ndogs\\nwe\\ndon’t\\neffectively\\nhave\\nto\\nwrite\\na\\nprogram\\nthat\\n(say)\\nexplicitly\\nfinds\\nwhiskers;\\ninstead\\nwe\\njust\\nshow\\nlots\\nof\\nexamples\\nof\\nwhat’s\\na\\ncat\\nand\\nwhat’s\\na\\ndog,\\nand\\nthen\\nhave\\nthe\\nnetwork\\n“machine\\nlearn”\\nfrom\\nthese\\nhow\\nto\\ndistinguish\\nthem.\\nAnd\\nthe\\npoint\\nis\\nthat\\nthe\\ntrained\\nnetwork\\n“generalizes”\\nfrom\\nthe\\nparticular\\nexamples\\nit’s\\nshown.\\nJust\\nas\\nwe’ve\\nseen\\nabove,\\nit\\nisn’t\\nsimply\\nthat\\nthe\\nnetwork\\nrecognizes\\nthe\\nparticular\\npixel\\npattern\\nof\\nan\\nexample\\ncat\\nimage\\nit\\nwas\\nshown;\\nrather\\nit’s\\nthat\\nthe\\nneural\\nnet\\nsomehow\\nmanages\\nto\\ndistinguish\\nimages\\non\\nthe\\nbasis\\nof\\nwhat\\nwe\\nconsider\\nto\\nbe\\nsome\\nkind\\nof\\n“general\\ncatness”.\\nSo\\nhow\\ndoes\\nneural\\nnet\\ntraining\\nactually\\nwork?\\nEssentially\\nwhat\\nwe’re\\nalways\\ntrying\\nto\\ndo\\nis\\nto\\nfind\\nweights\\nthat\\nmake\\nthe\\nneural\\nnet\\nsuccessfully\\nreproduce\\nthe\\nexamples\\nwe’ve\\ngiven.\\nAnd\\nthen\\nwe’re\\nrelying\\non\\nthe\\nneural' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 27}\n",
            "\n",
            "Page 29:\n",
            "page_content='net\\nto\\n“interpolate”\\n(or\\n“generalize”)\\n“between”\\nthese\\nexamples\\nin\\na\\n“reasonable”\\nway.\\nLet’s\\nlook\\nat\\na\\nproblem\\neven\\nsimpler\\nthan\\nthe\\nnearest-point\\none\\nabove.\\nLet’s\\njust\\ntry\\nto\\nget\\na\\nneural\\nnet\\nto\\nlearn\\nthe\\nfunction:\\nFor\\nthis\\ntask,\\nwe’ll\\nneed\\na\\nnetwork\\nthat\\nhas\\njust\\none\\ninput\\nand\\none\\noutput,\\nlike:\\nBut\\nwhat\\nweights,\\netc.\\nshould\\nwe\\nbe\\nusing?\\nWith\\nevery\\npossible\\nset\\nof\\nweights\\nthe\\nneural\\nnet\\nwill\\ncompute\\nsome\\nfunction.\\nAnd,\\nfor\\nexample,\\nhere’s\\nwhat\\nit\\ndoes\\nwith\\na\\nfew\\nrandomly\\nchosen\\nsets\\nof\\nweights:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 28}\n",
            "\n",
            "Page 30:\n",
            "page_content='And,\\nyes,\\nwe\\ncan\\nplainly\\nsee\\nthat\\nin\\nnone\\nof\\nthese\\ncases\\ndoes\\nit\\nget\\neven\\nclose\\nto\\nreproducing\\nthe\\nfunction\\nwe\\nwant.\\nSo\\nhow\\ndo\\nwe\\nfind\\nweights\\nthat\\nwill\\nreproduce\\nthe\\nfunction?\\nThe\\nbasic\\nidea\\nis\\nto\\nsupply\\nlots\\nof\\n“input\\n→\\noutput”\\nexamples\\nto\\n“learn\\nfrom”—and\\nthen\\nto\\ntry\\nto\\nfind\\nweights\\nthat\\nwill\\nreproduce\\nthese\\nexamples.\\nHere’s\\nthe\\nresult\\nof\\ndoing\\nthat\\nwith\\nprogressively\\nmore\\nexamples:\\nAt\\neach\\nstage\\nin\\nthis\\n“training”\\nthe\\nweights\\nin\\nthe\\nnetwork\\nare\\nprogressively\\nadjusted—and\\nwe\\nsee\\nthat\\neventually\\nwe\\nget\\na\\nnetwork\\nthat\\nsuccessfully\\nreproduces\\nthe\\nfunction\\nwe\\nwant.\\nSo\\nhow\\ndo\\nwe\\nadjust\\nthe\\nweights?\\nThe\\nbasic\\nidea\\nis\\nat\\neach\\nstage\\nto\\nsee\\n“how\\nfar\\naway\\nwe\\nare”\\nfrom\\ngetting\\nthe\\nfunction\\nwe\\nwant—and\\nthen\\nto\\nupdate\\nthe\\nweights\\nin\\nsuch\\na\\nway\\nas\\nto\\nget\\ncloser.\\nTo\\nfind\\nout\\n“how\\nfar\\naway\\nwe\\nare”\\nwe\\ncompute\\nwhat’s\\nusually\\ncalled\\na\\n“loss\\nfunction”\\n(or\\nsometimes\\n“cost\\nfunction”).\\nHere\\nwe’re\\nusing\\na\\nsimple\\n(L2)\\nloss\\nfunction\\nthat’s\\njust\\nthe\\nsum\\nof\\nthe\\nsquares\\nof\\nthe\\ndifferences\\nbetween' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 29}\n",
            "\n",
            "Page 31:\n",
            "page_content='the\\nvalues\\nwe\\nget,\\nand\\nthe\\ntrue\\nvalues.\\nAnd\\nwhat\\nwe\\nsee\\nis\\nthat\\nas\\nour\\ntraining\\nprocess\\nprogresses,\\nthe\\nloss\\nfunction\\nprogressively\\ndecreases\\n(following\\na\\ncertain\\n“learning\\ncurve”\\nthat’s\\ndifferent\\nfor\\ndifferent\\ntasks)—until\\nwe\\nreach\\na\\npoint\\nwhere\\nthe\\nnetwork\\n(at\\nleast\\nto\\na\\ngood\\napproximation)\\nsuccessfully\\nreproduces\\nthe\\nfunction\\nwe\\nwant:\\nAlright,\\nso\\nthe\\nlast\\nessential\\npiece\\nto\\nexplain\\nis\\nhow\\nthe\\nweights\\nare\\nadjusted\\nto\\nreduce\\nthe\\nloss\\nfunction.\\nAs\\nwe’ve\\nsaid,\\nthe\\nloss\\nfunction\\ngives\\nus\\na\\n“distance”\\nbetween\\nthe\\nvalues\\nwe’ve\\ngot,\\nand\\nthe\\ntrue\\nvalues.\\nBut\\nthe\\n“values\\nwe’ve\\ngot”\\nare\\ndetermined\\nat\\neach\\nstage\\nby\\nthe\\ncurrent\\nversion\\nof\\nneural\\nnet—and\\nby\\nthe\\nweights\\nin\\nit.\\nBut\\nnow\\nimagine\\nthat\\nthe\\nweights\\nare\\nvariables—say\\nw\\ni\\n.\\nWe\\nwant\\nto\\nfind\\nout\\nhow\\nto\\nadjust\\nthe\\nvalues\\nof\\nthese\\nvariables\\nto\\nminimize\\nthe\\nloss\\nthat\\ndepends\\non\\nthem.\\nFor\\nexample,\\nimagine\\n(in\\nan\\nincredible\\nsimplification\\nof\\ntypical\\nneural\\nnets\\nused\\nin\\npractice)\\nthat\\nwe\\nhave\\njust\\ntwo\\nweights\\nw\\n1\\nand\\nw\\n2\\n.\\nThen\\nwe\\nmight\\nhave\\na\\nloss\\nthat\\nas\\na\\nfunction\\nof\\nw\\n1\\nand\\nw\\n2\\nlooks\\nlike\\nthis:\\nNumerical\\nanalysis\\nprovides\\na\\nvariety\\nof\\ntechniques\\nfor\\nfinding\\nthe\\nminimum\\nin\\ncases\\nlike\\nthis.\\nBut\\na\\ntypical\\napproach\\nis\\njust\\nto\\nprogressively\\nfollow\\nthe\\npath\\nof\\nsteepest\\ndescent\\nfrom\\nwhatever\\nprevious\\nw\\n1\\n,\\nw\\n2\\nwe\\nhad:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 30}\n",
            "\n",
            "Page 32:\n",
            "page_content='Like\\nwater\\nflowing\\ndown\\na\\nmountain,\\nall\\nthat’s\\nguaranteed\\nis\\nthat\\nthis\\nprocedure\\nwill\\nend\\nup\\nat\\nsome\\nlocal\\nminimum\\nof\\nthe\\nsurface\\n(“a\\nmountain\\nlake”);\\nit\\nmight\\nwell\\nnot\\nreach\\nthe\\nultimate\\nglobal\\nminimum.\\nIt’s\\nnot\\nobvious\\nthat\\nit\\nwould\\nbe\\nfeasible\\nto\\nfind\\nthe\\npath\\nof\\nthe\\nsteepest\\ndescent\\non\\nthe\\n“weight\\nlandscape”.\\nBut\\ncalculus\\ncomes\\nto\\nthe\\nrescue.\\nAs\\nwe\\nmentioned\\nabove,\\none\\ncan\\nalways\\nthink\\nof\\na\\nneural\\nnet\\nas\\ncomputing\\na\\nmathematical\\nfunction—that\\ndepends\\non\\nits\\ninputs,\\nand\\nits\\nweights.\\nBut\\nnow\\nconsider\\ndifferentiating\\nwith\\nrespect\\nto\\nthese\\nweights.\\nIt\\nturns\\nout\\nthat\\nthe\\nchain\\nrule\\nof\\ncalculus\\nin\\neffect\\nlets\\nus\\n“unravel”\\nthe\\noperations\\ndone\\nby\\nsuccessive\\nlayers\\nin\\nthe\\nneural\\nnet.\\nAnd\\nthe\\nresult\\nis\\nthat\\nwe\\ncan—at\\nleast\\nin\\nsome\\nlocal\\napproximation—“invert”\\nthe\\noperation\\nof\\nthe\\nneural\\nnet,\\nand\\nprogressively\\nfind\\nweights\\nthat\\nminimize\\nthe\\nloss\\nassociated\\nwith\\nthe\\noutput.\\nThe\\npicture\\nabove\\nshows\\nthe\\nkind\\nof\\nminimization\\nwe\\nmight\\nneed\\nto\\ndo\\nin\\nthe\\nunrealistically\\nsimple\\ncase\\nof\\njust\\n2\\nweights.\\nBut\\nit\\nturns\\nout\\nthat\\neven\\nwith\\nmany\\nmore\\nweights\\n(ChatGPT\\nuses\\n175\\nbillion)\\nit’s\\nstill\\npossible\\nto\\ndo\\nthe\\nminimization,\\nat\\nleast\\nto\\nsome\\nlevel\\nof\\napproximation.\\nAnd\\nin\\nfact\\nthe\\nbig\\nbreakthrough\\nin\\n“deep\\nlearning”\\nthat\\noccurred\\naround\\n2011\\nwas\\nassociated\\nwith\\nthe\\ndiscovery\\nthat\\nin\\nsome\\nsense\\nit\\ncan\\nbe\\neasier\\nto\\ndo\\n(at\\nleast\\napproximate)\\nminimization\\nwhen\\nthere\\nare\\nlots\\nof\\nweights\\ninvolved\\nthan\\nwhen\\nthere\\nare\\nfairly\\nfew.\\nIn\\nother\\nwords—somewhat\\ncounterintuitively—it\\ncan\\nbe\\neasier\\nto\\nsolve\\nmore\\ncomplicated\\nproblems\\nwith\\nneural\\nnets\\nthan\\nsimpler\\nones.\\nAnd\\nthe' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 31}\n",
            "\n",
            "Page 33:\n",
            "page_content='rough\\nreason\\nfor\\nthis\\nseems\\nto\\nbe\\nthat\\nwhen\\none\\nhas\\na\\nlot\\nof\\n“weight\\nvariables”\\none\\nhas\\na\\nhigh-dimensional\\nspace\\nwith\\n“lots\\nof\\ndifferent\\ndirections”\\nthat\\ncan\\nlead\\none\\nto\\nthe\\nminimum—whereas\\nwith\\nfewer\\nvariables\\nit’s\\neasier\\nto\\nend\\nup\\ngetting\\nstuck\\nin\\na\\nlocal\\nminimum\\n(“mountain\\nlake”)\\nfrom\\nwhich\\nthere’s\\nno\\n“direction\\nto\\nget\\nout”.\\nIt’s\\nworth\\npointing\\nout\\nthat\\nin\\ntypical\\ncases\\nthere\\nare\\nmany\\ndifferent\\ncollections\\nof\\nweights\\nthat\\nwill\\nall\\ngive\\nneural\\nnets\\nthat\\nhave\\npretty\\nmuch\\nthe\\nsame\\nperformance.\\nAnd\\nusually\\nin\\npractical\\nneural\\nnet\\ntraining\\nthere\\nare\\nlots\\nof\\nrandom\\nchoices\\nmade—that\\nlead\\nto\\n“different-but-equivalent\\nsolutions”,\\nlike\\nthese:\\nBut\\neach\\nsuch\\n“different\\nsolution”\\nwill\\nhave\\nat\\nleast\\nslightly\\ndifferent\\nbehavior.\\nAnd\\nif\\nwe\\nask,\\nsay,\\nfor\\nan\\n“extrapolation”\\noutside\\nthe\\nregion\\nwhere\\nwe\\ngave\\ntraining\\nexamples,\\nwe\\ncan\\nget\\ndramatically\\ndifferent\\nresults:\\nBut\\nwhich\\nof\\nthese\\nis\\n“right”?\\nThere’s\\nreally\\nno\\nway\\nto\\nsay.\\nThey’re\\nall\\n“consistent\\nwith\\nthe\\nobserved\\ndata”.\\nBut\\nthey\\nall\\ncorrespond\\nto\\ndifferent\\n“innate”\\nways\\nto\\n“think\\nabout”\\nwhat\\nto\\ndo\\n“outside\\nthe\\nbox”.\\nAnd\\nsome\\nmay\\nseem\\n“more\\nreasonable”\\nto\\nus\\nhumans\\nthan\\nothers. \\nThe\\nPractice\\nand\\nLore\\nof\\nNeural\\nNet\\nTraining\\nParticularly\\nover\\nthe\\npast\\ndecade,\\nthere’ve\\nbeen\\nmany\\nadvances\\nin\\nthe\\nart\\nof\\ntraining\\nneural\\nnets.\\nAnd,\\nyes,\\nit\\nis\\nbasically\\nan\\nart.\\nSometimes—especially\\nin\\nretrospect—one\\ncan\\nsee\\nat\\nleast\\na\\nglimmer\\nof\\na\\n“scientific\\nexplanation”\\nfor\\nsomething\\nthat’s\\nbeing\\ndone.\\nBut\\nmostly\\nthings\\nhave\\nbeen\\ndiscovered\\nby\\ntrial\\nand\\nerror,\\nadding\\nideas\\nand\\ntricks\\nthat\\nhave\\nprogressively\\nbuilt\\na\\nsignificant\\nlore\\nabout\\nhow\\nto\\nwork\\nwith\\nneural\\nnets.\\nThere\\nare\\nseveral\\nkey\\nparts.\\nFirst,\\nthere’s\\nthe\\nmatter\\nof\\nwhat\\narchitecture\\nof\\nneural\\nnet\\none\\nshould\\nuse\\nfor\\na\\nparticular\\ntask.\\nThen\\nthere’s\\nthe\\ncritical\\nissue\\nof\\nhow\\none’s\\ngoing\\nto\\nget\\nthe\\ndata\\non\\nwhich\\nto\\ntrain\\nthe\\nneural\\nnet.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 32}\n",
            "\n",
            "Page 34:\n",
            "page_content='And\\nincreasingly\\none\\nisn’t\\ndealing\\nwith\\ntraining\\na\\nnet\\nfrom\\nscratch:\\ninstead\\na\\nnew\\nnet\\ncan\\neither\\ndirectly\\nincorporate\\nanother\\nalready-trained\\nnet,\\nor\\nat\\nleast\\ncan\\nuse\\nthat\\nnet\\nto\\ngenerate\\nmore\\ntraining\\nexamples\\nfor\\nitself.\\nOne\\nmight\\nhave\\nthought\\nthat\\nfor\\nevery\\nparticular\\nkind\\nof\\ntask\\none\\nwould\\nneed\\na\\ndifferent\\narchitecture\\nof\\nneural\\nnet.\\nBut\\nwhat’s\\nbeen\\nfound\\nis\\nthat\\nthe\\nsame\\narchitecture\\noften\\nseems\\nto\\nwork\\neven\\nfor\\napparently\\nquite\\ndifferent\\ntasks.\\nAt\\nsome\\nlevel\\nthis\\nreminds\\none\\nof\\nthe\\nidea\\nof\\nuniversal\\ncomputation\\n(and\\nmy\\nPrinciple\\nof\\nComputational\\nEquivalence\\n),\\nbut,\\nas\\nI’ll\\ndiscuss\\nlater,\\nI\\nthink\\nit’s\\nmore\\na\\nreflection\\nof\\nthe\\nfact\\nthat\\nthe\\ntasks\\nwe’re\\ntypically\\ntrying\\nto\\nget\\nneural\\nnets\\nto\\ndo\\nare\\n“human-like”\\nones—and\\nneural\\nnets\\ncan\\ncapture\\nquite\\ngeneral\\n“human-like\\nprocesses”.\\nIn\\nearlier\\ndays\\nof\\nneural\\nnets,\\nthere\\ntended\\nto\\nbe\\nthe\\nidea\\nthat\\none\\nshould\\n“make\\nthe\\nneural\\nnet\\ndo\\nas\\nlittle\\nas\\npossible”.\\nFor\\nexample,\\nin\\nconverting\\nspeech\\nto\\ntext\\nit\\nwas\\nthought\\nthat\\none\\nshould\\nfirst\\nanalyze\\nthe\\naudio\\nof\\nthe\\nspeech,\\nbreak\\nit\\ninto\\nphonemes,\\netc.\\nBut\\nwhat\\nwas\\nfound\\nis\\nthat—at\\nleast\\nfor\\n“human-like\\ntasks”—it’s\\nusually\\nbetter\\njust\\nto\\ntry\\nto\\ntrain\\nthe\\nneural\\nnet\\non\\nthe\\n“end-to-end\\nproblem”,\\nletting\\nit\\n“discover”\\nthe\\nnecessary\\nintermediate\\nfeatures,\\nencodings,\\netc.\\nfor\\nitself.\\nThere\\nwas\\nalso\\nthe\\nidea\\nthat\\none\\nshould\\nintroduce\\ncomplicated\\nindividual\\ncomponents\\ninto\\nthe\\nneural\\nnet,\\nto\\nlet\\nit\\nin\\neffect\\n“explicitly\\nimplement\\nparticular\\nalgorithmic\\nideas”.\\nBut\\nonce\\nagain,\\nthis\\nhas\\nmostly\\nturned\\nout\\nnot\\nto\\nbe\\nworthwhile;\\ninstead,\\nit’s\\nbetter\\njust\\nto\\ndeal\\nwith\\nvery\\nsimple\\ncomponents\\nand\\nlet\\nthem\\n“organize\\nthemselves”\\n(albeit\\nusually\\nin\\nways\\nwe\\ncan’t\\nunderstand)\\nto\\nachieve\\n(presumably)\\nthe\\nequivalent\\nof\\nthose\\nalgorithmic\\nideas.\\nThat’s\\nnot\\nto\\nsay\\nthat\\nthere\\nare\\nno\\n“structuring\\nideas”\\nthat\\nare\\nrelevant\\nfor\\nneural\\nnets.\\nThus,\\nfor\\nexample,\\nhaving\\n2D\\narrays\\nof\\nneurons\\nwith\\nlocal\\nconnections\\nseems\\nat\\nleast\\nvery\\nuseful\\nin\\nthe\\nearly\\nstages\\nof\\nprocessing\\nimages.\\nAnd\\nhaving\\npatterns\\nof\\nconnectivity\\nthat\\nconcentrate\\non\\n“looking\\nback\\nin\\nsequences”\\nseems\\nuseful—as\\nwe’ll\\nsee\\nlater—in\\ndealing\\nwith\\nthings\\nlike\\nhuman\\nlanguage,\\nfor\\nexample\\nin\\nChatGPT.\\nBut\\nan\\nimportant\\nfeature\\nof\\nneural\\nnets\\nis\\nthat—like\\ncomputers\\nin\\ngeneral—they’re\\nultimately\\njust\\ndealing\\nwith\\ndata.\\nAnd\\ncurrent\\nneural\\nnets—with\\ncurrent\\napproaches\\nto\\nneural\\nnet\\ntraining—\\nspecifically\\ndeal\\nwith\\narrays\\nof\\nnumbers\\n.\\nBut\\nin\\nthe\\ncourse\\nof\\nprocessing,\\nthose\\narrays\\ncan\\nbe\\ncompletely\\nrearranged\\nand\\nreshaped.\\nAnd\\nas\\nan\\nexample,\\nthe\\nnetwork\\nwe' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 33}\n",
            "\n",
            "Page 35:\n",
            "page_content='used\\nfor\\nidentifying\\ndigits\\nabove\\nstarts\\nwith\\na\\n2D\\n“image-like”\\narray,\\nquickly\\n“thickening”\\nto\\nmany\\nchannels,\\nbut\\nthen\\n“concentrating\\ndown”\\ninto\\na\\n1D\\narray\\nthat\\nwill\\nultimately\\ncontain\\nelements\\nrepresenting\\nthe\\ndifferent\\npossible\\noutput\\ndigits:\\nBut,\\nOK,\\nhow\\ncan\\none\\ntell\\nhow\\nbig\\na\\nneural\\nnet\\none\\nwill\\nneed\\nfor\\na\\nparticular\\ntask?\\nIt’s\\nsomething\\nof\\nan\\nart.\\nAt\\nsome\\nlevel\\nthe\\nkey\\nthing\\nis\\nto\\nknow\\n“how\\nhard\\nthe\\ntask\\nis”.\\nBut\\nfor\\nhuman-like\\ntasks\\nthat’s\\ntypically\\nvery\\nhard\\nto\\nestimate.\\nYes,\\nthere\\nmay\\nbe\\na\\nsystematic\\nway\\nto\\ndo\\nthe\\ntask\\nvery\\n“mechanically”\\nby\\ncomputer.\\nBut\\nit’s\\nhard\\nto\\nknow\\nif\\nthere\\nare\\nwhat\\none\\nmight\\nthink\\nof\\nas\\ntricks\\nor\\nshortcuts\\nthat\\nallow\\none\\nto\\ndo\\nthe\\ntask\\nat\\nleast\\nat\\na\\n“human-like\\nlevel”\\nvastly\\nmore\\neasily.\\nIt\\nmight\\ntake\\nenumerating\\na\\ngiant\\ngame\\ntree\\nto\\n“mechanically”\\nplay\\na\\ncertain\\ngame;\\nbut\\nthere\\nmight\\nbe\\na\\nmuch\\neasier\\n(“heuristic”)\\nway\\nto\\nachieve\\n“human-level\\nplay”.\\nWhen\\none’s\\ndealing\\nwith\\ntiny\\nneural\\nnets\\nand\\nsimple\\ntasks\\none\\ncan\\nsometimes\\nexplicitly\\nsee\\nthat\\none\\n“can’t\\nget\\nthere\\nfrom\\nhere”.\\nFor\\nexample,\\nhere’s\\nthe\\nbest\\none\\nseems\\nto\\nbe\\nable\\nto\\ndo\\non\\nthe\\ntask\\nfrom\\nthe\\nprevious\\nsection\\nwith\\na\\nfew\\nsmall\\nneural\\nnets:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 34}\n",
            "\n",
            "Page 36:\n",
            "page_content='And\\nwhat\\nwe\\nsee\\nis\\nthat\\nif\\nthe\\nnet\\nis\\ntoo\\nsmall,\\nit\\njust\\ncan’t\\nreproduce\\nthe\\nfunction\\nwe\\nwant.\\nBut\\nabove\\nsome\\nsize,\\nit\\nhas\\nno\\nproblem—at\\nleast\\nif\\none\\ntrains\\nit\\nfor\\nlong\\nenough,\\nwith\\nenough\\nexamples.\\nAnd,\\nby\\nthe\\nway,\\nthese\\npictures\\nillustrate\\na\\npiece\\nof\\nneural\\nnet\\nlore:\\nthat\\none\\ncan\\noften\\nget\\naway\\nwith\\na\\nsmaller\\nnetwork\\nif\\nthere’s\\na\\n“squeeze”\\nin\\nthe\\nmiddle\\nthat\\nforces\\neverything\\nto\\ngo\\nthrough\\na\\nsmaller\\nintermediate\\nnumber\\nof\\nneurons.\\n(It’s\\nalso\\nworth\\nmentioning\\nthat\\n“no-intermediate-layer”—or\\nso-called\\n“\\nperceptron\\n”—networks\\ncan\\nonly\\nlearn\\nessentially\\nlinear\\nfunctions—but\\nas\\nsoon\\nas\\nthere’s\\neven\\none\\nintermediate\\nlayer\\nit’s\\nalways\\nin\\nprinciple\\npossible\\nto\\napproximate\\nany\\nfunction\\narbitrarily\\nwell,\\nat\\nleast\\nif\\none\\nhas\\nenough\\nneurons,\\nthough\\nto\\nmake\\nit\\nfeasibly\\ntrainable\\none\\ntypically\\nhas\\nsome\\nkind\\nof\\nregularization\\nor\\nnormalization\\n.)\\nOK,\\nso\\nlet’s\\nsay\\none’s\\nsettled\\non\\na\\ncertain\\nneural\\nnet\\narchitecture.\\nNow\\nthere’s\\nthe\\nissue\\nof\\ngetting\\ndata\\nto\\ntrain\\nthe\\nnetwork\\nwith.\\nAnd\\nmany\\nof\\nthe\\npractical\\nchallenges\\naround\\nneural\\nnets—and\\nmachine\\nlearning\\nin\\ngeneral—center\\non\\nacquiring\\nor\\npreparing\\nthe\\nnecessary\\ntraining\\ndata.\\nIn\\nmany\\ncases\\n(“supervised\\nlearning”)\\none\\nwants\\nto\\nget\\nexplicit\\nexamples\\nof' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 35}\n",
            "\n",
            "Page 37:\n",
            "page_content='inputs\\nand\\nthe\\noutputs\\none\\nis\\nexpecting\\nfrom\\nthem.\\nThus,\\nfor\\nexample,\\none\\nmight\\nwant\\nimages\\ntagged\\nby\\nwhat’s\\nin\\nthem,\\nor\\nsome\\nother\\nattribute.\\nAnd\\nmaybe\\none\\nwill\\nhave\\nto\\nexplicitly\\ngo\\nthrough—usually\\nwith\\ngreat\\neffort—and\\ndo\\nthe\\ntagging.\\nBut\\nvery\\noften\\nit\\nturns\\nout\\nto\\nbe\\npossible\\nto\\npiggyback\\non\\nsomething\\nthat’s\\nalready\\nbeen\\ndone,\\nor\\nuse\\nit\\nas\\nsome\\nkind\\nof\\nproxy.\\nAnd\\nso,\\nfor\\nexample,\\none\\nmight\\nuse\\nalt\\ntags\\nthat\\nhave\\nbeen\\nprovided\\nfor\\nimages\\non\\nthe\\nweb.\\nOr,\\nin\\na\\ndifferent\\ndomain,\\none\\nmight\\nuse\\nclosed\\ncaptions\\nthat\\nhave\\nbeen\\ncreated\\nfor\\nvideos.\\nOr—for\\nlanguage\\ntranslation\\ntraining—one\\nmight\\nuse\\nparallel\\nversions\\nof\\nwebpages\\nor\\nother\\ndocuments\\nthat\\nexist\\nin\\ndifferent\\nlanguages.\\nHow\\nmuch\\ndata\\ndo\\nyou\\nneed\\nto\\nshow\\na\\nneural\\nnet\\nto\\ntrain\\nit\\nfor\\na\\nparticular\\ntask?\\nAgain,\\nit’s\\nhard\\nto\\nestimate\\nfrom\\nfirst\\nprinciples.\\nCertainly\\nthe\\nrequirements\\ncan\\nbe\\ndramatically\\nreduced\\nby\\nusing\\n“transfer\\nlearning”\\nto\\n“transfer\\nin”\\nthings\\nlike\\nlists\\nof\\nimportant\\nfeatures\\nthat\\nhave\\nalready\\nbeen\\nlearned\\nin\\nanother\\nnetwork.\\nBut\\ngenerally\\nneural\\nnets\\nneed\\nto\\n“see\\na\\nlot\\nof\\nexamples”\\nto\\ntrain\\nwell.\\nAnd\\nat\\nleast\\nfor\\nsome\\ntasks\\nit’s\\nan\\nimportant\\npiece\\nof\\nneural\\nnet\\nlore\\nthat\\nthe\\nexamples\\ncan\\nbe\\nincredibly\\nrepetitive.\\nAnd\\nindeed\\nit’s\\na\\nstandard\\nstrategy\\nto\\njust\\nshow\\na\\nneural\\nnet\\nall\\nthe\\nexamples\\none\\nhas,\\nover\\nand\\nover\\nagain.\\nIn\\neach\\nof\\nthese\\n“training\\nrounds”\\n(or\\n“epochs”)\\nthe\\nneural\\nnet\\nwill\\nbe\\nin\\nat\\nleast\\na\\nslightly\\ndifferent\\nstate,\\nand\\nsomehow\\n“reminding\\nit”\\nof\\na\\nparticular\\nexample\\nis\\nuseful\\nin\\ngetting\\nit\\nto\\n“remember\\nthat\\nexample”.\\n(And,\\nyes,\\nperhaps\\nthis\\nis\\nanalogous\\nto\\nthe\\nusefulness\\nof\\nrepetition\\nin\\nhuman\\nmemorization.)\\nBut\\noften\\njust\\nrepeating\\nthe\\nsame\\nexample\\nover\\nand\\nover\\nagain\\nisn’t\\nenough.\\nIt’s\\nalso\\nnecessary\\nto\\nshow\\nthe\\nneural\\nnet\\nvariations\\nof\\nthe\\nexample.\\nAnd\\nit’s\\na\\nfeature\\nof\\nneural\\nnet\\nlore\\nthat\\nthose\\n“data\\naugmentation”\\nvariations\\ndon’t\\nhave\\nto\\nbe\\nsophisticated\\nto\\nbe\\nuseful.\\nJust\\nslightly\\nmodifying\\nimages\\nwith\\nbasic\\nimage\\nprocessing\\ncan\\nmake\\nthem\\nessentially\\n“as\\ngood\\nas\\nnew”\\nfor\\nneural\\nnet\\ntraining.\\nAnd,\\nsimilarly,\\nwhen\\none’s\\nrun\\nout\\nof\\nactual\\nvideo,\\netc.\\nfor\\ntraining\\nself-driving\\ncars,\\none\\ncan\\ngo\\non\\nand\\njust\\nget\\ndata\\nfrom\\nrunning\\nsimulations\\nin\\na\\nmodel\\nvideogame-like\\nenvironment\\nwithout\\nall\\nthe\\ndetail\\nof\\nactual\\nreal-world\\nscenes.\\nHow\\nabout\\nsomething\\nlike\\nChatGPT?\\nWell,\\nit\\nhas\\nthe\\nnice\\nfeature\\nthat\\nit\\ncan\\ndo\\n“unsupervised\\nlearning”,\\nmaking\\nit\\nmuch\\neasier\\nto\\nget\\nit\\nexamples\\nto\\ntrain\\nfrom.\\nRecall\\nthat\\nthe\\nbasic\\ntask\\nfor\\nChatGPT\\nis\\nto\\nfigure\\nout\\nhow\\nto\\ncontinue\\na\\npiece\\nof\\ntext\\nthat\\nit’s\\nbeen\\ngiven.\\nSo\\nto\\nget\\nit\\n“training\\nexamples”\\nall\\none\\nhas\\nto\\ndo\\nis\\nget\\na\\npiece\\nof\\ntext,\\nand\\nmask\\nout\\nthe\\nend\\nof\\nit,\\nand\\nthen' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 36}\n",
            "\n",
            "Page 38:\n",
            "page_content='use\\nthis\\nas\\nthe\\n“input\\nto\\ntrain\\nfrom”—with\\nthe\\n“output”\\nbeing\\nthe\\ncomplete,\\nunmasked\\npiece\\nof\\ntext.\\nWe’ll\\ndiscuss\\nthis\\nmore\\nlater,\\nbut\\nthe\\nmain\\npoint\\nis\\nthat—unlike,\\nsay,\\nfor\\nlearning\\nwhat’s\\nin\\nimages—there’s\\nno\\n“explicit\\ntagging”\\nneeded;\\nChatGPT\\ncan\\nin\\neffect\\njust\\nlearn\\ndirectly\\nfrom\\nwhatever\\nexamples\\nof\\ntext\\nit’s\\ngiven.\\nOK,\\nso\\nwhat\\nabout\\nthe\\nactual\\nlearning\\nprocess\\nin\\na\\nneural\\nnet?\\nIn\\nthe\\nend\\nit’s\\nall\\nabout\\ndetermining\\nwhat\\nweights\\nwill\\nbest\\ncapture\\nthe\\ntraining\\nexamples\\nthat\\nhave\\nbeen\\ngiven.\\nAnd\\nthere\\nare\\nall\\nsorts\\nof\\ndetailed\\nchoices\\nand\\n“hyperparameter\\nsettings”\\n(so\\ncalled\\nbecause\\nthe\\nweights\\ncan\\nbe\\nthought\\nof\\nas\\n“parameters”)\\nthat\\ncan\\nbe\\nused\\nto\\ntweak\\nhow\\nthis\\nis\\ndone.\\nThere\\nare\\ndifferent\\nchoices\\nof\\nloss\\nfunction\\n(sum\\nof\\nsquares,\\nsum\\nof\\nabsolute\\nvalues,\\netc.).\\nThere\\nare\\ndifferent\\nways\\nto\\ndo\\nloss\\nminimization\\n(how\\nfar\\nin\\nweight\\nspace\\nto\\nmove\\nat\\neach\\nstep,\\netc.).\\nAnd\\nthen\\nthere\\nare\\nquestions\\nlike\\nhow\\nbig\\na\\n“batch”\\nof\\nexamples\\nto\\nshow\\nto\\nget\\neach\\nsuccessive\\nestimate\\nof\\nthe\\nloss\\none’s\\ntrying\\nto\\nminimize.\\nAnd,\\nyes,\\none\\ncan\\napply\\nmachine\\nlearning\\n(as\\nwe\\ndo,\\nfor\\nexample,\\nin\\nWolfram\\nLanguage)\\nto\\nautomate\\nmachine\\nlearning—and\\nto\\nautomatically\\nset\\nthings\\nlike\\nhyperparameters.\\nBut\\nin\\nthe\\nend\\nthe\\nwhole\\nprocess\\nof\\ntraining\\ncan\\nbe\\ncharacterized\\nby\\nseeing\\nhow\\nthe\\nloss\\nprogressively\\ndecreases\\n(as\\nin\\nthis\\nWolfram\\nLanguage\\nprogress\\nmonitor\\nfor\\na\\nsmall\\ntraining\\n):' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 37}\n",
            "\n",
            "Page 39:\n",
            "page_content='And\\nwhat\\none\\ntypically\\nsees\\nis\\nthat\\nthe\\nloss\\ndecreases\\nfor\\na\\nwhile,\\nbut\\neventually\\nflattens\\nout\\nat\\nsome\\nconstant\\nvalue.\\nIf\\nthat\\nvalue\\nis\\nsufficiently\\nsmall,\\nthen\\nthe\\ntraining\\ncan\\nbe\\nconsidered\\nsuccessful;\\notherwise\\nit’s\\nprobably\\na\\nsign\\none\\nshould\\ntry\\nchanging\\nthe\\nnetwork\\narchitecture.\\nCan\\none\\ntell\\nhow\\nlong\\nit\\nshould\\ntake\\nfor\\nthe\\n“learning\\ncurve”\\nto\\nflatten\\nout?\\nLike\\nfor\\nso\\nmany\\nother\\nthings,\\nthere\\nseem\\nto\\nbe\\napproximate\\npower-law\\nscaling\\nrelationships\\nthat\\ndepend\\non\\nthe\\nsize\\nof\\nneural\\nnet\\nand\\namount\\nof\\ndata\\none’s\\nusing.\\nBut\\nthe\\ngeneral\\nconclusion\\nis\\nthat\\ntraining\\na\\nneural\\nnet\\nis\\nhard—and\\ntakes\\na\\nlot\\nof\\ncomputational\\neffort.\\nAnd\\nas\\na\\npractical\\nmatter,\\nthe\\nvast\\nmajority\\nof\\nthat\\neffort\\nis\\nspent\\ndoing\\noperations\\non\\narrays\\nof\\nnumbers,' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 38}\n",
            "\n",
            "Page 40:\n",
            "page_content='which\\nis\\nwhat\\nGPUs\\nare\\ngood\\nat—which\\nis\\nwhy\\nneural\\nnet\\ntraining\\nis\\ntypically\\nlimited\\nby\\nthe\\navailability\\nof\\nGPUs.\\nIn\\nthe\\nfuture,\\nwill\\nthere\\nbe\\nfundamentally\\nbetter\\nways\\nto\\ntrain\\nneural\\nnets—or\\ngenerally\\ndo\\nwhat\\nneural\\nnets\\ndo?\\nAlmost\\ncertainly,\\nI\\nthink.\\nThe\\nfundamental\\nidea\\nof\\nneural\\nnets\\nis\\nto\\ncreate\\na\\nflexible\\n“computing\\nfabric”\\nout\\nof\\na\\nlarge\\nnumber\\nof\\nsimple\\n(essentially\\nidentical)\\ncomponents—and\\nto\\nhave\\nthis\\n“fabric”\\nbe\\none\\nthat\\ncan\\nbe\\nincrementally\\nmodified\\nto\\nlearn\\nfrom\\nexamples.\\nIn\\ncurrent\\nneural\\nnets,\\none’s\\nessentially\\nusing\\nthe\\nideas\\nof\\ncalculus—applied\\nto\\nreal\\nnumbers—to\\ndo\\nthat\\nincremental\\nmodification.\\nBut\\nit’s\\nincreasingly\\nclear\\nthat\\nhaving\\nhigh-precision\\nnumbers\\ndoesn’t\\nmatter;\\n8\\nbits\\nor\\nless\\nmight\\nbe\\nenough\\neven\\nwith\\ncurrent\\nmethods.\\nWith\\ncomputational\\nsystems\\nlike\\ncellular\\nautomata\\nthat\\nbasically\\noperate\\nin\\nparallel\\non\\nmany\\nindividual\\nbits\\nit’s\\nnever\\nbeen\\nclear\\nhow\\nto\\ndo\\nthis\\nkind\\nof\\nincremental\\nmodification\\n,\\nbut\\nthere’s\\nno\\nreason\\nto\\nthink\\nit\\nisn’t\\npossible.\\nAnd\\nin\\nfact,\\nmuch\\nlike\\nwith\\nthe\\n“\\ndeep-learning\\nbreakthrough\\nof\\n2012\\n”\\nit\\nmay\\nbe\\nthat\\nsuch\\nincremental\\nmodification\\nwill\\neffectively\\nbe\\neasier\\nin\\nmore\\ncomplicated\\ncases\\nthan\\nin\\nsimple\\nones.\\nNeural\\nnets—perhaps\\na\\nbit\\nlike\\nbrains—are\\nset\\nup\\nto\\nhave\\nan\\nessentially\\nfixed\\nnetwork\\nof\\nneurons,\\nwith\\nwhat’s\\nmodified\\nbeing\\nthe\\nstrength\\n(“weight”)\\nof\\nconnections\\nbetween\\nthem.\\n(Perhaps\\nin\\nat\\nleast\\nyoung\\nbrains\\nsignificant\\nnumbers\\nof\\nwholly\\nnew\\nconnections\\ncan\\nalso\\ngrow.)\\nBut\\nwhile\\nthis\\nmight\\nbe\\na\\nconvenient\\nsetup\\nfor\\nbiology,\\nit’s\\nnot\\nat\\nall\\nclear\\nthat\\nit’s\\neven\\nclose\\nto\\nthe\\nbest\\nway\\nto\\nachieve\\nthe\\nfunctionality\\nwe\\nneed.\\nAnd\\nsomething\\nthat\\ninvolves\\nthe\\nequivalent\\nof\\nprogressive\\nnetwork\\nrewriting\\n(perhaps\\nreminiscent\\nof\\nour\\nPhysics\\nProject\\n)\\nmight\\nwell\\nultimately\\nbe\\nbetter.\\nBut\\neven\\nwithin\\nthe\\nframework\\nof\\nexisting\\nneural\\nnets\\nthere’s\\ncurrently\\na\\ncrucial\\nlimitation:\\nneural\\nnet\\ntraining\\nas\\nit’s\\nnow\\ndone\\nis\\nfundamentally\\nsequential,\\nwith\\nthe\\neffects\\nof\\neach\\nbatch\\nof\\nexamples\\nbeing\\npropagated\\nback\\nto\\nupdate\\nthe\\nweights.\\nAnd\\nindeed\\nwith\\ncurrent\\ncomputer\\nhardware—even\\ntaking\\ninto\\naccount\\nGPUs—most\\nof\\na\\nneural\\nnet\\nis\\n“idle”\\nmost\\nof\\nthe\\ntime\\nduring\\ntraining,\\nwith\\njust\\none\\npart\\nat\\na\\ntime\\nbeing\\nupdated.\\nAnd\\nin\\na\\nsense\\nthis\\nis\\nbecause\\nour\\ncurrent\\ncomputers\\ntend\\nto\\nhave\\nmemory\\nthat\\nis\\nseparate\\nfrom\\ntheir\\nCPUs\\n(or\\nGPUs).\\nBut\\nin\\nbrains\\nit’s\\npresumably\\ndifferent—with\\nevery\\n“memory\\nelement”\\n(i.e.\\nneuron)\\nalso\\nbeing\\na\\npotentially\\nactive\\ncomputational\\nelement.\\nAnd\\nif\\nwe\\ncould\\nset\\nup\\nour\\nfuture' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 39}\n",
            "\n",
            "Page 41:\n",
            "page_content='computer\\nhardware\\nthis\\nway\\nit\\nmight\\nbecome\\npossible\\nto\\ndo\\ntraining\\nmuch\\nmore\\nefficiently. \\n“Surely\\na\\nNetwork\\nThat’s\\nBig\\nEnough\\nCan\\nDo\\nAnything!”\\nThe\\ncapabilities\\nof\\nsomething\\nlike\\nChatGPT\\nseem\\nso\\nimpressive\\nthat\\none\\nmight\\nimagine\\nthat\\nif\\none\\ncould\\njust\\n“keep\\ngoing”\\nand\\ntrain\\nlarger\\nand\\nlarger\\nneural\\nnetworks,\\nthen\\nthey’d\\neventually\\nbe\\nable\\nto\\n“do\\neverything”.\\nAnd\\nif\\none’s\\nconcerned\\nwith\\nthings\\nthat\\nare\\nreadily\\naccessible\\nto\\nimmediate\\nhuman\\nthinking,\\nit’s\\nquite\\npossible\\nthat\\nthis\\nis\\nthe\\ncase.\\nBut\\nthe\\nlesson\\nof\\nthe\\npast\\nseveral\\nhundred\\nyears\\nof\\nscience\\nis\\nthat\\nthere\\nare\\nthings\\nthat\\ncan\\nbe\\nfigured\\nout\\nby\\nformal\\nprocesses,\\nbut\\naren’t\\nreadily\\naccessible\\nto\\nimmediate\\nhuman\\nthinking.\\nNontrivial\\nmathematics\\nis\\none\\nbig\\nexample.\\nBut\\nthe\\ngeneral\\ncase\\nis\\nreally\\ncomputation.\\nAnd\\nultimately\\nthe\\nissue\\nis\\nthe\\nphenomenon\\nof\\ncomputational\\nirreducibility\\n.\\nThere\\nare\\nsome\\ncomputations\\nwhich\\none\\nmight\\nthink\\nwould\\ntake\\nmany\\nsteps\\nto\\ndo,\\nbut\\nwhich\\ncan\\nin\\nfact\\nbe\\n“reduced”\\nto\\nsomething\\nquite\\nimmediate.\\nBut\\nthe\\ndiscovery\\nof\\ncomputational\\nirreducibility\\nimplies\\nthat\\nthis\\ndoesn’t\\nalways\\nwork.\\nAnd\\ninstead\\nthere\\nare\\nprocesses—probably\\nlike\\nthe\\none\\nbelow—where\\nto\\nwork\\nout\\nwhat\\nhappens\\ninevitably\\nrequires\\nessentially\\ntracing\\neach\\ncomputational\\nstep:\\nThe\\nkinds\\nof\\nthings\\nthat\\nwe\\nnormally\\ndo\\nwith\\nour\\nbrains\\nare\\npresumably\\nspecifically\\nchosen\\nto\\navoid\\ncomputational\\nirreducibility.\\nIt\\ntakes\\nspecial\\neffort\\nto\\ndo\\nmath\\nin\\none’s\\nbrain.\\nAnd\\nit’s\\nin\\npractice\\nlargely\\nimpossible\\nto' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 40}\n",
            "\n",
            "Page 42:\n",
            "page_content='“think\\nthrough”\\nthe\\nsteps\\nin\\nthe\\noperation\\nof\\nany\\nnontrivial\\nprogram\\njust\\nin\\none’s\\nbrain.\\nBut\\nof\\ncourse\\nfor\\nthat\\nwe\\nhave\\ncomputers.\\nAnd\\nwith\\ncomputers\\nwe\\ncan\\nreadily\\ndo\\nlong,\\ncomputationally\\nirreducible\\nthings.\\nAnd\\nthe\\nkey\\npoint\\nis\\nthat\\nthere’s\\nin\\ngeneral\\nno\\nshortcut\\nfor\\nthese.\\nYes,\\nwe\\ncould\\nmemorize\\nlots\\nof\\nspecific\\nexamples\\nof\\nwhat\\nhappens\\nin\\nsome\\nparticular\\ncomputational\\nsystem.\\nAnd\\nmaybe\\nwe\\ncould\\neven\\nsee\\nsome\\n(“computationally\\nreducible”)\\npatterns\\nthat\\nwould\\nallow\\nus\\nto\\ndo\\na\\nlittle\\ngeneralization.\\nBut\\nthe\\npoint\\nis\\nthat\\ncomputational\\nirreducibility\\nmeans\\nthat\\nwe\\ncan\\nnever\\nguarantee\\nthat\\nthe\\nunexpected\\nwon’t\\nhappen—and\\nit’s\\nonly\\nby\\nexplicitly\\ndoing\\nthe\\ncomputation\\nthat\\nyou\\ncan\\ntell\\nwhat\\nactually\\nhappens\\nin\\nany\\nparticular\\ncase.\\nAnd\\nin\\nthe\\nend\\nthere’s\\njust\\na\\nfundamental\\ntension\\nbetween\\nlearnability\\nand\\ncomputational\\nirreducibility.\\nLearning\\ninvolves\\nin\\neffect\\ncompressing\\ndata\\nby\\nleveraging\\nregularities\\n.\\nBut\\ncomputational\\nirreducibility\\nimplies\\nthat\\nultimately\\nthere’s\\na\\nlimit\\nto\\nwhat\\nregularities\\nthere\\nmay\\nbe.\\nAs\\na\\npractical\\nmatter,\\none\\ncan\\nimagine\\nbuilding\\nlittle\\ncomputational\\ndevices—like\\ncellular\\nautomata\\nor\\nTuring\\nmachines—into\\ntrainable\\nsystems\\nlike\\nneural\\nnets.\\nAnd\\nindeed\\nsuch\\ndevices\\ncan\\nserve\\nas\\ngood\\n“tools”\\nfor\\nthe\\nneural\\nnet—like\\nWolfram|Alpha\\ncan\\nbe\\na\\ngood\\ntool\\nfor\\nChatGPT\\n.\\nBut\\ncomputational\\nirreducibility\\nimplies\\nthat\\none\\ncan’t\\nexpect\\nto\\n“get\\ninside”\\nthose\\ndevices\\nand\\nhave\\nthem\\nlearn.\\nOr\\nput\\nanother\\nway,\\nthere’s\\nan\\nultimate\\ntradeoff\\nbetween\\ncapability\\nand\\ntrainability:\\nthe\\nmore\\nyou\\nwant\\na\\nsystem\\nto\\nmake\\n“true\\nuse”\\nof\\nits\\ncomputational\\ncapabilities,\\nthe\\nmore\\nit’s\\ngoing\\nto\\nshow\\ncomputational\\nirreducibility,\\nand\\nthe\\nless\\nit’s\\ngoing\\nto\\nbe\\ntrainable.\\nAnd\\nthe\\nmore\\nit’s\\nfundamentally\\ntrainable,\\nthe\\nless\\nit’s\\ngoing\\nto\\nbe\\nable\\nto\\ndo\\nsophisticated\\ncomputation.\\n(For\\nChatGPT\\nas\\nit\\ncurrently\\nis,\\nthe\\nsituation\\nis\\nactually\\nmuch\\nmore\\nextreme,\\nbecause\\nthe\\nneural\\nnet\\nused\\nto\\ngenerate\\neach\\ntoken\\nof\\noutput\\nis\\na\\npure\\n“feed-forward”\\nnetwork,\\nwithout\\nloops,\\nand\\ntherefore\\nhas\\nno\\nability\\nto\\ndo\\nany\\nkind\\nof\\ncomputation\\nwith\\nnontrivial\\n“control\\nflow”.)\\nOf\\ncourse,\\none\\nmight\\nwonder\\nwhether\\nit’s\\nactually\\nimportant\\nto\\nbe\\nable\\nto\\ndo\\nirreducible\\ncomputations.\\nAnd\\nindeed\\nfor\\nmuch\\nof\\nhuman\\nhistory\\nit' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 41}\n",
            "\n",
            "Page 43:\n",
            "page_content='wasn’t\\nparticularly\\nimportant.\\nBut\\nour\\nmodern\\ntechnological\\nworld\\nhas\\nbeen\\nbuilt\\non\\nengineering\\nthat\\nmakes\\nuse\\nof\\nat\\nleast\\nmathematical\\ncomputations—and\\nincreasingly\\nalso\\nmore\\ngeneral\\ncomputations.\\nAnd\\nif\\nwe\\nlook\\nat\\nthe\\nnatural\\nworld,\\nit’s\\nfull\\nof\\nirreducible\\ncomputation\\n—that\\nwe’re\\nslowly\\nunderstanding\\nhow\\nto\\nemulate\\nand\\nuse\\nfor\\nour\\ntechnological\\npurposes.\\nYes,\\na\\nneural\\nnet\\ncan\\ncertainly\\nnotice\\nthe\\nkinds\\nof\\nregularities\\nin\\nthe\\nnatural\\nworld\\nthat\\nwe\\nmight\\nalso\\nreadily\\nnotice\\nwith\\n“unaided\\nhuman\\nthinking”.\\nBut\\nif\\nwe\\nwant\\nto\\nwork\\nout\\nthings\\nthat\\nare\\nin\\nthe\\npurview\\nof\\nmathematical\\nor\\ncomputational\\nscience\\nthe\\nneural\\nnet\\nisn’t\\ngoing\\nto\\nbe\\nable\\nto\\ndo\\nit—unless\\nit\\neffectively\\n“uses\\nas\\na\\ntool”\\nan\\n“ordinary”\\ncomputational\\nsystem.\\nBut\\nthere’s\\nsomething\\npotentially\\nconfusing\\nabout\\nall\\nof\\nthis.\\nIn\\nthe\\npast\\nthere\\nwere\\nplenty\\nof\\ntasks—including\\nwriting\\nessays—that\\nwe’ve\\nassumed\\nwere\\nsomehow\\n“fundamentally\\ntoo\\nhard”\\nfor\\ncomputers.\\nAnd\\nnow\\nthat\\nwe\\nsee\\nthem\\ndone\\nby\\nthe\\nlikes\\nof\\nChatGPT\\nwe\\ntend\\nto\\nsuddenly\\nthink\\nthat\\ncomputers\\nmust\\nhave\\nbecome\\nvastly\\nmore\\npowerful—in\\nparticular\\nsurpassing\\nthings\\nthey\\nwere\\nalready\\nbasically\\nable\\nto\\ndo\\n(like\\nprogressively\\ncomputing\\nthe\\nbehavior\\nof\\ncomputational\\nsystems\\nlike\\ncellular\\nautomata).\\nBut\\nthis\\nisn’t\\nthe\\nright\\nconclusion\\nto\\ndraw.\\nComputationally\\nirreducible\\nprocesses\\nare\\nstill\\ncomputationally\\nirreducible,\\nand\\nare\\nstill\\nfundamentally\\nhard\\nfor\\ncomputers—even\\nif\\ncomputers\\ncan\\nreadily\\ncompute\\ntheir\\nindividual\\nsteps.\\nAnd\\ninstead\\nwhat\\nwe\\nshould\\nconclude\\nis\\nthat\\ntasks—like\\nwriting\\nessays—that\\nwe\\nhumans\\ncould\\ndo,\\nbut\\nwe\\ndidn’t\\nthink\\ncomputers\\ncould\\ndo,\\nare\\nactually\\nin\\nsome\\nsense\\ncomputationally\\neasier\\nthan\\nwe\\nthought.\\nIn\\nother\\nwords,\\nthe\\nreason\\na\\nneural\\nnet\\ncan\\nbe\\nsuccessful\\nin\\nwriting\\nan\\nessay\\nis\\nbecause\\nwriting\\nan\\nessay\\nturns\\nout\\nto\\nbe\\na\\n“computationally\\nshallower”\\nproblem\\nthan\\nwe\\nthought.\\nAnd\\nin\\na\\nsense\\nthis\\ntakes\\nus\\ncloser\\nto\\n“having\\na\\ntheory”\\nof\\nhow\\nwe\\nhumans\\nmanage\\nto\\ndo\\nthings\\nlike\\nwriting\\nessays,\\nor\\nin\\ngeneral\\ndeal\\nwith\\nlanguage.\\nIf\\nyou\\nhad\\na\\nbig\\nenough\\nneural\\nnet\\nthen,\\nyes,\\nyou\\nmight\\nbe\\nable\\nto\\ndo\\nwhatever\\nhumans\\ncan\\nreadily\\ndo.\\nBut\\nyou\\nwouldn’t\\ncapture\\nwhat\\nthe\\nnatural\\nworld\\nin\\ngeneral\\ncan\\ndo—or\\nthat\\nthe\\ntools\\nthat\\nwe’ve\\nfashioned\\nfrom\\nthe\\nnatural\\nworld\\ncan\\ndo.\\nAnd\\nit’s\\nthe\\nuse\\nof\\nthose\\ntools—both\\npractical\\nand\\nconceptual—that\\nhave\\nallowed\\nus\\nin\\nrecent\\ncenturies\\nto\\ntranscend\\nthe\\nboundaries\\nof\\nwhat’s\\naccessible\\nto\\n“pure\\nunaided\\nhuman\\nthought”,\\nand' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 42}\n",
            "\n",
            "Page 44:\n",
            "page_content='capture\\nfor\\nhuman\\npurposes\\nmore\\nof\\nwhat’s\\nout\\nthere\\nin\\nthe\\nphysical\\nand\\ncomputational\\nuniverse. \\nThe\\nConcept\\nof\\nEmbeddings\\nNeural\\nnets—at\\nleast\\nas\\nthey’re\\ncurrently\\nset\\nup—are\\nfundamentally\\nbased\\non\\nnumbers.\\nSo\\nif\\nwe’re\\ngoing\\nto\\nto\\nuse\\nthem\\nto\\nwork\\non\\nsomething\\nlike\\ntext\\nwe’ll\\nneed\\na\\nway\\nto\\nrepresent\\nour\\ntext\\nwith\\nnumbers\\n.\\nAnd\\ncertainly\\nwe\\ncould\\nstart\\n(essentially\\nas\\nChatGPT\\ndoes)\\nby\\njust\\nassigning\\na\\nnumber\\nto\\nevery\\nword\\nin\\nthe\\ndictionary.\\nBut\\nthere’s\\nan\\nimportant\\nidea—that’s\\nfor\\nexample\\ncentral\\nto\\nChatGPT—that\\ngoes\\nbeyond\\nthat.\\nAnd\\nit’s\\nthe\\nidea\\nof\\n“embeddings”.\\nOne\\ncan\\nthink\\nof\\nan\\nembedding\\nas\\na\\nway\\nto\\ntry\\nto\\nrepresent\\nthe\\n“essence”\\nof\\nsomething\\nby\\nan\\narray\\nof\\nnumbers—with\\nthe\\nproperty\\nthat\\n“nearby\\nthings”\\nare\\nrepresented\\nby\\nnearby\\nnumbers.\\nAnd\\nso,\\nfor\\nexample,\\nwe\\ncan\\nthink\\nof\\na\\nword\\nembedding\\nas\\ntrying\\nto\\nlay\\nout\\nwords\\nin\\na\\nkind\\nof\\n“meaning\\nspace”\\nin\\nwhich\\nwords\\nthat\\nare\\nsomehow\\n“nearby\\nin\\nmeaning”\\nappear\\nnearby\\nin\\nthe\\nembedding.\\nThe\\nactual\\nembeddings\\nthat\\nare\\nused—say\\nin\\nChatGPT—tend\\nto\\ninvolve\\nlarge\\nlists\\nof\\nnumbers.\\nBut\\nif\\nwe\\nproject\\ndown\\nto\\n2D,\\nwe\\ncan\\nshow\\nexamples\\nof\\nhow\\nwords\\nare\\nlaid\\nout\\nby\\nthe\\nembedding:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 43}\n",
            "\n",
            "Page 45:\n",
            "page_content='And,\\nyes,\\nwhat\\nwe\\nsee\\ndoes\\nremarkably\\nwell\\nin\\ncapturing\\ntypical\\neveryday\\nimpressions.\\nBut\\nhow\\ncan\\nwe\\nconstruct\\nsuch\\nan\\nembedding?\\nRoughly\\nthe\\nidea\\nis\\nto\\nlook\\nat\\nlarge\\namounts\\nof\\ntext\\n(here\\n5\\nbillion\\nwords\\nfrom\\nthe\\nweb)\\nand\\nthen\\nsee\\n“how\\nsimilar”\\nthe\\n“environments”\\nare\\nin\\nwhich\\ndifferent\\nwords\\nappear.\\nSo,\\nfor\\nexample,\\n“alligator”\\nand\\n“crocodile”\\nwill\\noften\\nappear\\nalmost\\ninterchangeably\\nin\\notherwise\\nsimilar\\nsentences,\\nand\\nthat\\nmeans\\nthey’ll\\nbe\\nplaced\\nnearby\\nin\\nthe\\nembedding.\\nBut\\n“turnip”\\nand\\n“eagle”\\nwon’t\\ntend\\nto\\nappear\\nin\\notherwise\\nsimilar\\nsentences,\\nso\\nthey’ll\\nbe\\nplaced\\nfar\\napart\\nin\\nthe\\nembedding.\\nBut\\nhow\\ndoes\\none\\nactually\\nimplement\\nsomething\\nlike\\nthis\\nusing\\nneural\\nnets?\\nLet’s\\nstart\\nby\\ntalking\\nabout\\nembeddings\\nnot\\nfor\\nwords,\\nbut\\nfor\\nimages.\\nWe\\nwant\\nto\\nfind\\nsome\\nway\\nto\\ncharacterize\\nimages\\nby\\nlists\\nof\\nnumbers\\nin' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 44}\n",
            "\n",
            "Page 46:\n",
            "page_content='such\\na\\nway\\nthat\\n“images\\nwe\\nconsider\\nsimilar”\\nare\\nassigned\\nsimilar\\nlists\\nof\\nnumbers.\\nHow\\ndo\\nwe\\ntell\\nif\\nwe\\nshould\\n“consider\\nimages\\nsimilar”?\\nWell,\\nif\\nour\\nimages\\nare,\\nsay,\\nof\\nhandwritten\\ndigits\\nwe\\nmight\\n“consider\\ntwo\\nimages\\nsimilar”\\nif\\nthey\\nare\\nof\\nthe\\nsame\\ndigit.\\nEarlier\\nwe\\ndiscussed\\na\\nneural\\nnet\\nthat\\nwas\\ntrained\\nto\\nrecognize\\nhandwritten\\ndigits.\\nAnd\\nwe\\ncan\\nthink\\nof\\nthis\\nneural\\nnet\\nas\\nbeing\\nset\\nup\\nso\\nthat\\nin\\nits\\nfinal\\noutput\\nit\\nputs\\nimages\\ninto\\n10\\ndifferent\\nbins,\\none\\nfor\\neach\\ndigit.\\nBut\\nwhat\\nif\\nwe\\n“intercept”\\nwhat’s\\ngoing\\non\\ninside\\nthe\\nneural\\nnet\\nbefore\\nthe\\nfinal\\n“it’s\\na\\n‘4’”\\ndecision\\nis\\nmade?\\nWe\\nmight\\nexpect\\nthat\\ninside\\nthe\\nneural\\nnet\\nthere\\nare\\nnumbers\\nthat\\ncharacterize\\nimages\\nas\\nbeing\\n“mostly\\n4-like\\nbut\\na\\nbit\\n2-like”\\nor\\nsome\\nsuch.\\nAnd\\nthe\\nidea\\nis\\nto\\npick\\nup\\nsuch\\nnumbers\\nto\\nuse\\nas\\nelements\\nin\\nan\\nembedding.\\nSo\\nhere’s\\nthe\\nconcept.\\nRather\\nthan\\ndirectly\\ntrying\\nto\\ncharacterize\\n“what\\nimage\\nis\\nnear\\nwhat\\nother\\nimage”,\\nwe\\ninstead\\nconsider\\na\\nwell-defined\\ntask\\n(in\\nthis\\ncase\\ndigit\\nrecognition)\\nfor\\nwhich\\nwe\\ncan\\nget\\nexplicit\\ntraining\\ndata—then\\nuse\\nthe\\nfact\\nthat\\nin\\ndoing\\nthis\\ntask\\nthe\\nneural\\nnet\\nimplicitly\\nhas\\nto\\nmake\\nwhat\\namount\\nto\\n“nearness\\ndecisions”.\\nSo\\ninstead\\nof\\nus\\never\\nexplicitly\\nhaving\\nto\\ntalk\\nabout\\n“nearness\\nof\\nimages”\\nwe’re\\njust\\ntalking\\nabout\\nthe\\nconcrete\\nquestion\\nof\\nwhat\\ndigit\\nan\\nimage\\nrepresents,\\nand\\nthen\\nwe’re\\n“leaving\\nit\\nto\\nthe\\nneural\\nnet”\\nto\\nimplicitly\\ndetermine\\nwhat\\nthat\\nimplies\\nabout\\n“nearness\\nof\\nimages”.\\nSo\\nhow\\nin\\nmore\\ndetail\\ndoes\\nthis\\nwork\\nfor\\nthe\\ndigit\\nrecognition\\nnetwork?\\nWe\\ncan\\nthink\\nof\\nthe\\nnetwork\\nas\\nconsisting\\nof\\n11\\nsuccessive\\nlayers,\\nthat\\nwe\\nmight\\nsummarize\\niconically\\nlike\\nthis\\n(with\\nactivation\\nfunctions\\nshown\\nas\\nseparate\\nlayers):\\nAt\\nthe\\nbeginning\\nwe’re\\nfeeding\\ninto\\nthe\\nfirst\\nlayer\\nactual\\nimages,\\nrepresented\\nby\\n2D\\narrays\\nof\\npixel\\nvalues.\\nAnd\\nat\\nthe\\nend—from\\nthe\\nlast\\nlayer—we’re\\ngetting\\nout\\nan\\narray\\nof\\n10\\nvalues,\\nwhich\\nwe\\ncan\\nthink\\nof\\nsaying\\n“how\\ncertain”\\nthe\\nnetwork\\nis\\nthat\\nthe\\nimage\\ncorresponds\\nto\\neach\\nof\\nthe\\ndigits\\n0\\nthrough\\n9.\\nFeed\\nin\\nthe\\nimage\\nand\\nthe\\nvalues\\nof\\nthe\\nneurons\\nin\\nthat\\nlast\\nlayer\\nare:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 45}\n",
            "\n",
            "Page 47:\n",
            "page_content='In\\nother\\nwords,\\nthe\\nneural\\nnet\\nis\\nby\\nthis\\npoint\\n“incredibly\\ncertain”\\nthat\\nthis\\nimage\\nis\\na\\n4—and\\nto\\nactually\\nget\\nthe\\noutput\\n“4”\\nwe\\njust\\nhave\\nto\\npick\\nout\\nthe\\nposition\\nof\\nthe\\nneuron\\nwith\\nthe\\nlargest\\nvalue.\\nBut\\nwhat\\nif\\nwe\\nlook\\none\\nstep\\nearlier?\\nThe\\nvery\\nlast\\noperation\\nin\\nthe\\nnetwork\\nis\\na\\nso-called\\nsoftmax\\nwhich\\ntries\\nto\\n“force\\ncertainty”.\\nBut\\nbefore\\nthat’s\\nbeen\\napplied\\nthe\\nvalues\\nof\\nthe\\nneurons\\nare:\\nThe\\nneuron\\nrepresenting\\n“4”\\nstill\\nhas\\nthe\\nhighest\\nnumerical\\nvalue.\\nBut\\nthere’s\\nalso\\ninformation\\nin\\nthe\\nvalues\\nof\\nthe\\nother\\nneurons.\\nAnd\\nwe\\ncan\\nexpect\\nthat\\nthis\\nlist\\nof\\nnumbers\\ncan\\nin\\na\\nsense\\nbe\\nused\\nto\\ncharacterize\\nthe\\n“essence”\\nof\\nthe\\nimage—and\\nthus\\nto\\nprovide\\nsomething\\nwe\\ncan\\nuse\\nas\\nan\\nembedding.\\nAnd\\nso,\\nfor\\nexample,\\neach\\nof\\nthe\\n4’s\\nhere\\nhas\\na\\nslightly\\ndifferent\\n“signature”\\n(or\\n“feature\\nembedding”)—all\\nvery\\ndifferent\\nfrom\\nthe\\n8’s:\\nHere\\nwe’re\\nessentially\\nusing\\n10\\nnumbers\\nto\\ncharacterize\\nour\\nimages.\\nBut\\nit’s\\noften\\nbetter\\nto\\nuse\\nmuch\\nmore\\nthan\\nthat.\\nAnd\\nfor\\nexample\\nin\\nour\\ndigit\\nrecognition\\nnetwork\\nwe\\ncan\\nget\\nan\\narray\\nof\\n500\\nnumbers\\nby\\ntapping\\ninto\\nthe\\npreceding\\nlayer.\\nAnd\\nthis\\nis\\nprobably\\na\\nreasonable\\narray\\nto\\nuse\\nas\\nan\\n“image\\nembedding”.\\nIf\\nwe\\nwant\\nto\\nmake\\nan\\nexplicit\\nvisualization\\nof\\n“image\\nspace”\\nfor\\nhandwritten\\ndigits\\nwe\\nneed\\nto\\n“reduce\\nthe\\ndimension”,\\neffectively\\nby\\nprojecting\\nthe\\n500-dimensional\\nvector\\nwe’ve\\ngot\\ninto,\\nsay,\\n3D\\nspace:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 46}\n",
            "\n",
            "Page 48:\n",
            "page_content='We’ve\\njust\\ntalked\\nabout\\ncreating\\na\\ncharacterization\\n(and\\nthus\\nembedding)\\nfor\\nimages\\nbased\\neffectively\\non\\nidentifying\\nthe\\nsimilarity\\nof\\nimages\\nby\\ndetermining\\nwhether\\n(according\\nto\\nour\\ntraining\\nset)\\nthey\\ncorrespond\\nto\\nthe\\nsame\\nhandwritten\\ndigit.\\nAnd\\nwe\\ncan\\ndo\\nthe\\nsame\\nthing\\nmuch\\nmore\\ngenerally\\nfor\\nimages\\nif\\nwe\\nhave\\na\\ntraining\\nset\\nthat\\nidentifies,\\nsay,\\nwhich\\nof\\n5000\\ncommon\\ntypes\\nof\\nobject\\n(cat,\\ndog,\\nchair,\\n…)\\neach\\nimage\\nis\\nof.\\nAnd\\nin\\nthis\\nway\\nwe\\ncan\\nmake\\nan\\nimage\\nembedding\\nthat’s\\n“anchored”\\nby\\nour\\nidentification\\nof\\ncommon\\nobjects,\\nbut\\nthen\\n“generalizes\\naround\\nthat”\\naccording\\nto\\nthe\\nbehavior\\nof\\nthe\\nneural\\nnet.\\nAnd\\nthe\\npoint\\nis\\nthat\\ninsofar\\nas\\nthat\\nbehavior\\naligns\\nwith\\nhow\\nwe\\nhumans\\nperceive\\nand\\ninterpret\\nimages,' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 47}\n",
            "\n",
            "Page 49:\n",
            "page_content='this\\nwill\\nend\\nup\\nbeing\\nan\\nembedding\\nthat\\n“seems\\nright\\nto\\nus”,\\nand\\nis\\nuseful\\nin\\npractice\\nin\\ndoing\\n“human-judgement-like”\\ntasks.\\nOK,\\nso\\nhow\\ndo\\nwe\\nfollow\\nthe\\nsame\\nkind\\nof\\napproach\\nto\\nfind\\nembeddings\\nfor\\nwords?\\nThe\\nkey\\nis\\nto\\nstart\\nfrom\\na\\ntask\\nabout\\nwords\\nfor\\nwhich\\nwe\\ncan\\nreadily\\ndo\\ntraining.\\nAnd\\nthe\\nstandard\\nsuch\\ntask\\nis\\n“word\\nprediction”.\\nImagine\\nwe’re\\ngiven\\n“the\\n___\\ncat”.\\nBased\\non\\na\\nlarge\\ncorpus\\nof\\ntext\\n(say,\\nthe\\ntext\\ncontent\\nof\\nthe\\nweb),\\nwhat\\nare\\nthe\\nprobabilities\\nfor\\ndifferent\\nwords\\nthat\\nmight\\n“fill\\nin\\nthe\\nblank”?\\nOr,\\nalternatively,\\ngiven\\n“___\\nblack\\n___”\\nwhat\\nare\\nthe\\nprobabilities\\nfor\\ndifferent\\n“flanking\\nwords”?\\nHow\\ndo\\nwe\\nset\\nthis\\nproblem\\nup\\nfor\\na\\nneural\\nnet?\\nUltimately\\nwe\\nhave\\nto\\nformulate\\neverything\\nin\\nterms\\nof\\nnumbers.\\nAnd\\none\\nway\\nto\\ndo\\nthis\\nis\\njust\\nto\\nassign\\na\\nunique\\nnumber\\nto\\neach\\nof\\nthe\\n50,000\\nor\\nso\\ncommon\\nwords\\nin\\nEnglish.\\nSo,\\nfor\\nexample,\\n“the”\\nmight\\nbe\\n914,\\nand\\n“\\ncat”\\n(with\\na\\nspace\\nbefore\\nit)\\nmight\\nbe\\n3542.\\n(And\\nthese\\nare\\nthe\\nactual\\nnumbers\\nused\\nby\\nGPT-2.)\\nSo\\nfor\\nthe\\n“the\\n___\\ncat”\\nproblem,\\nour\\ninput\\nmight\\nbe\\n{914,\\n3542}.\\nWhat\\nshould\\nthe\\noutput\\nbe\\nlike?\\nWell,\\nit\\nshould\\nbe\\na\\nlist\\nof\\n50,000\\nor\\nso\\nnumbers\\nthat\\neffectively\\ngive\\nthe\\nprobabilities\\nfor\\neach\\nof\\nthe\\npossible\\n“fill-in”\\nwords.\\nAnd\\nonce\\nagain,\\nto\\nfind\\nan\\nembedding,\\nwe\\nwant\\nto\\n“intercept”\\nthe\\n“insides”\\nof\\nthe\\nneural\\nnet\\njust\\nbefore\\nit\\n“reaches\\nits\\nconclusion”—and\\nthen\\npick\\nup\\nthe\\nlist\\nof\\nnumbers\\nthat\\noccur\\nthere,\\nand\\nthat\\nwe\\ncan\\nthink\\nof\\nas\\n“characterizing\\neach\\nword”.\\nOK,\\nso\\nwhat\\ndo\\nthose\\ncharacterizations\\nlook\\nlike?\\nOver\\nthe\\npast\\n10\\nyears\\nthere’ve\\nbeen\\na\\nsequence\\nof\\ndifferent\\nsystems\\ndeveloped\\n(\\nword2vec\\n,\\nGloVe\\n,\\nBERT\\n,\\nGPT\\n,\\n…),\\neach\\nbased\\non\\na\\ndifferent\\nneural\\nnet\\napproach.\\nBut\\nultimately\\nall\\nof\\nthem\\ntake\\nwords\\nand\\ncharacterize\\nthem\\nby\\nlists\\nof\\nhundreds\\nto\\nthousands\\nof\\nnumbers.\\nIn\\ntheir\\nraw\\nform,\\nthese\\n“embedding\\nvectors”\\nare\\nquite\\nuninformative.\\nFor\\nexample,\\nhere’s\\nwhat\\nGPT-2\\nproduces\\nas\\nthe\\nraw\\nembedding\\nvectors\\nfor\\nthree\\nspecific\\nwords:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 48}\n",
            "\n",
            "Page 50:\n",
            "page_content='If\\nwe\\ndo\\nthings\\nlike\\nmeasure\\ndistances\\nbetween\\nthese\\nvectors,\\nthen\\nwe\\ncan\\nfind\\nthings\\nlike\\n“nearnesses”\\nof\\nwords.\\nLater\\nwe’ll\\ndiscuss\\nin\\nmore\\ndetail\\nwhat\\nwe\\nmight\\nconsider\\nthe\\n“cognitive”\\nsignificance\\nof\\nsuch\\nembeddings.\\nBut\\nfor\\nnow\\nthe\\nmain\\npoint\\nis\\nthat\\nwe\\nhave\\na\\nway\\nto\\nusefully\\nturn\\nwords\\ninto\\n“neural-net-friendly”\\ncollections\\nof\\nnumbers.\\nBut\\nactually\\nwe\\ncan\\ngo\\nfurther\\nthan\\njust\\ncharacterizing\\nwords\\nby\\ncollections\\nof\\nnumbers;\\nwe\\ncan\\nalso\\ndo\\nthis\\nfor\\nsequences\\nof\\nwords,\\nor\\nindeed\\nwhole\\nblocks\\nof\\ntext.\\nAnd\\ninside\\nChatGPT\\nthat’s\\nhow\\nit’s\\ndealing\\nwith\\nthings.\\nIt\\ntakes\\nthe\\ntext\\nit’s\\ngot\\nso\\nfar,\\nand\\ngenerates\\nan\\nembedding\\nvector\\nto\\nrepresent\\nit.\\nThen\\nits\\ngoal\\nis\\nto\\nfind\\nthe\\nprobabilities\\nfor\\ndifferent\\nwords\\nthat\\nmight\\noccur\\nnext.\\nAnd\\nit\\nrepresents\\nits\\nanswer\\nfor\\nthis\\nas\\na\\nlist\\nof\\nnumbers\\nthat\\nessentially\\ngive\\nthe\\nprobabilities\\nfor\\neach\\nof\\nthe\\n50,000\\nor\\nso\\npossible\\nwords.\\n(Strictly,\\nChatGPT\\ndoes\\nnot\\ndeal\\nwith\\nwords,\\nbut\\nrather\\nwith\\n“tokens”\\n—convenient\\nlinguistic\\nunits\\nthat\\nmight\\nbe\\nwhole\\nwords,\\nor\\nmight\\njust\\nbe\\npieces\\nlike\\n“pre”\\nor\\n“ing”\\nor\\n“ized”.\\nWorking\\nwith\\ntokens\\nmakes\\nit\\neasier\\nfor\\nChatGPT\\nto\\nhandle\\nrare,\\ncompound\\nand\\nnon-English\\nwords,\\nand,\\nsometimes,\\nfor\\nbetter\\nor\\nworse,\\nto\\ninvent\\nnew\\nwords.) \\nInside\\nChatGPT\\nOK,\\nso\\nwe’re\\nfinally\\nready\\nto\\ndiscuss\\nwhat’s\\ninside\\nChatGPT.\\nAnd,\\nyes,\\nultimately,\\nit’s\\na\\ngiant\\nneural\\nnet—currently\\na\\nversion\\nof\\nthe\\nso-called\\nGPT-3\\nnetwork\\nwith\\n175\\nbillion\\nweights.\\nIn\\nmany\\nways\\nthis\\nis\\na\\nneural\\nnet\\nvery\\nmuch\\nlike\\nthe\\nother\\nones\\nwe’ve\\ndiscussed.\\nBut\\nit’s\\na\\nneural\\nnet\\nthat’s\\nparticularly\\nset\\nup\\nfor\\ndealing\\nwith\\nlanguage.\\nAnd\\nits\\nmost\\nnotable\\nfeature\\nis\\na\\npiece\\nof\\nneural\\nnet\\narchitecture\\ncalled\\na\\n“transformer”.\\nIn\\nthe\\nfirst\\nneural\\nnets\\nwe\\ndiscussed\\nabove,\\nevery\\nneuron\\nat\\nany\\ngiven\\nlayer\\nwas\\nbasically\\nconnected\\n(at\\nleast\\nwith\\nsome\\nweight)\\nto\\nevery\\nneuron\\non\\nthe\\nlayer\\nbefore.\\nBut\\nthis\\nkind\\nof\\nfully\\nconnected\\nnetwork\\nis\\n(presumably)\\noverkill\\nif\\none’s\\nworking\\nwith\\ndata\\nthat\\nhas\\nparticular,\\nknown\\nstructure.\\nAnd\\nthus,\\nfor\\nexample,\\nin\\nthe\\nearly\\nstages\\nof\\ndealing\\nwith\\nimages,\\nit’s\\ntypical\\nto\\nuse\\nso-called\\nconvolutional\\nneural\\nnets\\n(“convnets”)\\nin\\nwhich\\nneurons\\nare\\neffectively\\nlaid\\nout\\non\\na\\ngrid\\nanalogous\\nto\\nthe\\npixels\\nin\\nthe\\nimage—and\\nconnected\\nonly\\nto\\nneurons\\nnearby\\non\\nthe\\ngrid.\\nThe\\nidea\\nof\\ntransformers\\nis\\nto\\ndo\\nsomething\\nat\\nleast\\nsomewhat\\nsimilar\\nfor\\nsequences\\nof\\ntokens\\nthat\\nmake\\nup\\na\\npiece\\nof\\ntext.\\nBut\\ninstead\\nof\\njust' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 49}\n",
            "\n",
            "Page 51:\n",
            "page_content='defining\\na\\nfixed\\nregion\\nin\\nthe\\nsequence\\nover\\nwhich\\nthere\\ncan\\nbe\\nconnections,\\ntransformers\\ninstead\\nintroduce\\nthe\\nnotion\\nof\\n“\\nattention\\n”—and\\nthe\\nidea\\nof\\n“paying\\nattention”\\nmore\\nto\\nsome\\nparts\\nof\\nthe\\nsequence\\nthan\\nothers.\\nMaybe\\none\\nday\\nit’ll\\nmake\\nsense\\nto\\njust\\nstart\\na\\ngeneric\\nneural\\nnet\\nand\\ndo\\nall\\ncustomization\\nthrough\\ntraining.\\nBut\\nat\\nleast\\nas\\nof\\nnow\\nit\\nseems\\nto\\nbe\\ncritical\\nin\\npractice\\nto\\n“modularize”\\nthings—as\\ntransformers\\ndo,\\nand\\nprobably\\nas\\nour\\nbrains\\nalso\\ndo.\\nOK,\\nso\\nwhat\\ndoes\\nChatGPT\\n(or,\\nrather,\\nthe\\nGPT-3\\nnetwork\\non\\nwhich\\nit’s\\nbased)\\nactually\\ndo?\\nRecall\\nthat\\nits\\noverall\\ngoal\\nis\\nto\\ncontinue\\ntext\\nin\\na\\n“reasonable”\\nway,\\nbased\\non\\nwhat\\nit’s\\nseen\\nfrom\\nthe\\ntraining\\nit’s\\nhad\\n(which\\nconsists\\nin\\nlooking\\nat\\nbillions\\nof\\npages\\nof\\ntext\\nfrom\\nthe\\nweb,\\netc.)\\nSo\\nat\\nany\\ngiven\\npoint,\\nit’s\\ngot\\na\\ncertain\\namount\\nof\\ntext—and\\nits\\ngoal\\nis\\nto\\ncome\\nup\\nwith\\nan\\nappropriate\\nchoice\\nfor\\nthe\\nnext\\ntoken\\nto\\nadd.\\nIt\\noperates\\nin\\nthree\\nbasic\\nstages.\\nFirst,\\nit\\ntakes\\nthe\\nsequence\\nof\\ntokens\\nthat\\ncorresponds\\nto\\nthe\\ntext\\nso\\nfar,\\nand\\nfinds\\nan\\nembedding\\n(i.e.\\nan\\narray\\nof\\nnumbers)\\nthat\\nrepresents\\nthese.\\nThen\\nit\\noperates\\non\\nthis\\nembedding—in\\na\\n“standard\\nneural\\nnet\\nway”,\\nwith\\nvalues\\n“rippling\\nthrough”\\nsuccessive\\nlayers\\nin\\na\\nnetwork—to\\nproduce\\na\\nnew\\nembedding\\n(i.e.\\na\\nnew\\narray\\nof\\nnumbers).\\nIt\\nthen\\ntakes\\nthe\\nlast\\npart\\nof\\nthis\\narray\\nand\\ngenerates\\nfrom\\nit\\nan\\narray\\nof\\nabout\\n50,000\\nvalues\\nthat\\nturn\\ninto\\nprobabilities\\nfor\\ndifferent\\npossible\\nnext\\ntokens.\\n(And,\\nyes,\\nit\\nso\\nhappens\\nthat\\nthere\\nare\\nabout\\nthe\\nsame\\nnumber\\nof\\ntokens\\nused\\nas\\nthere\\nare\\ncommon\\nwords\\nin\\nEnglish,\\nthough\\nonly\\nabout\\n3000\\nof\\nthe\\ntokens\\nare\\nwhole\\nwords,\\nand\\nthe\\nrest\\nare\\nfragments.)\\nA\\ncritical\\npoint\\nis\\nthat\\nevery\\npart\\nof\\nthis\\npipeline\\nis\\nimplemented\\nby\\na\\nneural\\nnetwork,\\nwhose\\nweights\\nare\\ndetermined\\nby\\nend-to-end\\ntraining\\nof\\nthe\\nnetwork.\\nIn\\nother\\nwords,\\nin\\neffect\\nnothing\\nexcept\\nthe\\noverall\\narchitecture\\nis\\n“explicitly\\nengineered”;\\neverything\\nis\\njust\\n“learned”\\nfrom\\ntraining\\ndata.\\nThere\\nare,\\nhowever,\\nplenty\\nof\\ndetails\\nin\\nthe\\nway\\nthe\\narchitecture\\nis\\nset\\nup—reflecting\\nall\\nsorts\\nof\\nexperience\\nand\\nneural\\nnet\\nlore.\\nAnd—even\\nthough\\nthis\\nis\\ndefinitely\\ngoing\\ninto\\nthe\\nweeds—I\\nthink\\nit’s\\nuseful\\nto\\ntalk\\nabout\\nsome\\nof\\nthose\\ndetails,\\nnot\\nleast\\nto\\nget\\na\\nsense\\nof\\njust\\nwhat\\ngoes\\ninto\\nbuilding\\nsomething\\nlike\\nChatGPT.\\nFirst\\ncomes\\nthe\\nembedding\\nmodule.\\nHere’s\\na\\nschematic\\nWolfram\\nLanguage\\nrepresentation\\nfor\\nit\\nfor\\nGPT-2:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 50}\n",
            "\n",
            "Page 52:\n",
            "page_content='The\\ninput\\nis\\na\\nvector\\nof\\nn\\ntokens\\n(represented\\nas\\nin\\nthe\\nprevious\\nsection\\nby\\nintegers\\nfrom\\n1\\nto\\nabout\\n50,000).\\nEach\\nof\\nthese\\ntokens\\nis\\nconverted\\n(by\\na\\nsingle-layer\\nneural\\nnet\\n)\\ninto\\nan\\nembedding\\nvector\\n(of\\nlength\\n768\\nfor\\nGPT-2\\nand\\n12,288\\nfor\\nChatGPT’s\\nGPT-3).\\nMeanwhile,\\nthere’s\\na\\n“secondary\\npathway”\\nthat\\ntakes\\nthe\\nsequence\\nof\\n(integer)\\npositions\\nfor\\nthe\\ntokens,\\nand\\nfrom\\nthese\\nintegers\\ncreates\\nanother\\nembedding\\nvector.\\nAnd\\nfinally\\nthe\\nembedding\\nvectors\\nfrom\\nthe\\ntoken\\nvalue\\nand\\nthe\\ntoken\\nposition\\nare\\nadded\\ntogether\\n—to\\nproduce\\nthe\\nfinal\\nsequence\\nof\\nembedding\\nvectors\\nfrom\\nthe\\nembedding\\nmodule.\\nWhy\\ndoes\\none\\njust\\nadd\\nthe\\ntoken-value\\nand\\ntoken-position\\nembedding\\nvectors\\ntogether?\\nI\\ndon’t\\nthink\\nthere’s\\nany\\nparticular\\nscience\\nto\\nthis.\\nIt’s\\njust\\nthat\\nvarious\\ndifferent\\nthings\\nhave\\nbeen\\ntried,\\nand\\nthis\\nis\\none\\nthat\\nseems\\nto\\nwork.\\nAnd\\nit’s\\npart\\nof\\nthe\\nlore\\nof\\nneural\\nnets\\nthat—in\\nsome\\nsense—so\\nlong\\nas\\nthe\\nsetup\\none\\nhas\\nis\\n“roughly\\nright”\\nit’s\\nusually\\npossible\\nto\\nhome\\nin\\non\\ndetails\\njust\\nby\\ndoing\\nsufficient\\ntraining,\\nwithout\\never\\nreally\\nneeding\\nto\\n“understand\\nat\\nan\\nengineering\\nlevel”\\nquite\\nhow\\nthe\\nneural\\nnet\\nhas\\nended\\nup\\nconfiguring\\nitself.\\nHere’s\\nwhat\\nthe\\nembedding\\nmodule\\ndoes,\\noperating\\non\\nthe\\nstring\\nhello\\nhello\\nhello\\nhello\\nhello\\nhello\\nhello\\nhello\\nhello\\nhello\\nbye\\nbye\\nbye\\nbye\\nbye\\nbye\\nbye\\nbye\\nbye\\nbye\\n:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 51}\n",
            "\n",
            "Page 53:\n",
            "page_content='The\\nelements\\nof\\nthe\\nembedding\\nvector\\nfor\\neach\\ntoken\\nare\\nshown\\ndown\\nthe\\npage,\\nand\\nacross\\nthe\\npage\\nwe\\nsee\\nfirst\\na\\nrun\\nof\\n“\\nhello\\n”\\nembeddings,\\nfollowed\\nby\\na\\nrun\\nof\\n“\\nbye\\n”\\nones.\\nThe\\nsecond\\narray\\nabove\\nis\\nthe\\npositional\\nembedding—with\\nits\\nsomewhat-random-looking\\nstructure\\nbeing\\njust\\nwhat\\n“happened\\nto\\nbe\\nlearned”\\n(in\\nthis\\ncase\\nin\\nGPT-2).\\nOK,\\nso\\nafter\\nthe\\nembedding\\nmodule\\ncomes\\nthe\\n“main\\nevent”\\nof\\nthe\\ntransformer:\\na\\nsequence\\nof\\nso-called\\n“attention\\nblocks”\\n(12\\nfor\\nGPT-2,\\n96\\nfor\\nChatGPT’s\\nGPT-3).\\nIt’s\\nall\\npretty\\ncomplicated—and\\nreminiscent\\nof\\ntypical\\nlarge\\nhard-to-understand\\nengineering\\nsystems,\\nor,\\nfor\\nthat\\nmatter,\\nbiological\\nsystems.\\nBut\\nanyway,\\nhere’s\\na\\nschematic\\nrepresentation\\nof\\na\\nsingle\\n“attention\\nblock”\\n(for\\nGPT-2):\\nWithin\\neach\\nsuch\\nattention\\nblock\\nthere\\nare\\na\\ncollection\\nof\\n“attention\\nheads”\\n(12\\nfor\\nGPT-2,\\n96\\nfor\\nChatGPT’s\\nGPT-3)—each\\nof\\nwhich\\noperates\\nindependently\\non\\ndifferent\\nchunks\\nof\\nvalues\\nin\\nthe\\nembedding\\nvector.\\n(And,\\nyes,\\nwe\\ndon’t\\nknow\\nany\\nparticular\\nreason\\nwhy\\nit’s\\na\\ngood\\nidea\\nto\\nsplit\\nup\\nthe\\nembedding\\nvector,\\nor\\nwhat\\nthe\\ndifferent\\nparts\\nof\\nit\\n“mean”;\\nthis\\nis\\njust\\none\\nof\\nthose\\nthings\\nthat’s\\nbeen\\n“found\\nto\\nwork”.)' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 52}\n",
            "\n",
            "Page 54:\n",
            "page_content='OK,\\nso\\nwhat\\ndo\\nthe\\nattention\\nheads\\ndo?\\nBasically\\nthey’re\\na\\nway\\nof\\n“looking\\nback”\\nin\\nthe\\nsequence\\nof\\ntokens\\n(i.e.\\nin\\nthe\\ntext\\nproduced\\nso\\nfar),\\nand\\n“packaging\\nup\\nthe\\npast”\\nin\\na\\nform\\nthat’s\\nuseful\\nfor\\nfinding\\nthe\\nnext\\ntoken.\\nIn\\nthe\\nfirst\\nsection\\nabove\\nwe\\ntalked\\nabout\\nusing\\n2-gram\\nprobabilities\\nto\\npick\\nwords\\nbased\\non\\ntheir\\nimmediate\\npredecessors.\\nWhat\\nthe\\n“attention”\\nmechanism\\nin\\ntransformers\\ndoes\\nis\\nto\\nallow\\n“attention\\nto”\\neven\\nmuch\\nearlier\\nwords—thus\\npotentially\\ncapturing\\nthe\\nway,\\nsay,\\nverbs\\ncan\\nrefer\\nto\\nnouns\\nthat\\nappear\\nmany\\nwords\\nbefore\\nthem\\nin\\na\\nsentence.\\nAt\\na\\nmore\\ndetailed\\nlevel,\\nwhat\\nan\\nattention\\nhead\\ndoes\\nis\\nto\\nrecombine\\nchunks\\nin\\nthe\\nembedding\\nvectors\\nassociated\\nwith\\ndifferent\\ntokens,\\nwith\\ncertain\\nweights.\\nAnd\\nso,\\nfor\\nexample,\\nthe\\n12\\nattention\\nheads\\nin\\nthe\\nfirst\\nattention\\nblock\\n(in\\nGPT-2)\\nhave\\nthe\\nfollowing\\n(“look-back-all-the-way-to-the-beginning-of-the-sequence-of-tokens”)\\npatterns\\nof\\n“recombination\\nweights”\\nfor\\nthe\\n“\\nhello\\n,\\nbye\\n”\\nstring\\nabove:\\nAfter\\nbeing\\nprocessed\\nby\\nthe\\nattention\\nheads,\\nthe\\nresulting\\n“re-weighted\\nembedding\\nvector”\\n(of\\nlength\\n768\\nfor\\nGPT-2\\nand\\nlength\\n12,288\\nfor\\nChatGPT’s\\nGPT-3)\\nis\\npassed\\nthrough\\na\\nstandard\\n“fully\\nconnected”\\nneural\\nnet\\nlayer\\n.\\nIt’s\\nhard\\nto\\nget\\na\\nhandle\\non\\nwhat\\nthis\\nlayer\\nis\\ndoing.\\nBut\\nhere’s\\na\\nplot\\nof\\nthe\\n768×768\\nmatrix\\nof\\nweights\\nit’s\\nusing\\n(here\\nfor\\nGPT-2):' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 53}\n",
            "\n",
            "Page 55:\n",
            "page_content='Taking\\n64×64\\nmoving\\naverages,\\nsome\\n(random-walk-ish)\\nstructure\\nbegins\\nto\\nemerge:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 54}\n",
            "\n",
            "Page 56:\n",
            "page_content='What\\ndetermines\\nthis\\nstructure?\\nUltimately\\nit’s\\npresumably\\nsome\\n“neural\\nnet\\nencoding”\\nof\\nfeatures\\nof\\nhuman\\nlanguage.\\nBut\\nas\\nof\\nnow,\\nwhat\\nthose\\nfeatures\\nmight\\nbe\\nis\\nquite\\nunknown.\\nIn\\neffect,\\nwe’re\\n“opening\\nup\\nthe\\nbrain\\nof\\nChatGPT”\\n(or\\nat\\nleast\\nGPT-2)\\nand\\ndiscovering,\\nyes,\\nit’s\\ncomplicated\\nin\\nthere,\\nand\\nwe\\ndon’t\\nunderstand\\nit—even\\nthough\\nin\\nthe\\nend\\nit’s\\nproducing\\nrecognizable\\nhuman\\nlanguage.\\nOK,\\nso\\nafter\\ngoing\\nthrough\\none\\nattention\\nblock,\\nwe’ve\\ngot\\na\\nnew\\nembedding\\nvector—which\\nis\\nthen\\nsuccessively\\npassed\\nthrough\\nadditional\\nattention\\nblocks\\n(a\\ntotal\\nof\\n12\\nfor\\nGPT-2;\\n96\\nfor\\nGPT-3).\\nEach\\nattention\\nblock\\nhas\\nits\\nown\\nparticular\\npattern\\nof\\n“attention”\\nand\\n“fully\\nconnected”\\nweights.\\nHere\\nfor\\nGPT-2\\nare\\nthe\\nsequence\\nof\\nattention\\nweights\\nfor\\nthe\\n“hello,\\nbye”\\ninput,\\nfor\\nthe\\nfirst\\nattention\\nhead:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 55}\n",
            "\n",
            "Page 57:\n",
            "page_content='And\\nhere\\nare\\nthe\\n(moving-averaged)\\n“matrices”\\nfor\\nthe\\nfully\\nconnected\\nlayers:\\nCuriously,\\neven\\nthough\\nthese\\n“matrices\\nof\\nweights”\\nin\\ndifferent\\nattention\\nblocks\\nlook\\nquite\\nsimilar,\\nthe\\ndistributions\\nof\\nthe\\nsizes\\nof\\nweights\\ncan\\nbe\\nsomewhat\\ndifferent\\n(and\\nare\\nnot\\nalways\\nGaussian):' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 56}\n",
            "\n",
            "Page 58:\n",
            "page_content='So\\nafter\\ngoing\\nthrough\\nall\\nthese\\nattention\\nblocks\\nwhat\\nis\\nthe\\nnet\\neffect\\nof\\nthe\\ntransformer?\\nEssentially\\nit’s\\nto\\ntransform\\nthe\\noriginal\\ncollection\\nof\\nembeddings\\nfor\\nthe\\nsequence\\nof\\ntokens\\nto\\na\\nfinal\\ncollection.\\nAnd\\nthe\\nparticular\\nway\\nChatGPT\\nworks\\nis\\nthen\\nto\\npick\\nup\\nthe\\nlast\\nembedding\\nin\\nthis\\ncollection,\\nand\\n“decode”\\nit\\nto\\nproduce\\na\\nlist\\nof\\nprobabilities\\nfor\\nwhat\\ntoken\\nshould\\ncome\\nnext.\\nSo\\nthat’s\\nin\\noutline\\nwhat’s\\ninside\\nChatGPT.\\nIt\\nmay\\nseem\\ncomplicated\\n(not\\nleast\\nbecause\\nof\\nits\\nmany\\ninevitably\\nsomewhat\\narbitrary\\n“engineering\\nchoices”),\\nbut\\nactually\\nthe\\nultimate\\nelements\\ninvolved\\nare\\nremarkably\\nsimple.\\nBecause\\nin\\nthe\\nend\\nwhat\\nwe’re\\ndealing\\nwith\\nis\\njust\\na\\nneural\\nnet\\nmade\\nof\\n“artificial\\nneurons”,\\neach\\ndoing\\nthe\\nsimple\\noperation\\nof\\ntaking\\na\\ncollection\\nof\\nnumerical\\ninputs,\\nand\\nthen\\ncombining\\nthem\\nwith\\ncertain\\nweights.\\nThe\\noriginal\\ninput\\nto\\nChatGPT\\nis\\nan\\narray\\nof\\nnumbers\\n(the\\nembedding\\nvectors\\nfor\\nthe\\ntokens\\nso\\nfar),\\nand\\nwhat\\nhappens\\nwhen\\nChatGPT\\n“runs”\\nto\\nproduce\\na\\nnew\\ntoken\\nis\\njust\\nthat\\nthese\\nnumbers\\n“ripple\\nthrough”\\nthe\\nlayers\\nof\\nthe\\nneural\\nnet,\\nwith\\neach\\nneuron\\n“doing\\nits\\nthing”\\nand\\npassing\\nthe\\nresult\\nto\\nneurons\\non\\nthe\\nnext\\nlayer.\\nThere’s\\nno\\nlooping\\nor\\n“going\\nback”.\\nEverything\\njust\\n“feeds\\nforward”\\nthrough\\nthe\\nnetwork.\\nIt’s\\na\\nvery\\ndifferent\\nsetup\\nfrom\\na\\ntypical\\ncomputational\\nsystem—like\\na\\nTuring\\nmachine\\n—in\\nwhich\\nresults\\nare\\nrepeatedly\\n“reprocessed”\\nby\\nthe\\nsame\\ncomputational\\nelements.\\nHere—at\\nleast\\nin\\ngenerating\\na\\ngiven\\ntoken\\nof\\noutput—each\\ncomputational\\nelement\\n(i.e.\\nneuron)\\nis\\nused\\nonly\\nonce.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 57}\n",
            "\n",
            "Page 59:\n",
            "page_content='But\\nthere\\nis\\nin\\na\\nsense\\nstill\\nan\\n“outer\\nloop”\\nthat\\nreuses\\ncomputational\\nelements\\neven\\nin\\nChatGPT.\\nBecause\\nwhen\\nChatGPT\\nis\\ngoing\\nto\\ngenerate\\na\\nnew\\ntoken,\\nit\\nalways\\n“reads”\\n(i.e.\\ntakes\\nas\\ninput)\\nthe\\nwhole\\nsequence\\nof\\ntokens\\nthat\\ncome\\nbefore\\nit,\\nincluding\\ntokens\\nthat\\nChatGPT\\nitself\\nhas\\n“written”\\npreviously.\\nAnd\\nwe\\ncan\\nthink\\nof\\nthis\\nsetup\\nas\\nmeaning\\nthat\\nChatGPT\\ndoes—at\\nleast\\nat\\nits\\noutermost\\nlevel—involve\\na\\n“feedback\\nloop”,\\nalbeit\\none\\nin\\nwhich\\nevery\\niteration\\nis\\nexplicitly\\nvisible\\nas\\na\\ntoken\\nthat\\nappears\\nin\\nthe\\ntext\\nthat\\nit\\ngenerates.\\nBut\\nlet’s\\ncome\\nback\\nto\\nthe\\ncore\\nof\\nChatGPT:\\nthe\\nneural\\nnet\\nthat’s\\nbeing\\nrepeatedly\\nused\\nto\\ngenerate\\neach\\ntoken.\\nAt\\nsome\\nlevel\\nit’s\\nvery\\nsimple:\\na\\nwhole\\ncollection\\nof\\nidentical\\nartificial\\nneurons.\\nAnd\\nsome\\nparts\\nof\\nthe\\nnetwork\\njust\\nconsist\\nof\\n(“\\nfully\\nconnected\\n”)\\nlayers\\nof\\nneurons\\nin\\nwhich\\nevery\\nneuron\\non\\na\\ngiven\\nlayer\\nis\\nconnected\\n(with\\nsome\\nweight)\\nto\\nevery\\nneuron\\non\\nthe\\nlayer\\nbefore.\\nBut\\nparticularly\\nwith\\nits\\ntransformer\\narchitecture,\\nChatGPT\\nhas\\nparts\\nwith\\nmore\\nstructure,\\nin\\nwhich\\nonly\\nspecific\\nneurons\\non\\ndifferent\\nlayers\\nare\\nconnected.\\n(Of\\ncourse,\\none\\ncould\\nstill\\nsay\\nthat\\n“all\\nneurons\\nare\\nconnected”—but\\nsome\\njust\\nhave\\nzero\\nweight.)\\nIn\\naddition,\\nthere\\nare\\naspects\\nof\\nthe\\nneural\\nnet\\nin\\nChatGPT\\nthat\\naren’t\\nmost\\nnaturally\\nthought\\nof\\nas\\njust\\nconsisting\\nof\\n“homogeneous”\\nlayers.\\nAnd\\nfor\\nexample—as\\nthe\\niconic\\nsummary\\nabove\\nindicates—inside\\nan\\nattention\\nblock\\nthere\\nare\\nplaces\\nwhere\\n“multiple\\ncopies\\nare\\nmade”\\nof\\nincoming\\ndata,\\neach\\nthen\\ngoing\\nthrough\\na\\ndifferent\\n“processing\\npath”,\\npotentially\\ninvolving\\na\\ndifferent\\nnumber\\nof\\nlayers,\\nand\\nonly\\nlater\\nrecombining.\\nBut\\nwhile\\nthis\\nmay\\nbe\\na\\nconvenient\\nrepresentation\\nof\\nwhat’s\\ngoing\\non,\\nit’s\\nalways\\nat\\nleast\\nin\\nprinciple\\npossible\\nto\\nthink\\nof\\n“densely\\nfilling\\nin”\\nlayers,\\nbut\\njust\\nhaving\\nsome\\nweights\\nbe\\nzero.\\nIf\\none\\nlooks\\nat\\nthe\\nlongest\\npath\\nthrough\\nChatGPT,\\nthere\\nare\\nabout\\n400\\n(core)\\nlayers\\ninvolved—in\\nsome\\nways\\nnot\\na\\nhuge\\nnumber.\\nBut\\nthere\\nare\\nmillions\\nof\\nneurons—with\\na\\ntotal\\nof\\n175\\nbillion\\nconnections\\nand\\ntherefore\\n175\\nbillion\\nweights.\\nAnd\\none\\nthing\\nto\\nrealize\\nis\\nthat\\nevery\\ntime\\nChatGPT\\ngenerates\\na\\nnew\\ntoken,\\nit\\nhas\\nto\\ndo\\na\\ncalculation\\ninvolving\\nevery\\nsingle\\none\\nof\\nthese\\nweights.\\nImplementationally\\nthese\\ncalculations\\ncan\\nbe\\nsomewhat\\norganized\\n“by\\nlayer”\\ninto\\nhighly\\nparallel\\narray\\noperations\\nthat\\ncan\\nconveniently\\nbe\\ndone\\non\\nGPUs.\\nBut\\nfor\\neach\\ntoken\\nthat’s\\nproduced,\\nthere\\nstill\\nhave\\nto\\nbe\\n175\\nbillion\\ncalculations\\ndone\\n(and\\nin\\nthe\\nend\\na\\nbit\\nmore)—so\\nthat,\\nyes,\\nit’s\\nnot\\nsurprising\\nthat\\nit\\ncan\\ntake\\na\\nwhile\\nto\\ngenerate\\na\\nlong\\npiece\\nof\\ntext\\nwith\\nChatGPT.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 58}\n",
            "\n",
            "Page 60:\n",
            "page_content='But\\nin\\nthe\\nend,\\nthe\\nremarkable\\nthing\\nis\\nthat\\nall\\nthese\\noperations—individually\\nas\\nsimple\\nas\\nthey\\nare—can\\nsomehow\\ntogether\\nmanage\\nto\\ndo\\nsuch\\na\\ngood\\n“human-like”\\njob\\nof\\ngenerating\\ntext.\\nIt\\nhas\\nto\\nbe\\nemphasized\\nagain\\nthat\\n(at\\nleast\\nso\\nfar\\nas\\nwe\\nknow)\\nthere’s\\nno\\n“ultimate\\ntheoretical\\nreason”\\nwhy\\nanything\\nlike\\nthis\\nshould\\nwork.\\nAnd\\nin\\nfact,\\nas\\nwe’ll\\ndiscuss,\\nI\\nthink\\nwe\\nhave\\nto\\nview\\nthis\\nas\\na—potentially\\nsurprising—scientific\\ndiscovery:\\nthat\\nsomehow\\nin\\na\\nneural\\nnet\\nlike\\nChatGPT’s\\nit’s\\npossible\\nto\\ncapture\\nthe\\nessence\\nof\\nwhat\\nhuman\\nbrains\\nmanage\\nto\\ndo\\nin\\ngenerating\\nlanguage.\\nThe\\nTraining\\nof\\nChatGPT\\nOK,\\nso\\nwe’ve\\nnow\\ngiven\\nan\\noutline\\nof\\nhow\\nChatGPT\\nworks\\nonce\\nit’s\\nset\\nup.\\nBut\\nhow\\ndid\\nit\\nget\\nset\\nup?\\nHow\\nwere\\nall\\nthose\\n175\\nbillion\\nweights\\nin\\nits\\nneural\\nnet\\ndetermined?\\nBasically\\nthey’re\\nthe\\nresult\\nof\\nvery\\nlarge-scale\\ntraining,\\nbased\\non\\na\\nhuge\\ncorpus\\nof\\ntext—on\\nthe\\nweb,\\nin\\nbooks,\\netc.—written\\nby\\nhumans.\\nAs\\nwe’ve\\nsaid,\\neven\\ngiven\\nall\\nthat\\ntraining\\ndata,\\nit’s\\ncertainly\\nnot\\nobvious\\nthat\\na\\nneural\\nnet\\nwould\\nbe\\nable\\nto\\nsuccessfully\\nproduce\\n“human-like”\\ntext.\\nAnd,\\nonce\\nagain,\\nthere\\nseem\\nto\\nbe\\ndetailed\\npieces\\nof\\nengineering\\nneeded\\nto\\nmake\\nthat\\nhappen.\\nBut\\nthe\\nbig\\nsurprise—and\\ndiscovery—of\\nChatGPT\\nis\\nthat\\nit’s\\npossible\\nat\\nall.\\nAnd\\nthat—in\\neffect—a\\nneural\\nnet\\nwith\\n“just”\\n175\\nbillion\\nweights\\ncan\\nmake\\na\\n“reasonable\\nmodel”\\nof\\ntext\\nhumans\\nwrite.\\nIn\\nmodern\\ntimes,\\nthere’s\\nlots\\nof\\ntext\\nwritten\\nby\\nhumans\\nthat’s\\nout\\nthere\\nin\\ndigital\\nform.\\nThe\\npublic\\nweb\\nhas\\nat\\nleast\\nseveral\\nbillion\\nhuman-written\\npages,\\nwith\\naltogether\\nperhaps\\na\\ntrillion\\nwords\\nof\\ntext.\\nAnd\\nif\\none\\nincludes\\nnon-public\\nwebpages,\\nthe\\nnumbers\\nmight\\nbe\\nat\\nleast\\n100\\ntimes\\nlarger.\\nSo\\nfar,\\nmore\\nthan\\n5\\nmillion\\ndigitized\\nbooks\\nhave\\nbeen\\nmade\\navailable\\n(out\\nof\\n100\\nmillion\\nor\\nso\\nthat\\nhave\\never\\nbeen\\npublished),\\ngiving\\nanother\\n100\\nbillion\\nor\\nso\\nwords\\nof\\ntext.\\nAnd\\nthat’s\\nnot\\neven\\nmentioning\\ntext\\nderived\\nfrom\\nspeech\\nin\\nvideos,\\netc.\\n(As\\na\\npersonal\\ncomparison,\\nmy\\ntotal\\nlifetime\\noutput\\nof\\npublished\\nmaterial\\nhas\\nbeen\\na\\nbit\\nunder\\n3\\nmillion\\nwords,\\nand\\nover\\nthe\\npast\\n30\\nyears\\nI’ve\\nwritten\\nabout\\n15\\nmillion\\nwords\\nof\\nemail,\\nand\\naltogether\\ntyped\\nperhaps\\n50\\nmillion\\nwords—and\\nin\\njust\\nthe\\npast\\ncouple\\nof\\nyears\\nI’ve\\nspoken\\nmore\\nthan\\n10\\nmillion\\nwords\\non\\nlivestreams\\n.\\nAnd,\\nyes,\\nI’ll\\ntrain\\na\\nbot\\nfrom\\nall\\nof\\nthat.)\\nBut,\\nOK,\\ngiven\\nall\\nthis\\ndata,\\nhow\\ndoes\\none\\ntrain\\na\\nneural\\nnet\\nfrom\\nit?\\nThe\\nbasic\\nprocess\\nis\\nvery\\nmuch\\nas\\nwe\\ndiscussed\\nit\\nin\\nthe\\nsimple\\nexamples\\nabove.\\nYou\\npresent\\na\\nbatch\\nof\\nexamples,\\nand\\nthen\\nyou\\nadjust\\nthe\\nweights\\nin\\nthe' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 59}\n",
            "\n",
            "Page 61:\n",
            "page_content='network\\nto\\nminimize\\nthe\\nerror\\n(“loss”)\\nthat\\nthe\\nnetwork\\nmakes\\non\\nthose\\nexamples.\\nThe\\nmain\\nthing\\nthat’s\\nexpensive\\nabout\\n“back\\npropagating”\\nfrom\\nthe\\nerror\\nis\\nthat\\neach\\ntime\\nyou\\ndo\\nthis,\\nevery\\nweight\\nin\\nthe\\nnetwork\\nwill\\ntypically\\nchange\\nat\\nleast\\na\\ntiny\\nbit,\\nand\\nthere\\nare\\njust\\na\\nlot\\nof\\nweights\\nto\\ndeal\\nwith.\\n(The\\nactual\\n“back\\ncomputation”\\nis\\ntypically\\nonly\\na\\nsmall\\nconstant\\nfactor\\nharder\\nthan\\nthe\\nforward\\none.)\\nWith\\nmodern\\nGPU\\nhardware,\\nit’s\\nstraightforward\\nto\\ncompute\\nthe\\nresults\\nfrom\\nbatches\\nof\\nthousands\\nof\\nexamples\\nin\\nparallel.\\nBut\\nwhen\\nit\\ncomes\\nto\\nactually\\nupdating\\nthe\\nweights\\nin\\nthe\\nneural\\nnet,\\ncurrent\\nmethods\\nrequire\\none\\nto\\ndo\\nthis\\nbasically\\nbatch\\nby\\nbatch.\\n(And,\\nyes,\\nthis\\nis\\nprobably\\nwhere\\nactual\\nbrains—with\\ntheir\\ncombined\\ncomputation\\nand\\nmemory\\nelements—have,\\nfor\\nnow,\\nat\\nleast\\nan\\narchitectural\\nadvantage.)\\nEven\\nin\\nthe\\nseemingly\\nsimple\\ncases\\nof\\nlearning\\nnumerical\\nfunctions\\nthat\\nwe\\ndiscussed\\nearlier,\\nwe\\nfound\\nwe\\noften\\nhad\\nto\\nuse\\nmillions\\nof\\nexamples\\nto\\nsuccessfully\\ntrain\\na\\nnetwork,\\nat\\nleast\\nfrom\\nscratch.\\nSo\\nhow\\nmany\\nexamples\\ndoes\\nthis\\nmean\\nwe’ll\\nneed\\nin\\norder\\nto\\ntrain\\na\\n“human-like\\nlanguage”\\nmodel?\\nThere\\ndoesn’t\\nseem\\nto\\nbe\\nany\\nfundamental\\n“theoretical”\\nway\\nto\\nknow.\\nBut\\nin\\npractice\\nChatGPT\\nwas\\nsuccessfully\\ntrained\\non\\na\\nfew\\nhundred\\nbillion\\nwords\\nof\\ntext.\\nSome\\nof\\nthe\\ntext\\nit\\nwas\\nfed\\nseveral\\ntimes,\\nsome\\nof\\nit\\nonly\\nonce.\\nBut\\nsomehow\\nit\\n“got\\nwhat\\nit\\nneeded”\\nfrom\\nthe\\ntext\\nit\\nsaw.\\nBut\\ngiven\\nthis\\nvolume\\nof\\ntext\\nto\\nlearn\\nfrom,\\nhow\\nlarge\\na\\nnetwork\\nshould\\nit\\nrequire\\nto\\n“learn\\nit\\nwell”?\\nAgain,\\nwe\\ndon’t\\nyet\\nhave\\na\\nfundamental\\ntheoretical\\nway\\nto\\nsay.\\nUltimately—as\\nwe’ll\\ndiscuss\\nfurther\\nbelow—there’s\\npresumably\\na\\ncertain\\n“total\\nalgorithmic\\ncontent”\\nto\\nhuman\\nlanguage\\nand\\nwhat\\nhumans\\ntypically\\nsay\\nwith\\nit.\\nBut\\nthe\\nnext\\nquestion\\nis\\nhow\\nefficient\\na\\nneural\\nnet\\nwill\\nbe\\nat\\nimplementing\\na\\nmodel\\nbased\\non\\nthat\\nalgorithmic\\ncontent.\\nAnd\\nagain\\nwe\\ndon’t\\nknow—although\\nthe\\nsuccess\\nof\\nChatGPT\\nsuggests\\nit’s\\nreasonably\\nefficient.\\nAnd\\nin\\nthe\\nend\\nwe\\ncan\\njust\\nnote\\nthat\\nChatGPT\\ndoes\\nwhat\\nit\\ndoes\\nusing\\na\\ncouple\\nhundred\\nbillion\\nweights—comparable\\nin\\nnumber\\nto\\nthe\\ntotal\\nnumber\\nof\\nwords\\n(or\\ntokens)\\nof\\ntraining\\ndata\\nit’s\\nbeen\\ngiven.\\nIn\\nsome\\nways\\nit’s\\nperhaps\\nsurprising\\n(though\\nempirically\\nobserved\\nalso\\nin\\nsmaller\\nanalogs\\nof\\nChatGPT)\\nthat\\nthe\\n“size\\nof\\nthe\\nnetwork”\\nthat\\nseems\\nto\\nwork\\nwell\\nis\\nso\\ncomparable\\nto\\nthe\\n“size\\nof\\nthe\\ntraining\\ndata”.\\nAfter\\nall,\\nit’s\\ncertainly\\nnot\\nthat\\nsomehow\\n“inside\\nChatGPT”\\nall\\nthat\\ntext\\nfrom\\nthe\\nweb\\nand\\nbooks\\nand\\nso\\non\\nis\\n“directly\\nstored”.\\nBecause\\nwhat’s\\nactually\\ninside\\nChatGPT\\nare\\na\\nbunch\\nof' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 60}\n",
            "\n",
            "Page 62:\n",
            "page_content='numbers—with\\na\\nbit\\nless\\nthan\\n10\\ndigits\\nof\\nprecision—that\\nare\\nsome\\nkind\\nof\\ndistributed\\nencoding\\nof\\nthe\\naggregate\\nstructure\\nof\\nall\\nthat\\ntext.\\nPut\\nanother\\nway,\\nwe\\nmight\\nask\\nwhat\\nthe\\n“effective\\ninformation\\ncontent”\\nis\\nof\\nhuman\\nlanguage\\nand\\nwhat’s\\ntypically\\nsaid\\nwith\\nit.\\nThere’s\\nthe\\nraw\\ncorpus\\nof\\nexamples\\nof\\nlanguage.\\nAnd\\nthen\\nthere’s\\nthe\\nrepresentation\\nin\\nthe\\nneural\\nnet\\nof\\nChatGPT.\\nThat\\nrepresentation\\nis\\nvery\\nlikely\\nfar\\nfrom\\nthe\\n“algorithmically\\nminimal”\\nrepresentation\\n(as\\nwe’ll\\ndiscuss\\nbelow).\\nBut\\nit’s\\na\\nrepresentation\\nthat’s\\nreadily\\nusable\\nby\\nthe\\nneural\\nnet.\\nAnd\\nin\\nthis\\nrepresentation\\nit\\nseems\\nthere’s\\nin\\nthe\\nend\\nrather\\nlittle\\n“compression”\\nof\\nthe\\ntraining\\ndata;\\nit\\nseems\\non\\naverage\\nto\\nbasically\\ntake\\nonly\\na\\nbit\\nless\\nthan\\none\\nneural\\nnet\\nweight\\nto\\ncarry\\nthe\\n“information\\ncontent”\\nof\\na\\nword\\nof\\ntraining\\ndata.\\nWhen\\nwe\\nrun\\nChatGPT\\nto\\ngenerate\\ntext,\\nwe’re\\nbasically\\nhaving\\nto\\nuse\\neach\\nweight\\nonce.\\nSo\\nif\\nthere\\nare\\nn\\nweights,\\nwe’ve\\ngot\\nof\\norder\\nn\\ncomputational\\nsteps\\nto\\ndo—though\\nin\\npractice\\nmany\\nof\\nthem\\ncan\\ntypically\\nbe\\ndone\\nin\\nparallel\\nin\\nGPUs.\\nBut\\nif\\nwe\\nneed\\nabout\\nn\\nwords\\nof\\ntraining\\ndata\\nto\\nset\\nup\\nthose\\nweights,\\nthen\\nfrom\\nwhat\\nwe’ve\\nsaid\\nabove\\nwe\\ncan\\nconclude\\nthat\\nwe’ll\\nneed\\nabout\\nn\\n2\\ncomputational\\nsteps\\nto\\ndo\\nthe\\ntraining\\nof\\nthe\\nnetwork—which\\nis\\nwhy,\\nwith\\ncurrent\\nmethods,\\none\\nends\\nup\\nneeding\\nto\\ntalk\\nabout\\nbillion-dollar\\ntraining\\nefforts. \\nBeyond\\nBasic\\nTraining\\nThe\\nmajority\\nof\\nthe\\neffort\\nin\\ntraining\\nChatGPT\\nis\\nspent\\n“showing\\nit”\\nlarge\\namounts\\nof\\nexisting\\ntext\\nfrom\\nthe\\nweb,\\nbooks,\\netc.\\nBut\\nit\\nturns\\nout\\nthere’s\\nanother—apparently\\nrather\\nimportant—part\\ntoo.\\nAs\\nsoon\\nas\\nit’s\\nfinished\\nits\\n“raw\\ntraining”\\nfrom\\nthe\\noriginal\\ncorpus\\nof\\ntext\\nit’s\\nbeen\\nshown,\\nthe\\nneural\\nnet\\ninside\\nChatGPT\\nis\\nready\\nto\\nstart\\ngenerating\\nits\\nown\\ntext,\\ncontinuing\\nfrom\\nprompts,\\netc.\\nBut\\nwhile\\nthe\\nresults\\nfrom\\nthis\\nmay\\noften\\nseem\\nreasonable,\\nthey\\ntend—particularly\\nfor\\nlonger\\npieces\\nof\\ntext—to\\n“wander\\noff”\\nin\\noften\\nrather\\nnon-human-like\\nways.\\nIt’s\\nnot\\nsomething\\none\\ncan\\nreadily\\ndetect,\\nsay,\\nby\\ndoing\\ntraditional\\nstatistics\\non\\nthe\\ntext.\\nBut\\nit’s\\nsomething\\nthat\\nactual\\nhumans\\nreading\\nthe\\ntext\\neasily\\nnotice.\\nAnd\\na\\nkey\\nidea\\nin\\nthe\\nconstruction\\nof\\nChatGPT\\nwas\\nto\\nhave\\nanother\\nstep\\nafter\\n“passively\\nreading”\\nthings\\nlike\\nthe\\nweb:\\nto\\nhave\\nactual\\nhumans\\nactively\\ninteract\\nwith\\nChatGPT,\\nsee\\nwhat\\nit\\nproduces,\\nand\\nin\\neffect\\ngive\\nit\\nfeedback\\non\\n“how\\nto\\nbe\\na\\ngood\\nchatbot”.\\nBut\\nhow\\ncan\\nthe\\nneural\\nnet\\nuse\\nthat' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 61}\n",
            "\n",
            "Page 63:\n",
            "page_content='feedback?\\nThe\\nfirst\\nstep\\nis\\njust\\nto\\nhave\\nhumans\\nrate\\nresults\\nfrom\\nthe\\nneural\\nnet.\\nBut\\nthen\\nanother\\nneural\\nnet\\nmodel\\nis\\nbuilt\\nthat\\nattempts\\nto\\npredict\\nthose\\nratings.\\nBut\\nnow\\nthis\\nprediction\\nmodel\\ncan\\nbe\\nrun—essentially\\nlike\\na\\nloss\\nfunction—on\\nthe\\noriginal\\nnetwork,\\nin\\neffect\\nallowing\\nthat\\nnetwork\\nto\\nbe\\n“tuned\\nup”\\nby\\nthe\\nhuman\\nfeedback\\nthat’s\\nbeen\\ngiven.\\nAnd\\nthe\\nresults\\nin\\npractice\\nseem\\nto\\nhave\\na\\nbig\\neffect\\non\\nthe\\nsuccess\\nof\\nthe\\nsystem\\nin\\nproducing\\n“human-like”\\noutput.\\nIn\\ngeneral,\\nit’s\\ninteresting\\nhow\\nlittle\\n“poking”\\nthe\\n“originally\\ntrained”\\nnetwork\\nseems\\nto\\nneed\\nto\\nget\\nit\\nto\\nusefully\\ngo\\nin\\nparticular\\ndirections.\\nOne\\nmight\\nhave\\nthought\\nthat\\nto\\nhave\\nthe\\nnetwork\\nbehave\\nas\\nif\\nit’s\\n“learned\\nsomething\\nnew”\\none\\nwould\\nhave\\nto\\ngo\\nin\\nand\\nrun\\na\\ntraining\\nalgorithm,\\nadjusting\\nweights,\\nand\\nso\\non.\\nBut\\nthat’s\\nnot\\nthe\\ncase.\\nInstead,\\nit\\nseems\\nto\\nbe\\nsufficient\\nto\\nbasically\\ntell\\nChatGPT\\nsomething\\none\\ntime—as\\npart\\nof\\nthe\\nprompt\\nyou\\ngive—and\\nthen\\nit\\ncan\\nsuccessfully\\nmake\\nuse\\nof\\nwhat\\nyou\\ntold\\nit\\nwhen\\nit\\ngenerates\\ntext.\\nAnd\\nonce\\nagain,\\nthe\\nfact\\nthat\\nthis\\nworks\\nis,\\nI\\nthink,\\nan\\nimportant\\nclue\\nin\\nunderstanding\\nwhat\\nChatGPT\\nis\\n“really\\ndoing”\\nand\\nhow\\nit\\nrelates\\nto\\nthe\\nstructure\\nof\\nhuman\\nlanguage\\nand\\nthinking.\\nThere’s\\ncertainly\\nsomething\\nrather\\nhuman-like\\nabout\\nit:\\nthat\\nat\\nleast\\nonce\\nit’s\\nhad\\nall\\nthat\\npre-training\\nyou\\ncan\\ntell\\nit\\nsomething\\njust\\nonce\\nand\\nit\\ncan\\n“remember\\nit”—at\\nleast\\n“long\\nenough”\\nto\\ngenerate\\na\\npiece\\nof\\ntext\\nusing\\nit.\\nSo\\nwhat’s\\ngoing\\non\\nin\\na\\ncase\\nlike\\nthis?\\nIt\\ncould\\nbe\\nthat\\n“everything\\nyou\\nmight\\ntell\\nit\\nis\\nalready\\nin\\nthere\\nsomewhere”—and\\nyou’re\\njust\\nleading\\nit\\nto\\nthe\\nright\\nspot.\\nBut\\nthat\\ndoesn’t\\nseem\\nplausible.\\nInstead,\\nwhat\\nseems\\nmore\\nlikely\\nis\\nthat,\\nyes,\\nthe\\nelements\\nare\\nalready\\nin\\nthere,\\nbut\\nthe\\nspecifics\\nare\\ndefined\\nby\\nsomething\\nlike\\na\\n“trajectory\\nbetween\\nthose\\nelements”\\nand\\nthat’s\\nwhat\\nyou’re\\nintroducing\\nwhen\\nyou\\ntell\\nit\\nsomething.\\nAnd\\nindeed,\\nmuch\\nlike\\nfor\\nhumans,\\nif\\nyou\\ntell\\nit\\nsomething\\nbizarre\\nand\\nunexpected\\nthat\\ncompletely\\ndoesn’t\\nfit\\ninto\\nthe\\nframework\\nit\\nknows,\\nit\\ndoesn’t\\nseem\\nlike\\nit’ll\\nsuccessfully\\nbe\\nable\\nto\\n“integrate”\\nthis.\\nIt\\ncan\\n“integrate”\\nit\\nonly\\nif\\nit’s\\nbasically\\nriding\\nin\\na\\nfairly\\nsimple\\nway\\non\\ntop\\nof\\nthe\\nframework\\nit\\nalready\\nhas.\\nIt’s\\nalso\\nworth\\npointing\\nout\\nagain\\nthat\\nthere\\nare\\ninevitably\\n“algorithmic\\nlimits”\\nto\\nwhat\\nthe\\nneural\\nnet\\ncan\\n“pick\\nup”.\\nTell\\nit\\n“shallow”\\nrules\\nof\\nthe\\nform\\n“this\\ngoes\\nto\\nthat”,\\netc.,\\nand\\nthe\\nneural\\nnet\\nwill\\nmost\\nlikely\\nbe\\nable\\nto\\nrepresent\\nand\\nreproduce\\nthese\\njust\\nfine—and\\nindeed\\nwhat\\nit\\n“already' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 62}\n",
            "\n",
            "Page 64:\n",
            "page_content='knows”\\nfrom\\nlanguage\\nwill\\ngive\\nit\\nan\\nimmediate\\npattern\\nto\\nfollow.\\nBut\\ntry\\nto\\ngive\\nit\\nrules\\nfor\\nan\\nactual\\n“deep”\\ncomputation\\nthat\\ninvolves\\nmany\\npotentially\\ncomputationally\\nirreducible\\nsteps\\nand\\nit\\njust\\nwon’t\\nwork.\\n(Remember\\nthat\\nat\\neach\\nstep\\nit’s\\nalways\\njust\\n“feeding\\ndata\\nforward”\\nin\\nits\\nnetwork,\\nnever\\nlooping\\nexcept\\nby\\nvirtue\\nof\\ngenerating\\nnew\\ntokens.)\\nOf\\ncourse,\\nthe\\nnetwork\\ncan\\nlearn\\nthe\\nanswer\\nto\\nspecific\\n“irreducible”\\ncomputations.\\nBut\\nas\\nsoon\\nas\\nthere\\nare\\ncombinatorial\\nnumbers\\nof\\npossibilities,\\nno\\nsuch\\n“table-lookup-style”\\napproach\\nwill\\nwork.\\nAnd\\nso,\\nyes,\\njust\\nlike\\nhumans,\\nit’s\\ntime\\nthen\\nfor\\nneural\\nnets\\nto\\n“reach\\nout”\\nand\\nuse\\nactual\\ncomputational\\ntools.\\n(And,\\nyes,\\nWolfram|Alpha\\nand\\nWolfram\\nLanguage\\nare\\nuniquely\\nsuitable\\n,\\nbecause\\nthey’ve\\nbeen\\nbuilt\\nto\\n“talk\\nabout\\nthings\\nin\\nthe\\nworld”,\\njust\\nlike\\nthe\\nlanguage-model\\nneural\\nnets.) \\nWhat\\nReally\\nLets\\nChatGPT\\nWork?\\nHuman\\nlanguage—and\\nthe\\nprocesses\\nof\\nthinking\\ninvolved\\nin\\ngenerating\\nit—have\\nalways\\nseemed\\nto\\nrepresent\\na\\nkind\\nof\\npinnacle\\nof\\ncomplexity.\\nAnd\\nindeed\\nit’s\\nseemed\\nsomewhat\\nremarkable\\nthat\\nhuman\\nbrains—with\\ntheir\\nnetwork\\nof\\na\\n“mere”\\n100\\nbillion\\nor\\nso\\nneurons\\n(and\\nmaybe\\n100\\ntrillion\\nconnections)\\ncould\\nbe\\nresponsible\\nfor\\nit.\\nPerhaps,\\none\\nmight\\nhave\\nimagined,\\nthere’s\\nsomething\\nmore\\nto\\nbrains\\nthan\\ntheir\\nnetworks\\nof\\nneurons—like\\nsome\\nnew\\nlayer\\nof\\nundiscovered\\nphysics.\\nBut\\nnow\\nwith\\nChatGPT\\nwe’ve\\ngot\\nan\\nimportant\\nnew\\npiece\\nof\\ninformation:\\nwe\\nknow\\nthat\\na\\npure,\\nartificial\\nneural\\nnetwork\\nwith\\nabout\\nas\\nmany\\nconnections\\nas\\nbrains\\nhave\\nneurons\\nis\\ncapable\\nof\\ndoing\\na\\nsurprisingly\\ngood\\njob\\nof\\ngenerating\\nhuman\\nlanguage.\\nAnd,\\nyes,\\nthat’s\\nstill\\na\\nbig\\nand\\ncomplicated\\nsystem—with\\nabout\\nas\\nmany\\nneural\\nnet\\nweights\\nas\\nthere\\nare\\nwords\\nof\\ntext\\ncurrently\\navailable\\nout\\nthere\\nin\\nthe\\nworld.\\nBut\\nat\\nsome\\nlevel\\nit\\nstill\\nseems\\ndifficult\\nto\\nbelieve\\nthat\\nall\\nthe\\nrichness\\nof\\nlanguage\\nand\\nthe\\nthings\\nit\\ncan\\ntalk\\nabout\\ncan\\nbe\\nencapsulated\\nin\\nsuch\\na\\nfinite\\nsystem.\\nPart\\nof\\nwhat’s\\ngoing\\non\\nis\\nno\\ndoubt\\na\\nreflection\\nof\\nthe\\nubiquitous\\nphenomenon\\n(that\\nfirst\\nbecame\\nevident\\nin\\nthe\\nexample\\nof\\nrule\\n30\\n)\\nthat\\ncomputational\\nprocesses\\ncan\\nin\\neffect\\ngreatly\\namplify\\nthe\\napparent\\ncomplexity\\nof\\nsystems\\neven\\nwhen\\ntheir\\nunderlying\\nrules\\nare\\nsimple.\\nBut,\\nactually,\\nas\\nwe\\ndiscussed\\nabove,\\nneural\\nnets\\nof\\nthe\\nkind\\nused\\nin\\nChatGPT\\ntend\\nto\\nbe\\nspecifically\\nconstructed\\nto\\nrestrict\\nthe\\neffect\\nof\\nthis\\nphenomenon—and\\nthe\\ncomputational\\nirreducibility\\nassociated\\nwith\\nit—in\\nthe\\ninterest\\nof\\nmaking\\ntheir\\ntraining\\nmore\\naccessible.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 63}\n",
            "\n",
            "Page 65:\n",
            "page_content='So\\nhow\\nis\\nit,\\nthen,\\nthat\\nsomething\\nlike\\nChatGPT\\ncan\\nget\\nas\\nfar\\nas\\nit\\ndoes\\nwith\\nlanguage?\\nThe\\nbasic\\nanswer,\\nI\\nthink,\\nis\\nthat\\nlanguage\\nis\\nat\\na\\nfundamental\\nlevel\\nsomehow\\nsimpler\\nthan\\nit\\nseems.\\nAnd\\nthis\\nmeans\\nthat\\nChatGPT—even\\nwith\\nits\\nultimately\\nstraightforward\\nneural\\nnet\\nstructure—is\\nsuccessfully\\nable\\nto\\n“capture\\nthe\\nessence”\\nof\\nhuman\\nlanguage\\nand\\nthe\\nthinking\\nbehind\\nit.\\nAnd\\nmoreover,\\nin\\nits\\ntraining,\\nChatGPT\\nhas\\nsomehow\\n“implicitly\\ndiscovered”\\nwhatever\\nregularities\\nin\\nlanguage\\n(and\\nthinking)\\nmake\\nthis\\npossible.\\nThe\\nsuccess\\nof\\nChatGPT\\nis,\\nI\\nthink,\\ngiving\\nus\\nevidence\\nof\\na\\nfundamental\\nand\\nimportant\\npiece\\nof\\nscience:\\nit’s\\nsuggesting\\nthat\\nwe\\ncan\\nexpect\\nthere\\nto\\nbe\\nmajor\\nnew\\n“laws\\nof\\nlanguage”—and\\neffectively\\n“laws\\nof\\nthought”—out\\nthere\\nto\\ndiscover.\\nIn\\nChatGPT—built\\nas\\nit\\nis\\nas\\na\\nneural\\nnet—those\\nlaws\\nare\\nat\\nbest\\nimplicit.\\nBut\\nif\\nwe\\ncould\\nsomehow\\nmake\\nthe\\nlaws\\nexplicit,\\nthere’s\\nthe\\npotential\\nto\\ndo\\nthe\\nkinds\\nof\\nthings\\nChatGPT\\ndoes\\nin\\nvastly\\nmore\\ndirect,\\nefficient—and\\ntransparent—ways.\\nBut,\\nOK,\\nso\\nwhat\\nmight\\nthese\\nlaws\\nbe\\nlike?\\nUltimately\\nthey\\nmust\\ngive\\nus\\nsome\\nkind\\nof\\nprescription\\nfor\\nhow\\nlanguage—and\\nthe\\nthings\\nwe\\nsay\\nwith\\nit—are\\nput\\ntogether.\\nLater\\nwe’ll\\ndiscuss\\nhow\\n“looking\\ninside\\nChatGPT”\\nmay\\nbe\\nable\\nto\\ngive\\nus\\nsome\\nhints\\nabout\\nthis,\\nand\\nhow\\nwhat\\nwe\\nknow\\nfrom\\nbuilding\\ncomputational\\nlanguage\\nsuggests\\na\\npath\\nforward.\\nBut\\nfirst\\nlet’s\\ndiscuss\\ntwo\\nlong-known\\nexamples\\nof\\nwhat\\namount\\nto\\n“laws\\nof\\nlanguage”—and\\nhow\\nthey\\nrelate\\nto\\nthe\\noperation\\nof\\nChatGPT.\\nThe\\nfirst\\nis\\nthe\\nsyntax\\nof\\nlanguage.\\nLanguage\\nis\\nnot\\njust\\na\\nrandom\\njumble\\nof\\nwords.\\nInstead,\\nthere\\nare\\n(fairly)\\ndefinite\\ngrammatical\\nrules\\nfor\\nhow\\nwords\\nof\\ndifferent\\nkinds\\ncan\\nbe\\nput\\ntogether:\\nin\\nEnglish,\\nfor\\nexample,\\nnouns\\ncan\\nbe\\npreceded\\nby\\nadjectives\\nand\\nfollowed\\nby\\nverbs,\\nbut\\ntypically\\ntwo\\nnouns\\ncan’t\\nbe\\nright\\nnext\\nto\\neach\\nother.\\nSuch\\ngrammatical\\nstructure\\ncan\\n(at\\nleast\\napproximately)\\nbe\\ncaptured\\nby\\na\\nset\\nof\\nrules\\nthat\\ndefine\\nhow\\nwhat\\namount\\nto\\n“parse\\ntrees”\\ncan\\nbe\\nput\\ntogether\\n:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 64}\n",
            "\n",
            "Page 66:\n",
            "page_content='ChatGPT\\ndoesn’t\\nhave\\nany\\nexplicit\\n“knowledge”\\nof\\nsuch\\nrules.\\nBut\\nsomehow\\nin\\nits\\ntraining\\nit\\nimplicitly\\n“discovers”\\nthem—and\\nthen\\nseems\\nto\\nbe\\ngood\\nat\\nfollowing\\nthem.\\nSo\\nhow\\ndoes\\nthis\\nwork?\\nAt\\na\\n“big\\npicture”\\nlevel\\nit’s\\nnot\\nclear.\\nBut\\nto\\nget\\nsome\\ninsight\\nit’s\\nperhaps\\ninstructive\\nto\\nlook\\nat\\na\\nmuch\\nsimpler\\nexample.\\nConsider\\na\\n“language”\\nformed\\nfrom\\nsequences\\nof\\n(’s\\nand\\n)’s,\\nwith\\na\\ngrammar\\nthat\\nspecifies\\nthat\\nparentheses\\nshould\\nalways\\nbe\\nbalanced,\\nas\\nrepresented\\nby\\na\\nparse\\ntree\\nlike:\\nCan\\nwe\\ntrain\\na\\nneural\\nnet\\nto\\nproduce\\n“grammatically\\ncorrect”\\nparenthesis\\nsequences?\\nThere\\nare\\nvarious\\nways\\nto\\nhandle\\nsequences\\nin\\nneural\\nnets,\\nbut\\nlet’s\\nuse\\ntransformer\\nnets,\\nas\\nChatGPT\\ndoes.\\nAnd\\ngiven\\na\\nsimple\\ntransformer\\nnet,\\nwe\\ncan\\nstart\\nfeeding\\nit\\ngrammatically\\ncorrect\\nparenthesis\\nsequences\\nas\\ntraining\\nexamples.\\nA\\nsubtlety\\n(which\\nactually\\nalso\\nappears\\nin\\nChatGPT’s\\ngeneration\\nof\\nhuman\\nlanguage)\\nis\\nthat\\nin\\naddition\\nto\\nour\\n“content\\ntokens”\\n(here\\n“(”\\nand\\n“)”)\\nwe\\nhave\\nto\\ninclude\\nan\\n“End”\\ntoken,\\nthat’s\\ngenerated\\nto\\nindicate\\nthat\\nthe\\noutput\\nshouldn’t\\ncontinue\\nany\\nfurther\\n(i.e.\\nfor\\nChatGPT,\\nthat\\none’s\\nreached\\nthe\\n“end\\nof\\nthe\\nstory”).\\nIf\\nwe\\nset\\nup\\na\\ntransformer\\nnet\\nwith\\njust\\none\\nattention\\nblock\\nwith\\n8\\nheads\\nand\\nfeature\\nvectors\\nof\\nlength\\n128\\n(ChatGPT\\nalso\\nuses\\nfeature\\nvectors\\nof' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 65}\n",
            "\n",
            "Page 67:\n",
            "page_content='length\\n128,\\nbut\\nhas\\n96\\nattention\\nblocks,\\neach\\nwith\\n96\\nheads)\\nthen\\nit\\ndoesn’t\\nseem\\npossible\\nto\\nget\\nit\\nto\\nlearn\\nmuch\\nabout\\nparenthesis\\nlanguage.\\nBut\\nwith\\n2\\nattention\\nblocks,\\nthe\\nlearning\\nprocess\\nseems\\nto\\nconverge—at\\nleast\\nafter\\n10\\nmillion\\nor\\nso\\nexamples\\nhave\\nbeen\\ngiven\\n(and,\\nas\\nis\\ncommon\\nwith\\ntransformer\\nnets,\\nshowing\\nyet\\nmore\\nexamples\\njust\\nseems\\nto\\ndegrade\\nits\\nperformance).\\nSo\\nwith\\nthis\\nnetwork,\\nwe\\ncan\\ndo\\nthe\\nanalog\\nof\\nwhat\\nChatGPT\\ndoes,\\nand\\nask\\nfor\\nprobabilities\\nfor\\nwhat\\nthe\\nnext\\ntoken\\nshould\\nbe—in\\na\\nparenthesis\\nsequence:\\nAnd\\nin\\nthe\\nfirst\\ncase,\\nthe\\nnetwork\\nis\\n“pretty\\nsure”\\nthat\\nthe\\nsequence\\ncan’t\\nend\\nhere—which\\nis\\ngood,\\nbecause\\nif\\nit\\ndid,\\nthe\\nparentheses\\nwould\\nbe\\nleft\\nunbalanced.\\nIn\\nthe\\nsecond\\ncase,\\nhowever,\\nit\\n“correctly\\nrecognizes”\\nthat\\nthe\\nsequence\\ncan\\nend\\nhere,\\nthough\\nit\\nalso\\n“points\\nout”\\nthat\\nit’s\\npossible\\nto\\n“start\\nagain”,\\nputting\\ndown\\na\\n“(”,\\npresumably\\nwith\\na\\n“)”\\nto\\nfollow.\\nBut,\\noops,\\neven\\nwith\\nits\\n400,000\\nor\\nso\\nlaboriously\\ntrained\\nweights,\\nit\\nsays\\nthere’s\\na\\n15%\\nprobability\\nto\\nhave\\n“)”\\nas\\nthe\\nnext\\ntoken—which\\nisn’t\\nright,\\nbecause\\nthat\\nwould\\nnecessarily\\nlead\\nto\\nan\\nunbalanced\\nparenthesis.\\nHere’s\\nwhat\\nwe\\nget\\nif\\nwe\\nask\\nthe\\nnetwork\\nfor\\nthe\\nhighest-probability\\ncompletions\\nfor\\nprogressively\\nlonger\\nsequences\\nof\\n(’s:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 66}\n",
            "\n",
            "Page 68:\n",
            "page_content='And,\\nyes,\\nup\\nto\\na\\ncertain\\nlength\\nthe\\nnetwork\\ndoes\\njust\\nfine.\\nBut\\nthen\\nit\\nstarts\\nfailing.\\nIt’s\\na\\npretty\\ntypical\\nkind\\nof\\nthing\\nto\\nsee\\nin\\na\\n“precise”\\nsituation\\nlike\\nthis\\nwith\\na\\nneural\\nnet\\n(or\\nwith\\nmachine\\nlearning\\nin\\ngeneral).\\nCases\\nthat\\na\\nhuman\\n“can\\nsolve\\nin\\na\\nglance”\\nthe\\nneural\\nnet\\ncan\\nsolve\\ntoo.\\nBut\\ncases\\nthat\\nrequire\\ndoing\\nsomething\\n“more\\nalgorithmic”\\n(e.g.\\nexplicitly\\ncounting\\nparentheses\\nto\\nsee\\nif\\nthey’re\\nclosed)\\nthe\\nneural\\nnet\\ntends\\nto\\nsomehow\\nbe\\n“too\\ncomputationally\\nshallow”\\nto\\nreliably\\ndo.\\n(By\\nthe\\nway,\\neven\\nthe\\nfull\\ncurrent\\nChatGPT\\nhas\\na\\nhard\\ntime\\ncorrectly\\nmatching\\nparentheses\\nin\\nlong\\nsequences.)\\nSo\\nwhat\\ndoes\\nthis\\nmean\\nfor\\nthings\\nlike\\nChatGPT\\nand\\nthe\\nsyntax\\nof\\na\\nlanguage\\nlike\\nEnglish?\\nThe\\nparenthesis\\nlanguage\\nis\\n“austere”—and\\nmuch\\nmore\\nof\\nan\\n“algorithmic\\nstory”.\\nBut\\nin\\nEnglish\\nit’s\\nmuch\\nmore\\nrealistic\\nto\\nbe' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 67}\n",
            "\n",
            "Page 69:\n",
            "page_content='able\\nto\\n“guess”\\nwhat’s\\ngrammatically\\ngoing\\nto\\nfit\\non\\nthe\\nbasis\\nof\\nlocal\\nchoices\\nof\\nwords\\nand\\nother\\nhints.\\nAnd,\\nyes,\\nthe\\nneural\\nnet\\nis\\nmuch\\nbetter\\nat\\nthis—even\\nthough\\nperhaps\\nit\\nmight\\nmiss\\nsome\\n“formally\\ncorrect”\\ncase\\nthat,\\nwell,\\nhumans\\nmight\\nmiss\\nas\\nwell.\\nBut\\nthe\\nmain\\npoint\\nis\\nthat\\nthe\\nfact\\nthat\\nthere’s\\nan\\noverall\\nsyntactic\\nstructure\\nto\\nthe\\nlanguage—with\\nall\\nthe\\nregularity\\nthat\\nimplies—in\\na\\nsense\\nlimits\\n“how\\nmuch”\\nthe\\nneural\\nnet\\nhas\\nto\\nlearn.\\nAnd\\na\\nkey\\n“natural-science-like”\\nobservation\\nis\\nthat\\nthe\\ntransformer\\narchitecture\\nof\\nneural\\nnets\\nlike\\nthe\\none\\nin\\nChatGPT\\nseems\\nto\\nsuccessfully\\nbe\\nable\\nto\\nlearn\\nthe\\nkind\\nof\\nnested-tree-like\\nsyntactic\\nstructure\\nthat\\nseems\\nto\\nexist\\n(at\\nleast\\nin\\nsome\\napproximation)\\nin\\nall\\nhuman\\nlanguages.\\nSyntax\\nprovides\\none\\nkind\\nof\\nconstraint\\non\\nlanguage.\\nBut\\nthere\\nare\\nclearly\\nmore.\\nA\\nsentence\\nlike\\n“Inquisitive\\nelectrons\\neat\\nblue\\ntheories\\nfor\\nfish”\\nis\\ngrammatically\\ncorrect\\nbut\\nisn’t\\nsomething\\none\\nwould\\nnormally\\nexpect\\nto\\nsay,\\nand\\nwouldn’t\\nbe\\nconsidered\\na\\nsuccess\\nif\\nChatGPT\\ngenerated\\nit—because,\\nwell,\\nwith\\nthe\\nnormal\\nmeanings\\nfor\\nthe\\nwords\\nin\\nit,\\nit’s\\nbasically\\nmeaningless.\\nBut\\nis\\nthere\\na\\ngeneral\\nway\\nto\\ntell\\nif\\na\\nsentence\\nis\\nmeaningful?\\nThere’s\\nno\\ntraditional\\noverall\\ntheory\\nfor\\nthat.\\nBut\\nit’s\\nsomething\\nthat\\none\\ncan\\nthink\\nof\\nChatGPT\\nas\\nhaving\\nimplicitly\\n“developed\\na\\ntheory\\nfor”\\nafter\\nbeing\\ntrained\\nwith\\nbillions\\nof\\n(presumably\\nmeaningful)\\nsentences\\nfrom\\nthe\\nweb,\\netc.\\nWhat\\nmight\\nthis\\ntheory\\nbe\\nlike?\\nWell,\\nthere’s\\none\\ntiny\\ncorner\\nthat’s\\nbasically\\nbeen\\nknown\\nfor\\ntwo\\nmillennia,\\nand\\nthat’s\\nlogic\\n.\\nAnd\\ncertainly\\nin\\nthe\\nsyllogistic\\nform\\nin\\nwhich\\nAristotle\\ndiscovered\\nit,\\nlogic\\nis\\nbasically\\na\\nway\\nof\\nsaying\\nthat\\nsentences\\nthat\\nfollow\\ncertain\\npatterns\\nare\\nreasonable,\\nwhile\\nothers\\nare\\nnot.\\nThus,\\nfor\\nexample,\\nit’s\\nreasonable\\nto\\nsay\\n“All\\nX\\nare\\nY.\\nThis\\nis\\nnot\\nY,\\nso\\nit’s\\nnot\\nan\\nX”\\n(as\\nin\\n“All\\nfishes\\nare\\nblue.\\nThis\\nis\\nnot\\nblue,\\nso\\nit’s\\nnot\\na\\nfish.”).\\nAnd\\njust\\nas\\none\\ncan\\nsomewhat\\nwhimsically\\nimagine\\nthat\\nAristotle\\ndiscovered\\nsyllogistic\\nlogic\\nby\\ngoing\\n(“machine-learning-style”)\\nthrough\\nlots\\nof\\nexamples\\nof\\nrhetoric,\\nso\\ntoo\\none\\ncan\\nimagine\\nthat\\nin\\nthe\\ntraining\\nof\\nChatGPT\\nit\\nwill\\nhave\\nbeen\\nable\\nto\\n“discover\\nsyllogistic\\nlogic”\\nby\\nlooking\\nat\\nlots\\nof\\ntext\\non\\nthe\\nweb,\\netc.\\n(And,\\nyes,\\nwhile\\none\\ncan\\ntherefore\\nexpect\\nChatGPT\\nto\\nproduce\\ntext\\nthat\\ncontains\\n“correct\\ninferences”\\nbased\\non\\nthings\\nlike\\nsyllogistic\\nlogic,\\nit’s\\na\\nquite\\ndifferent\\nstory\\nwhen\\nit\\ncomes\\nto\\nmore\\nsophisticated\\nformal\\nlogic—and\\nI\\nthink\\none\\ncan\\nexpect\\nit\\nto\\nfail\\nhere\\nfor\\nthe\\nsame\\nkind\\nof\\nreasons\\nit\\nfails\\nin\\nparenthesis\\nmatching.)' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 68}\n",
            "\n",
            "Page 70:\n",
            "page_content='But\\nbeyond\\nthe\\nnarrow\\nexample\\nof\\nlogic,\\nwhat\\ncan\\nbe\\nsaid\\nabout\\nhow\\nto\\nsystematically\\nconstruct\\n(or\\nrecognize)\\neven\\nplausibly\\nmeaningful\\ntext?\\nYes,\\nthere\\nare\\nthings\\nlike\\nMad\\nLibs\\nthat\\nuse\\nvery\\nspecific\\n“phrasal\\ntemplates”.\\nBut\\nsomehow\\nChatGPT\\nimplicitly\\nhas\\na\\nmuch\\nmore\\ngeneral\\nway\\nto\\ndo\\nit.\\nAnd\\nperhaps\\nthere’s\\nnothing\\nto\\nbe\\nsaid\\nabout\\nhow\\nit\\ncan\\nbe\\ndone\\nbeyond\\n“somehow\\nit\\nhappens\\nwhen\\nyou\\nhave\\n175\\nbillion\\nneural\\nnet\\nweights”.\\nBut\\nI\\nstrongly\\nsuspect\\nthat\\nthere’s\\na\\nmuch\\nsimpler\\nand\\nstronger\\nstory. \\nMeaning\\nSpace\\nand\\nSemantic\\nLaws\\nof\\nMotion\\nWe\\ndiscussed\\nabove\\nthat\\ninside\\nChatGPT\\nany\\npiece\\nof\\ntext\\nis\\neffectively\\nrepresented\\nby\\nan\\narray\\nof\\nnumbers\\nthat\\nwe\\ncan\\nthink\\nof\\nas\\ncoordinates\\nof\\na\\npoint\\nin\\nsome\\nkind\\nof\\n“linguistic\\nfeature\\nspace”.\\nSo\\nwhen\\nChatGPT\\ncontinues\\na\\npiece\\nof\\ntext\\nthis\\ncorresponds\\nto\\ntracing\\nout\\na\\ntrajectory\\nin\\nlinguistic\\nfeature\\nspace.\\nBut\\nnow\\nwe\\ncan\\nask\\nwhat\\nmakes\\nthis\\ntrajectory\\ncorrespond\\nto\\ntext\\nwe\\nconsider\\nmeaningful.\\nAnd\\nmight\\nthere\\nperhaps\\nbe\\nsome\\nkind\\nof\\n“semantic\\nlaws\\nof\\nmotion”\\nthat\\ndefine—or\\nat\\nleast\\nconstrain—how\\npoints\\nin\\nlinguistic\\nfeature\\nspace\\ncan\\nmove\\naround\\nwhile\\npreserving\\n“meaningfulness”?\\nSo\\nwhat\\nis\\nthis\\nlinguistic\\nfeature\\nspace\\nlike?\\nHere’s\\nan\\nexample\\nof\\nhow\\nsingle\\nwords\\n(here,\\ncommon\\nnouns)\\nmight\\nget\\nlaid\\nout\\nif\\nwe\\nproject\\nsuch\\na\\nfeature\\nspace\\ndown\\nto\\n2D:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 69}\n",
            "\n",
            "Page 71:\n",
            "page_content='We\\nsaw\\nanother\\nexample\\nabove\\nbased\\non\\nwords\\nrepresenting\\nplants\\nand\\nanimals.\\nBut\\nthe\\npoint\\nin\\nboth\\ncases\\nis\\nthat\\n“semantically\\nsimilar\\nwords”\\nare\\nplaced\\nnearby.\\nAs\\nanother\\nexample,\\nhere’s\\nhow\\nwords\\ncorresponding\\nto\\ndifferent\\nparts\\nof\\nspeech\\nget\\nlaid\\nout:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 70}\n",
            "\n",
            "Page 72:\n",
            "page_content='Of\\ncourse,\\na\\ngiven\\nword\\ndoesn’t\\nin\\ngeneral\\njust\\nhave\\n“one\\nmeaning”\\n(or\\nnecessarily\\ncorrespond\\nto\\njust\\none\\npart\\nof\\nspeech).\\nAnd\\nby\\nlooking\\nat\\nhow\\nsentences\\ncontaining\\na\\nword\\nlay\\nout\\nin\\nfeature\\nspace,\\none\\ncan\\noften\\n“tease\\napart”\\ndifferent\\nmeanings—as\\nin\\nthe\\nexample\\nhere\\nfor\\nthe\\nword\\n“crane”\\n(bird\\nor\\nmachine?):' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 71}\n",
            "\n",
            "Page 73:\n",
            "page_content='OK,\\nso\\nit’s\\nat\\nleast\\nplausible\\nthat\\nwe\\ncan\\nthink\\nof\\nthis\\nfeature\\nspace\\nas\\nplacing\\n“words\\nnearby\\nin\\nmeaning”\\nclose\\nin\\nthis\\nspace.\\nBut\\nwhat\\nkind\\nof\\nadditional\\nstructure\\ncan\\nwe\\nidentify\\nin\\nthis\\nspace?\\nIs\\nthere\\nfor\\nexample\\nsome\\nkind\\nof\\nnotion\\nof\\n“parallel\\ntransport”\\nthat\\nwould\\nreflect\\n“flatness”\\nin\\nthe\\nspace?\\nOne\\nway\\nto\\nget\\na\\nhandle\\non\\nthat\\nis\\nto\\nlook\\nat\\nanalogies:\\nAnd,\\nyes,\\neven\\nwhen\\nwe\\nproject\\ndown\\nto\\n2D,\\nthere’s\\noften\\nat\\nleast\\na\\n“hint\\nof\\nflatness”,\\nthough\\nit’s\\ncertainly\\nnot\\nuniversally\\nseen.\\nSo\\nwhat\\nabout\\ntrajectories?\\nWe\\ncan\\nlook\\nat\\nthe\\ntrajectory\\nthat\\na\\nprompt\\nfor\\nChatGPT\\nfollows\\nin\\nfeature\\nspace—and\\nthen\\nwe\\ncan\\nsee\\nhow\\nChatGPT\\ncontinues\\nthat:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 72}\n",
            "\n",
            "Page 74:\n",
            "page_content='There’s\\ncertainly\\nno\\n“geometrically\\nobvious”\\nlaw\\nof\\nmotion\\nhere.\\nAnd\\nthat’s\\nnot\\nat\\nall\\nsurprising;\\nwe\\nfully\\nexpect\\nthis\\nto\\nbe\\na\\nconsiderably\\nmore\\ncomplicated\\nstory\\n.\\nAnd,\\nfor\\nexample,\\nit’s\\nfar\\nfrom\\nobvious\\nthat\\neven\\nif\\nthere\\nis\\na\\n“semantic\\nlaw\\nof\\nmotion”\\nto\\nbe\\nfound,\\nwhat\\nkind\\nof\\nembedding\\n(or,\\nin\\neffect,\\nwhat\\n“variables”)\\nit’ll\\nmost\\nnaturally\\nbe\\nstated\\nin.\\nIn\\nthe\\npicture\\nabove,\\nwe’re\\nshowing\\nseveral\\nsteps\\nin\\nthe\\n“trajectory”—where\\nat\\neach\\nstep\\nwe’re\\npicking\\nthe\\nword\\nthat\\nChatGPT\\nconsiders\\nthe\\nmost\\nprobable\\n(the\\n“zero\\ntemperature”\\ncase).\\nBut\\nwe\\ncan\\nalso\\nask\\nwhat\\nwords\\ncan\\n“come\\nnext”\\nwith\\nwhat\\nprobabilities\\nat\\na\\ngiven\\npoint:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 73}\n",
            "\n",
            "Page 75:\n",
            "page_content='And\\nwhat\\nwe\\nsee\\nin\\nthis\\ncase\\nis\\nthat\\nthere’s\\na\\n“fan”\\nof\\nhigh-probability\\nwords\\nthat\\nseems\\nto\\ngo\\nin\\na\\nmore\\nor\\nless\\ndefinite\\ndirection\\nin\\nfeature\\nspace.\\nWhat\\nhappens\\nif\\nwe\\ngo\\nfurther?\\nHere\\nare\\nthe\\nsuccessive\\n“fans”\\nthat\\nappear\\nas\\nwe\\n“move\\nalong”\\nthe\\ntrajectory:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 74}\n",
            "\n",
            "Page 76:\n",
            "page_content='Here’s\\na\\n3D\\nrepresentation,\\ngoing\\nfor\\na\\ntotal\\nof\\n40\\nsteps:' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 75}\n",
            "\n",
            "Page 77:\n",
            "page_content='And,\\nyes,\\nthis\\nseems\\nlike\\na\\nmess—and\\ndoesn’t\\ndo\\nanything\\nto\\nparticularly\\nencourage\\nthe\\nidea\\nthat\\none\\ncan\\nexpect\\nto\\nidentify\\n“mathematical-physics-like”\\n“semantic\\nlaws\\nof\\nmotion”\\nby\\nempirically\\nstudying\\n“what\\nChatGPT\\nis\\ndoing\\ninside”.\\nBut\\nperhaps\\nwe’re\\njust\\nlooking\\nat\\nthe\\n“wrong\\nvariables”\\n(or\\nwrong\\ncoordinate\\nsystem)\\nand\\nif\\nonly\\nwe\\nlooked\\nat\\nthe\\nright\\none,\\nwe’d\\nimmediately\\nsee\\nthat\\nChatGPT\\nis\\ndoing\\nsomething\\n“mathematical-physics-simple”\\nlike\\nfollowing\\ngeodesics.\\nBut\\nas\\nof\\nnow,\\nwe’re\\nnot\\nready\\nto\\n“empirically\\ndecode”\\nfrom\\nits\\n“internal\\nbehavior”\\nwhat\\nChatGPT\\nhas\\n“discovered”\\nabout\\nhow\\nhuman\\nlanguage\\nis\\n“put\\ntogether”. \\nSemantic\\nGrammar\\nand\\nthe\\nPower\\nof\\nComputational\\nLanguage\\nWhat\\ndoes\\nit\\ntake\\nto\\nproduce\\n“meaningful\\nhuman\\nlanguage”?\\nIn\\nthe\\npast,\\nwe\\nmight\\nhave\\nassumed\\nit\\ncould\\nbe\\nnothing\\nshort\\nof\\na\\nhuman\\nbrain.\\nBut\\nnow\\nwe\\nknow\\nit\\ncan\\nbe\\ndone\\nquite\\nrespectably\\nby\\nthe\\nneural\\nnet\\nof\\nChatGPT.\\nStill,\\nmaybe\\nthat’s\\nas\\nfar\\nas\\nwe\\ncan\\ngo,\\nand\\nthere’ll\\nbe\\nnothing' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 76}\n",
            "\n",
            "Page 78:\n",
            "page_content='simpler—or\\nmore\\nhuman\\nunderstandable—that\\nwill\\nwork.\\nBut\\nmy\\nstrong\\nsuspicion\\nis\\nthat\\nthe\\nsuccess\\nof\\nChatGPT\\nimplicitly\\nreveals\\nan\\nimportant\\n“scientific”\\nfact:\\nthat\\nthere’s\\nactually\\na\\nlot\\nmore\\nstructure\\nand\\nsimplicity\\nto\\nmeaningful\\nhuman\\nlanguage\\nthan\\nwe\\never\\nknew—and\\nthat\\nin\\nthe\\nend\\nthere\\nmay\\nbe\\neven\\nfairly\\nsimple\\nrules\\nthat\\ndescribe\\nhow\\nsuch\\nlanguage\\ncan\\nbe\\nput\\ntogether.\\nAs\\nwe\\nmentioned\\nabove,\\nsyntactic\\ngrammar\\ngives\\nrules\\nfor\\nhow\\nwords\\ncorresponding\\nto\\nthings\\nlike\\ndifferent\\nparts\\nof\\nspeech\\ncan\\nbe\\nput\\ntogether\\nin\\nhuman\\nlanguage.\\nBut\\nto\\ndeal\\nwith\\nmeaning,\\nwe\\nneed\\nto\\ngo\\nfurther.\\nAnd\\none\\nversion\\nof\\nhow\\nto\\ndo\\nthis\\nis\\nto\\nthink\\nabout\\nnot\\njust\\na\\nsyntactic\\ngrammar\\nfor\\nlanguage,\\nbut\\nalso\\na\\nsemantic\\none.\\nFor\\npurposes\\nof\\nsyntax,\\nwe\\nidentify\\nthings\\nlike\\nnouns\\nand\\nverbs.\\nBut\\nfor\\npurposes\\nof\\nsemantics,\\nwe\\nneed\\n“finer\\ngradations”.\\nSo,\\nfor\\nexample,\\nwe\\nmight\\nidentify\\nthe\\nconcept\\nof\\n“moving”,\\nand\\nthe\\nconcept\\nof\\nan\\n“object”\\nthat\\n“maintains\\nits\\nidentity\\nindependent\\nof\\nlocation”.\\nThere\\nare\\nendless\\nspecific\\nexamples\\nof\\neach\\nof\\nthese\\n“semantic\\nconcepts”.\\nBut\\nfor\\nthe\\npurposes\\nof\\nour\\nsemantic\\ngrammar,\\nwe’ll\\njust\\nhave\\nsome\\ngeneral\\nkind\\nof\\nrule\\nthat\\nbasically\\nsays\\nthat\\n“objects”\\ncan\\n“move”.\\nThere’s\\na\\nlot\\nto\\nsay\\nabout\\nhow\\nall\\nthis\\nmight\\nwork\\n(\\nsome\\nof\\nwhich\\nI’ve\\nsaid\\nbefore\\n).\\nBut\\nI’ll\\ncontent\\nmyself\\nhere\\nwith\\njust\\na\\nfew\\nremarks\\nthat\\nindicate\\nsome\\nof\\nthe\\npotential\\npath\\nforward.\\nIt’s\\nworth\\nmentioning\\nthat\\neven\\nif\\na\\nsentence\\nis\\nperfectly\\nOK\\naccording\\nto\\nthe\\nsemantic\\ngrammar,\\nthat\\ndoesn’t\\nmean\\nit’s\\nbeen\\nrealized\\n(or\\neven\\ncould\\nbe\\nrealized)\\nin\\npractice.\\n“The\\nelephant\\ntraveled\\nto\\nthe\\nMoon”\\nwould\\ndoubtless\\n“pass”\\nour\\nsemantic\\ngrammar,\\nbut\\nit\\ncertainly\\nhasn’t\\nbeen\\nrealized\\n(at\\nleast\\nyet)\\nin\\nour\\nactual\\nworld—though\\nit’s\\nabsolutely\\nfair\\ngame\\nfor\\na\\nfictional\\nworld.\\nWhen\\nwe\\nstart\\ntalking\\nabout\\n“semantic\\ngrammar”\\nwe’re\\nsoon\\nled\\nto\\nask\\n“What’s\\nunderneath\\nit?”\\nWhat\\n“model\\nof\\nthe\\nworld”\\nis\\nit\\nassuming?\\nA\\nsyntactic\\ngrammar\\nis\\nreally\\njust\\nabout\\nthe\\nconstruction\\nof\\nlanguage\\nfrom\\nwords.\\nBut\\na\\nsemantic\\ngrammar\\nnecessarily\\nengages\\nwith\\nsome\\nkind\\nof\\n“model\\nof\\nthe\\nworld”—something\\nthat\\nserves\\nas\\na\\n“skeleton”\\non\\ntop\\nof\\nwhich\\nlanguage\\nmade\\nfrom\\nactual\\nwords\\ncan\\nbe\\nlayered.\\nUntil\\nrecent\\ntimes,\\nwe\\nmight\\nhave\\nimagined\\nthat\\n(human)\\nlanguage\\nwould\\nbe\\nthe\\nonly\\ngeneral\\nway\\nto\\ndescribe\\nour\\n“model\\nof\\nthe\\nworld”.\\nAlready\\na\\nfew\\ncenturies\\nago\\nthere\\nstarted\\nto\\nbe\\nformalizations\\nof\\nspecific\\nkinds\\nof\\nthings,' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 77}\n",
            "\n",
            "Page 79:\n",
            "page_content='based\\nparticularly\\non\\nmathematics.\\nBut\\nnow\\nthere’s\\na\\nmuch\\nmore\\ngeneral\\napproach\\nto\\nformalization:\\ncomputational\\nlanguage\\n.\\nAnd,\\nyes,\\nthat’s\\nbeen\\nmy\\nbig\\nproject\\nover\\nthe\\ncourse\\nof\\nmore\\nthan\\nfour\\ndecades\\n(as\\nnow\\nembodied\\nin\\nthe\\nWolfram\\nLanguage\\n):\\nto\\ndevelop\\na\\nprecise\\nsymbolic\\nrepresentation\\nthat\\ncan\\ntalk\\nas\\nbroadly\\nas\\npossible\\nabout\\nthings\\nin\\nthe\\nworld,\\nas\\nwell\\nas\\nabstract\\nthings\\nthat\\nwe\\ncare\\nabout.\\nAnd\\nso,\\nfor\\nexample,\\nwe\\nhave\\nsymbolic\\nrepresentations\\nfor\\ncities\\nand\\nmolecules\\nand\\nimages\\nand\\nneural\\nnetworks\\n,\\nand\\nwe\\nhave\\nbuilt-in\\nknowledge\\nabout\\nhow\\nto\\ncompute\\nabout\\nthose\\nthings.\\nAnd,\\nafter\\ndecades\\nof\\nwork,\\nwe’ve\\ncovered\\na\\nlot\\nof\\nareas\\nin\\nthis\\nway.\\nBut\\nin\\nthe\\npast,\\nwe\\nhaven’t\\nparticularly\\ndealt\\nwith\\n“\\neveryday\\ndiscourse\\n”.\\nIn\\n“I\\nbought\\ntwo\\npounds\\nof\\napples”\\nwe\\ncan\\nreadily\\nrepresent\\n(and\\ndo\\nnutrition\\nand\\nother\\ncomputations\\non)\\nthe\\n“two\\npounds\\nof\\napples”.\\nBut\\nwe\\ndon’t\\n(quite\\nyet)\\nhave\\na\\nsymbolic\\nrepresentation\\nfor\\n“I\\nbought”.\\nIt’s\\nall\\nconnected\\nto\\nthe\\nidea\\nof\\nsemantic\\ngrammar—and\\nthe\\ngoal\\nof\\nhaving\\na\\ngeneric\\nsymbolic\\n“construction\\nkit”\\nfor\\nconcepts,\\nthat\\nwould\\ngive\\nus\\nrules\\nfor\\nwhat\\ncould\\nfit\\ntogether\\nwith\\nwhat,\\nand\\nthus\\nfor\\nthe\\n“flow”\\nof\\nwhat\\nwe\\nmight\\nturn\\ninto\\nhuman\\nlanguage.\\nBut\\nlet’s\\nsay\\nwe\\nhad\\nthis\\n“symbolic\\ndiscourse\\nlanguage”.\\nWhat\\nwould\\nwe\\ndo\\nwith\\nit?\\nWe\\ncould\\nstart\\noff\\ndoing\\nthings\\nlike\\ngenerating\\n“locally\\nmeaningful\\ntext”.\\nBut\\nultimately\\nwe’re\\nlikely\\nto\\nwant\\nmore\\n“globally\\nmeaningful”\\nresults—which\\nmeans\\n“computing”\\nmore\\nabout\\nwhat\\ncan\\nactually\\nexist\\nor\\nhappen\\nin\\nthe\\nworld\\n(or\\nperhaps\\nin\\nsome\\nconsistent\\nfictional\\nworld).\\nRight\\nnow\\nin\\nWolfram\\nLanguage\\nwe\\nhave\\na\\nhuge\\namount\\nof\\nbuilt-in\\ncomputational\\nknowledge\\nabout\\nlots\\nof\\nkinds\\nof\\nthings.\\nBut\\nfor\\na\\ncomplete\\nsymbolic\\ndiscourse\\nlanguage\\nwe’d\\nhave\\nto\\nbuild\\nin\\nadditional\\n“calculi”\\nabout\\ngeneral\\nthings\\nin\\nthe\\nworld:\\nif\\nan\\nobject\\nmoves\\nfrom\\nA\\nto\\nB\\nand\\nfrom\\nB\\nto\\nC,\\nthen\\nit’s\\nmoved\\nfrom\\nA\\nto\\nC,\\netc.\\nGiven\\na\\nsymbolic\\ndiscourse\\nlanguage\\nwe\\nmight\\nuse\\nit\\nto\\nmake\\n“standalone\\nstatements”.\\nBut\\nwe\\ncan\\nalso\\nuse\\nit\\nto\\nask\\nquestions\\nabout\\nthe\\nworld,\\n“Wolfram|Alpha\\nstyle”.\\nOr\\nwe\\ncan\\nuse\\nit\\nto\\nstate\\nthings\\nthat\\nwe\\n“want\\nto\\nmake\\nso”,\\npresumably\\nwith\\nsome\\nexternal\\nactuation\\nmechanism.\\nOr\\nwe\\ncan\\nuse\\nit\\nto\\nmake\\nassertions—perhaps\\nabout\\nthe\\nactual\\nworld,\\nor\\nperhaps\\nabout\\nsome\\nspecific\\nworld\\nwe’re\\nconsidering,\\nfictional\\nor\\notherwise.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 78}\n",
            "\n",
            "Page 80:\n",
            "page_content='Human\\nlanguage\\nis\\nfundamentally\\nimprecise,\\nnot\\nleast\\nbecause\\nit\\nisn’t\\n“tethered”\\nto\\na\\nspecific\\ncomputational\\nimplementation,\\nand\\nits\\nmeaning\\nis\\nbasically\\ndefined\\njust\\nby\\na\\n“social\\ncontract”\\nbetween\\nits\\nusers.\\nBut\\ncomputational\\nlanguage,\\nby\\nits\\nnature,\\nhas\\na\\ncertain\\nfundamental\\nprecision—because\\nin\\nthe\\nend\\nwhat\\nit\\nspecifies\\ncan\\nalways\\nbe\\n“unambiguously\\nexecuted\\non\\na\\ncomputer”.\\nHuman\\nlanguage\\ncan\\nusually\\nget\\naway\\nwith\\na\\ncertain\\nvagueness.\\n(When\\nwe\\nsay\\n“planet”\\ndoes\\nit\\ninclude\\nexoplanets\\nor\\nnot,\\netc.?)\\nBut\\nin\\ncomputational\\nlanguage\\nwe\\nhave\\nto\\nbe\\nprecise\\nand\\nclear\\nabout\\nall\\nthe\\ndistinctions\\nwe’re\\nmaking.\\nIt’s\\noften\\nconvenient\\nto\\nleverage\\nordinary\\nhuman\\nlanguage\\nin\\nmaking\\nup\\nnames\\nin\\ncomputational\\nlanguage.\\nBut\\nthe\\nmeanings\\nthey\\nhave\\nin\\ncomputational\\nlanguage\\nare\\nnecessarily\\nprecise—and\\nmight\\nor\\nmight\\nnot\\ncover\\nsome\\nparticular\\nconnotation\\nin\\ntypical\\nhuman\\nlanguage\\nusage.\\nHow\\nshould\\none\\nfigure\\nout\\nthe\\nfundamental\\n“ontology”\\nsuitable\\nfor\\na\\ngeneral\\nsymbolic\\ndiscourse\\nlanguage?\\nWell,\\nit’s\\nnot\\neasy.\\nWhich\\nis\\nperhaps\\nwhy\\nlittle\\nhas\\nbeen\\ndone\\nsince\\nthe\\nprimitive\\nbeginnings\\nAristotle\\nmade\\nmore\\nthan\\ntwo\\nmillennia\\nago.\\nBut\\nit\\nreally\\nhelps\\nthat\\ntoday\\nwe\\nnow\\nknow\\nso\\nmuch\\nabout\\nhow\\nto\\nthink\\nabout\\nthe\\nworld\\ncomputationally\\n(and\\nit\\ndoesn’t\\nhurt\\nto\\nhave\\na\\n“fundamental\\nmetaphysics”\\nfrom\\nour\\nPhysics\\nProject\\nand\\nthe\\nidea\\nof\\nthe\\nruliad\\n).\\nBut\\nwhat\\ndoes\\nall\\nthis\\nmean\\nin\\nthe\\ncontext\\nof\\nChatGPT?\\nFrom\\nits\\ntraining\\nChatGPT\\nhas\\neffectively\\n“pieced\\ntogether”\\na\\ncertain\\n(rather\\nimpressive)\\nquantity\\nof\\nwhat\\namounts\\nto\\nsemantic\\ngrammar.\\nBut\\nits\\nvery\\nsuccess\\ngives\\nus\\na\\nreason\\nto\\nthink\\nthat\\nit’s\\ngoing\\nto\\nbe\\nfeasible\\nto\\nconstruct\\nsomething\\nmore\\ncomplete\\nin\\ncomputational\\nlanguage\\nform.\\nAnd,\\nunlike\\nwhat\\nwe’ve\\nso\\nfar\\nfigured\\nout\\nabout\\nthe\\ninnards\\nof\\nChatGPT,\\nwe\\ncan\\nexpect\\nto\\ndesign\\nthe\\ncomputational\\nlanguage\\nso\\nthat\\nit’s\\nreadily\\nunderstandable\\nto\\nhumans.\\nWhen\\nwe\\ntalk\\nabout\\nsemantic\\ngrammar,\\nwe\\ncan\\ndraw\\nan\\nanalogy\\nto\\nsyllogistic\\nlogic.\\nAt\\nfirst,\\nsyllogistic\\nlogic\\nwas\\nessentially\\na\\ncollection\\nof\\nrules\\nabout\\nstatements\\nexpressed\\nin\\nhuman\\nlanguage.\\nBut\\n(yes,\\ntwo\\nmillennia\\nlater)\\nwhen\\nformal\\nlogic\\nwas\\ndeveloped\\n,\\nthe\\noriginal\\nbasic\\nconstructs\\nof\\nsyllogistic\\nlogic\\ncould\\nnow\\nbe\\nused\\nto\\nbuild\\nhuge\\n“formal\\ntowers”\\nthat\\ninclude,\\nfor\\nexample,\\nthe\\noperation\\nof\\nmodern\\ndigital\\ncircuitry.\\nAnd\\nso,\\nwe\\ncan\\nexpect,\\nit\\nwill\\nbe\\nwith\\nmore\\ngeneral\\nsemantic\\ngrammar.\\nAt\\nfirst,\\nit\\nmay\\njust\\nbe\\nable\\nto\\ndeal\\nwith\\nsimple\\npatterns,\\nexpressed,\\nsay,\\nas\\ntext.\\nBut\\nonce\\nits\\nwhole\\ncomputational\\nlanguage\\nframework\\nis\\nbuilt,\\nwe\\ncan\\nexpect\\nthat\\nit\\nwill' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 79}\n",
            "\n",
            "Page 81:\n",
            "page_content='be\\nable\\nto\\nbe\\nused\\nto\\nerect\\ntall\\ntowers\\nof\\n“generalized\\nsemantic\\nlogic”,\\nthat\\nallow\\nus\\nto\\nwork\\nin\\na\\nprecise\\nand\\nformal\\nway\\nwith\\nall\\nsorts\\nof\\nthings\\nthat\\nhave\\nnever\\nbeen\\naccessible\\nto\\nus\\nbefore,\\nexcept\\njust\\nat\\na\\n“ground-floor\\nlevel”\\nthrough\\nhuman\\nlanguage,\\nwith\\nall\\nits\\nvagueness.\\nWe\\ncan\\nthink\\nof\\nthe\\nconstruction\\nof\\ncomputational\\nlanguage—and\\nsemantic\\ngrammar—as\\nrepresenting\\na\\nkind\\nof\\nultimate\\ncompression\\nin\\nrepresenting\\nthings.\\nBecause\\nit\\nallows\\nus\\nto\\ntalk\\nabout\\nthe\\nessence\\nof\\nwhat’s\\npossible,\\nwithout,\\nfor\\nexample,\\ndealing\\nwith\\nall\\nthe\\n“turns\\nof\\nphrase”\\nthat\\nexist\\nin\\nordinary\\nhuman\\nlanguage.\\nAnd\\nwe\\ncan\\nview\\nthe\\ngreat\\nstrength\\nof\\nChatGPT\\nas\\nbeing\\nsomething\\na\\nbit\\nsimilar:\\nbecause\\nit\\ntoo\\nhas\\nin\\na\\nsense\\n“drilled\\nthrough”\\nto\\nthe\\npoint\\nwhere\\nit\\ncan\\n“put\\nlanguage\\ntogether\\nin\\na\\nsemantically\\nmeaningful\\nway”\\nwithout\\nconcern\\nfor\\ndifferent\\npossible\\nturns\\nof\\nphrase.\\nSo\\nwhat\\nwould\\nhappen\\nif\\nwe\\napplied\\nChatGPT\\nto\\nunderlying\\ncomputational\\nlanguage?\\nThe\\ncomputational\\nlanguage\\ncan\\ndescribe\\nwhat’s\\npossible.\\nBut\\nwhat\\ncan\\nstill\\nbe\\nadded\\nis\\na\\nsense\\nof\\n“what’s\\npopular”—based\\nfor\\nexample\\non\\nreading\\nall\\nthat\\ncontent\\non\\nthe\\nweb.\\nBut\\nthen—underneath—operating\\nwith\\ncomputational\\nlanguage\\nmeans\\nthat\\nsomething\\nlike\\nChatGPT\\nhas\\nimmediate\\nand\\nfundamental\\naccess\\nto\\nwhat\\namount\\nto\\nultimate\\ntools\\nfor\\nmaking\\nuse\\nof\\npotentially\\nirreducible\\ncomputations.\\nAnd\\nthat\\nmakes\\nit\\na\\nsystem\\nthat\\ncan\\nnot\\nonly\\n“generate\\nreasonable\\ntext”,\\nbut\\ncan\\nexpect\\nto\\nwork\\nout\\nwhatever\\ncan\\nbe\\nworked\\nout\\nabout\\nwhether\\nthat\\ntext\\nactually\\nmakes\\n“correct”\\nstatements\\nabout\\nthe\\nworld—or\\nwhatever\\nit’s\\nsupposed\\nto\\nbe\\ntalking\\nabout. \\nSo\\n…\\nWhat\\nIs\\nChatGPT\\nDoing,\\nand\\nWhy\\nDoes\\nIt\\nWork?\\nThe\\nbasic\\nconcept\\nof\\nChatGPT\\nis\\nat\\nsome\\nlevel\\nrather\\nsimple.\\nStart\\nfrom\\na\\nhuge\\nsample\\nof\\nhuman-created\\ntext\\nfrom\\nthe\\nweb,\\nbooks,\\netc.\\nThen\\ntrain\\na\\nneural\\nnet\\nto\\ngenerate\\ntext\\nthat’s\\n“like\\nthis”.\\nAnd\\nin\\nparticular,\\nmake\\nit\\nable\\nto\\nstart\\nfrom\\na\\n“prompt”\\nand\\nthen\\ncontinue\\nwith\\ntext\\nthat’s\\n“like\\nwhat\\nit’s\\nbeen\\ntrained\\nwith”.\\nAs\\nwe’ve\\nseen,\\nthe\\nactual\\nneural\\nnet\\nin\\nChatGPT\\nis\\nmade\\nup\\nof\\nvery\\nsimple\\nelements—though\\nbillions\\nof\\nthem.\\nAnd\\nthe\\nbasic\\noperation\\nof\\nthe\\nneural\\nnet\\nis\\nalso\\nvery\\nsimple,\\nconsisting\\nessentially\\nof\\npassing\\ninput\\nderived\\nfrom\\nthe\\ntext\\nit’s\\ngenerated\\nso\\nfar\\n“once\\nthrough\\nits\\nelements”\\n(without\\nany\\nloops,\\netc.)\\nfor\\nevery\\nnew\\nword\\n(or\\npart\\nof\\na\\nword)\\nthat\\nit\\ngenerates.' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 80}\n",
            "\n",
            "Page 82:\n",
            "page_content='But\\nthe\\nremarkable—and\\nunexpected—thing\\nis\\nthat\\nthis\\nprocess\\ncan\\nproduce\\ntext\\nthat’s\\nsuccessfully\\n“like”\\nwhat’s\\nout\\nthere\\non\\nthe\\nweb,\\nin\\nbooks,\\netc.\\nAnd\\nnot\\nonly\\nis\\nit\\ncoherent\\nhuman\\nlanguage,\\nit\\nalso\\n“says\\nthings”\\nthat\\n“follow\\nits\\nprompt”\\nmaking\\nuse\\nof\\ncontent\\nit’s\\n“read”.\\nIt\\ndoesn’t\\nalways\\nsay\\nthings\\nthat\\n“globally\\nmake\\nsense”\\n(or\\ncorrespond\\nto\\ncorrect\\ncomputations)—because\\n(without,\\nfor\\nexample,\\naccessing\\nthe\\n“computational\\nsuperpowers”\\nof\\nWolfram|Alpha\\n)\\nit’s\\njust\\nsaying\\nthings\\nthat\\n“sound\\nright”\\nbased\\non\\nwhat\\nthings\\n“sounded\\nlike”\\nin\\nits\\ntraining\\nmaterial.\\nThe\\nspecific\\nengineering\\nof\\nChatGPT\\nhas\\nmade\\nit\\nquite\\ncompelling.\\nBut\\nultimately\\n(at\\nleast\\nuntil\\nit\\ncan\\nuse\\noutside\\ntools)\\nChatGPT\\nis\\n“merely”\\npulling\\nout\\nsome\\n“coherent\\nthread\\nof\\ntext”\\nfrom\\nthe\\n“statistics\\nof\\nconventional\\nwisdom”\\nthat\\nit’s\\naccumulated.\\nBut\\nit’s\\namazing\\nhow\\nhuman-like\\nthe\\nresults\\nare.\\nAnd\\nas\\nI’ve\\ndiscussed,\\nthis\\nsuggests\\nsomething\\nthat’s\\nat\\nleast\\nscientifically\\nvery\\nimportant:\\nthat\\nhuman\\nlanguage\\n(and\\nthe\\npatterns\\nof\\nthinking\\nbehind\\nit)\\nare\\nsomehow\\nsimpler\\nand\\nmore\\n“law\\nlike”\\nin\\ntheir\\nstructure\\nthan\\nwe\\nthought.\\nChatGPT\\nhas\\nimplicitly\\ndiscovered\\nit.\\nBut\\nwe\\ncan\\npotentially\\nexplicitly\\nexpose\\nit,\\nwith\\nsemantic\\ngrammar,\\ncomputational\\nlanguage,\\netc.\\nWhat\\nChatGPT\\ndoes\\nin\\ngenerating\\ntext\\nis\\nvery\\nimpressive—and\\nthe\\nresults\\nare\\nusually\\nvery\\nmuch\\nlike\\nwhat\\nwe\\nhumans\\nwould\\nproduce.\\nSo\\ndoes\\nthis\\nmean\\nChatGPT\\nis\\nworking\\nlike\\na\\nbrain?\\nIts\\nunderlying\\nartificial-neural-net\\nstructure\\nwas\\nultimately\\nmodeled\\non\\nan\\nidealization\\nof\\nthe\\nbrain.\\nAnd\\nit\\nseems\\nquite\\nlikely\\nthat\\nwhen\\nwe\\nhumans\\ngenerate\\nlanguage\\nmany\\naspects\\nof\\nwhat’s\\ngoing\\non\\nare\\nquite\\nsimilar.\\nWhen\\nit\\ncomes\\nto\\ntraining\\n(AKA\\nlearning)\\nthe\\ndifferent\\n“hardware”\\nof\\nthe\\nbrain\\nand\\nof\\ncurrent\\ncomputers\\n(as\\nwell\\nas,\\nperhaps,\\nsome\\nundeveloped\\nalgorithmic\\nideas)\\nforces\\nChatGPT\\nto\\nuse\\na\\nstrategy\\nthat’s\\nprobably\\nrather\\ndifferent\\n(and\\nin\\nsome\\nways\\nmuch\\nless\\nefficient)\\nthan\\nthe\\nbrain.\\nAnd\\nthere’s\\nsomething\\nelse\\nas\\nwell:\\nunlike\\neven\\nin\\ntypical\\nalgorithmic\\ncomputation,\\nChatGPT\\ndoesn’t\\ninternally\\n“have\\nloops”\\nor\\n“recompute\\non\\ndata”.\\nAnd\\nthat\\ninevitably\\nlimits\\nits\\ncomputational\\ncapability—even\\nwith\\nrespect\\nto\\ncurrent\\ncomputers,\\nbut\\ndefinitely\\nwith\\nrespect\\nto\\nthe\\nbrain.\\nIt’s\\nnot\\nclear\\nhow\\nto\\n“fix\\nthat”\\nand\\nstill\\nmaintain\\nthe\\nability\\nto\\ntrain\\nthe\\nsystem\\nwith\\nreasonable\\nefficiency.\\nBut\\nto\\ndo\\nso\\nwill\\npresumably\\nallow\\na\\nfuture\\nChatGPT\\nto\\ndo\\neven\\nmore\\n“brain-like\\nthings”.\\nOf\\ncourse,\\nthere\\nare\\nplenty\\nof\\nthings\\nthat\\nbrains\\ndon’t\\ndo\\nso\\nwell—particularly\\ninvolving\\nwhat' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 81}\n",
            "\n",
            "Page 83:\n",
            "page_content='amount\\nto\\nirreducible\\ncomputations.\\nAnd\\nfor\\nthese\\nboth\\nbrains\\nand\\nthings\\nlike\\nChatGPT\\nhave\\nto\\nseek\\n“outside\\ntools”—like\\nWolfram\\nLanguage\\n.\\nBut\\nfor\\nnow\\nit’s\\nexciting\\nto\\nsee\\nwhat\\nChatGPT\\nhas\\nalready\\nbeen\\nable\\nto\\ndo.\\nAt\\nsome\\nlevel\\nit’s\\na\\ngreat\\nexample\\nof\\nthe\\nfundamental\\nscientific\\nfact\\nthat\\nlarge\\nnumbers\\nof\\nsimple\\ncomputational\\nelements\\ncan\\ndo\\nremarkable\\nand\\nunexpected\\nthings.\\nBut\\nit\\nalso\\nprovides\\nperhaps\\nthe\\nbest\\nimpetus\\nwe’ve\\nhad\\nin\\ntwo\\nthousand\\nyears\\nto\\nunderstand\\nbetter\\njust\\nwhat\\nthe\\nfundamental\\ncharacter\\nand\\nprinciples\\nmight\\nbe\\nof\\nthat\\ncentral\\nfeature\\nof\\nthe\\nhuman\\ncondition\\nthat\\nis\\nhuman\\nlanguage\\nand\\nthe\\nprocesses\\nof\\nthinking\\nbehind\\nit. \\nThanks\\nI’ve\\nbeen\\nfollowing\\nthe\\ndevelopment\\nof\\nneural\\nnets\\nnow\\nfor\\nabout\\n43\\nyears,\\nand\\nduring\\nthat\\ntime\\nI’ve\\ninteracted\\nwith\\nmany\\npeople\\nabout\\nthem.\\nAmong\\nthem—some\\nfrom\\nlong\\nago,\\nsome\\nfrom\\nrecently,\\nand\\nsome\\nacross\\nmany\\nyears—have\\nbeen:\\nGiulio\\nAlessandrini,\\nDario\\nAmodei,\\nEtienne\\nBernard,\\nTaliesin\\nBeynon,\\nSebastian\\nBodenstein,\\nGreg\\nBrockman,\\nJack\\nCowan,\\nPedro\\nDomingos,\\nJesse\\nGalef,\\nRoger\\nGermundsson,\\nRobert\\nHecht-Nielsen,\\nGeoff\\nHinton,\\nJohn\\nHopfield,\\nYann\\nLeCun,\\nJerry\\nLettvin,\\nJerome\\nLouradour,\\nMarvin\\nMinsky,\\nEric\\nMjolsness,\\nCayden\\nPierce,\\nTomaso\\nPoggio,\\nMatteo\\nSalvarezza,\\nTerry\\nSejnowski,\\nOliver\\nSelfridge,\\nGordon\\nShaw,\\nJonas\\nSjöberg,\\nIlya\\nSutskever,\\nGerry\\nTesauro\\nand\\nTimothee\\nVerdier.\\nFor\\nhelp\\nwith\\nthis\\npiece,\\nI’d\\nparticularly\\nlike\\nto\\nthank\\nGiulio\\nAlessandrini\\nand\\nBrad\\nKlee. \\nAdditional\\nResources\\n“Machine\\nLearning\\nfor\\nMiddle\\nSchoolers”\\n»\\nIntroduction\\nto\\nMachine\\nLearning\\n»\\nWolfram\\nU\\nMachine\\nLearning\\nCourses\\n»' metadata={'source': '/content/Stephen Wolfram - What Is ChatGPT Doing ... and Why Does It Work?.pdf', 'page': 82}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexes"
      ],
      "metadata": {
        "id": "NJ8ssntoX3IR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/farchettiensis/farchettiensis.github.io/main/gpt.txt\"\n",
        "res = requests.get(url)\n",
        "with open(\"gpt.txt\", \"w\") as f:\n",
        "  f.write(res.text)"
      ],
      "metadata": {
        "id": "xs1Uvom1X5xF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Document Loader\n",
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('/content/gpt.txt')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "W2I89hatX7dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# documents"
      ],
      "metadata": {
        "id": "PMgJ8J8tX9FT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Splitter\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Mc881_bHX_ss",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b24c2a5-2f8c-41f1-e5bb-71c8820c777b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.text_splitter:Created a chunk of size 1043, which is longer than the specified 1000\n",
            "WARNING:langchain.text_splitter:Created a chunk of size 1121, which is longer than the specified 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "1EbQbXvVYAsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ef6dda-c270-42de-9071-483cd3910739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "144"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "Eh6f6cO1a_K2",
        "outputId": "78582833-6b50-4554-9d13-12075784573f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What Is ChatGPT Doing … and Why Does It Work?\n",
            "\n",
            "It’s Just Adding One Word at a Time\n",
            "That ChatGPT can automatically generate something that reads even superficially like human-written text is remarkable, and unexpected. But how does it do it? And why does it work? My purpose here is to give a rough outline of what’s going on inside ChatGPT—and then to explore why it is that it can do so well in producing what we might consider to be meaningful text. I should say at the outset that I’m going to focus on the big picture of what’s going on—and while I’ll mention some engineering details, I won’t get deeply into them. (And the essence of what I’ll say applies just as well to other current “large language models” [LLMs] as to ChatGPT.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "id": "FEmaKHiDYDVL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c25541e-8229-4435-e649-cab1fcb13b71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.30.1-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=995ebb7dac1dad5a7023cd74eae80a21f4c76bfcc196f3274d75a301f0c7ebd5\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, huggingface-hub, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Embeddings\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "VfOAIya3YEak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "nl1li406YQDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14ab58d8-5ced-4b1c-be69-a0517576ae73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cz6VKP9MhB3E",
        "outputId": "92ce32e3-b72e-4436-f924-d721df824ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorstore: https://python.langchain.com/en/latest/modules/indexes/vectorstores.html\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "query = \"What is temperature in the context of LLMs?\"\n",
        "docs = db.similarity_search(query)"
      ],
      "metadata": {
        "id": "XfSeiSJbYR_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "id": "yi-GDVJPYVWy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6f189a-6774-42c8-af71-2b235d8f4ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The fact that there’s randomness here means that if we use the same prompt multiple times, we’re likely to get different essays each time. And, in keeping with the idea of voodoo, there’s a particular so-called “temperature” parameter that determines how often lower-ranked words will be used, and for essay generation, it turns out that a “temperature” of 0.8 seems best. (It’s worth emphasizing that there’s no “theory” being used here; it’s just a matter of what’s been found to work in practice. And for example the concept of “temperature” is there because exponential distributions familiar from statistical physics happen to be being used, but there’s no “physical” connection—at least so far as we know.)\n"
          ]
        }
      ]
    }
  ]
}